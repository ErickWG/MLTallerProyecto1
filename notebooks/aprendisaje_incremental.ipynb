{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7001d621",
   "metadata": {},
   "source": [
    "## APRENDIZAJE INCREMENTAL DIARIO - MODELO DE DETECCI√ìN DE FRAUDE\n",
    "\n",
    "### -Este notebook actualiza el modelo general con datos del d√≠a anterior\n",
    "### -Se ejecuta una vez al d√≠a para mantener el modelo actualizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81ee973c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Librer√≠as importadas para aprendizaje incremental\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from river import anomaly\n",
    "from river import preprocessing\n",
    "import pickle\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Librer√≠as importadas para aprendizaje incremental\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcb72a5",
   "metadata": {},
   "source": [
    "# # 1. CONFIGURACI√ìN Y RUTAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0e1b656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Directorio de modelos: C:\\Users\\User\\Desktop\\TESIS\\CodigoGithub\\MLTallerProyecto1\\Modelos\n",
      "üìÅ Directorio de datos diarios: C:\\Users\\User\\Desktop\\TESIS\\Datos_Diarios\n",
      "üìÑ Archivo a procesar: datos_primer_dia.csv\n"
     ]
    }
   ],
   "source": [
    "# Rutas de archivos\n",
    "MODELS_PATH = r\"C:\\Users\\User\\Desktop\\TESIS\\CodigoGithub\\MLTallerProyecto1\\Modelos\"\n",
    "DAILY_DATA_PATH = r\"C:\\Users\\User\\Desktop\\TESIS\\Datos_Diarios\"  # Aqu√≠ llegan los CSVs diarios\n",
    "\n",
    "# Crear directorio de datos diarios si no existe\n",
    "os.makedirs(DAILY_DATA_PATH, exist_ok=True)\n",
    "\n",
    "# Fecha del d√≠a anterior (para el archivo a procesar)\n",
    "archivo_ayer = \"datos_primer_dia.csv\"\n",
    "ruta_archivo_ayer = os.path.join(DAILY_DATA_PATH, archivo_ayer)\n",
    "\n",
    "print(f\"üìÅ Directorio de modelos: {MODELS_PATH}\")\n",
    "print(f\"üìÅ Directorio de datos diarios: {DAILY_DATA_PATH}\")\n",
    "print(f\"üìÑ Archivo a procesar: {archivo_ayer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e906fc6",
   "metadata": {},
   "source": [
    "# # 2. CARGAR MODELO Y CONFIGURACI√ìN EXISTENTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9498c465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Modelo cargado exitosamente\n",
      "üìä Pa√≠ses en contexto hist√≥rico: 188\n",
      "üìä Pa√≠ses en stats actuales: 188\n",
      "üéØ Umbral actual: 0.9724\n"
     ]
    }
   ],
   "source": [
    "modelo_path = os.path.join(MODELS_PATH, \"modelo_general.pkl\")\n",
    "with open(modelo_path, 'rb') as f:\n",
    "    modelo = pickle.load(f)\n",
    "\n",
    "# Cargar scaler\n",
    "scaler_path = os.path.join(MODELS_PATH, \"scaler_general.pkl\")\n",
    "with open(scaler_path, 'rb') as f:\n",
    "    scaler = pickle.load(f)\n",
    "\n",
    "# Cargar configuraci√≥n\n",
    "config_path = os.path.join(MODELS_PATH, \"config_modelo_general.pkl\")\n",
    "with open(config_path, 'rb') as f:\n",
    "    config = pickle.load(f)\n",
    "\n",
    "# Extraer componentes importantes\n",
    "stats_dict = config['stats_por_pais']\n",
    "contexto_historico = config['contexto_historico']  # CONTEXTOS PERMANENTES\n",
    "umbral_global = config['umbral_global']\n",
    "parametros_features = config['parametros_features']\n",
    "\n",
    "print(f\"‚úÖ Modelo cargado exitosamente\")\n",
    "print(f\"üìä Pa√≠ses en contexto hist√≥rico: {len(contexto_historico)}\")\n",
    "print(f\"üìä Pa√≠ses en stats actuales: {len(stats_dict)}\")\n",
    "print(f\"üéØ Umbral actual: {umbral_global:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55fd03c",
   "metadata": {},
   "source": [
    "# # 3. VERIFICAR Y CARGAR DATOS DEL D√çA ANTERIOR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacbed7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nüìÑ Verificando datos del d√≠a anterior...\")\n",
    "\n",
    "if not os.path.exists(ruta_archivo_ayer):\n",
    "    print(f\"‚ùå ERROR: No se encontr√≥ el archivo {archivo_ayer}\")\n",
    "    print(f\"üìÅ Ruta esperada: {ruta_archivo_ayer}\")\n",
    "    print(\"üîç Archivos disponibles en el directorio:\")\n",
    "    \n",
    "    if os.path.exists(DAILY_DATA_PATH):\n",
    "        archivos = [f for f in os.listdir(DAILY_DATA_PATH) if f.endswith('.csv')]\n",
    "        for archivo in archivos:\n",
    "            print(f\"   - {archivo}\")\n",
    "    else:\n",
    "        print(\"   (Directorio no existe)\")\n",
    "    \n",
    "    # Usar archivo m√°s reciente si existe\n",
    "    if os.path.exists(DAILY_DATA_PATH):\n",
    "        archivos = [f for f in os.listdir(DAILY_DATA_PATH) if f.endswith('.csv')]\n",
    "        if archivos:\n",
    "            archivo_mas_reciente = max(archivos)\n",
    "            ruta_archivo_ayer = os.path.join(DAILY_DATA_PATH, archivo_mas_reciente)\n",
    "            print(f\"üìÑ Usando archivo m√°s reciente: {archivo_mas_reciente}\")\n",
    "        else:\n",
    "            raise FileNotFoundError(\"No hay archivos CSV disponibles para procesar\")\n",
    "    else:\n",
    "        raise FileNotFoundError(\"Directorio de datos diarios no existe\")\n",
    "\n",
    "# Cargar datos del d√≠a anterior\n",
    "print(f\"üîÑ Cargando datos de {os.path.basename(ruta_archivo_ayer)}...\")\n",
    "df_ayer = pd.read_csv(ruta_archivo_ayer)\n",
    "\n",
    "# Convertir fecha si existe\n",
    "if 'FECHA' in df_ayer.columns:\n",
    "    df_ayer['FECHA'] = pd.to_datetime(df_ayer['FECHA'], format='%d/%m/%Y', errors='coerce')\n",
    "\n",
    "print(f\"üìã Datos cargados - Shape: {df_ayer.shape}\")\n",
    "print(f\"üåç Pa√≠ses en datos nuevos: {df_ayer['CODIGODEPAIS'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279af1fd",
   "metadata": {},
   "source": [
    "# # 4. FUNCI√ìN DE FEATURES (IMPORTADA DEL MODELO ORIGINAL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6946d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crear_features_contextualizadas_mejorada(row, stats_pais_dict):\n",
    "    \"\"\"\n",
    "    Funci√≥n id√©ntica a la del modelo original - MANTENER CONSISTENCIA\n",
    "    \"\"\"\n",
    "    pais = row['CODIGODEPAIS']\n",
    "    llamadas = row['N_LLAMADAS']\n",
    "    minutos = row['N_MINUTOS']\n",
    "    destinos = row['N_DESTINOS']\n",
    "    \n",
    "    # Usar par√°metros guardados en config\n",
    "    PESO_MINUTOS_NORMAL = parametros_features['peso_minutos_normal']\n",
    "    PESO_MINUTOS_EXTREMOS = parametros_features['peso_minutos_extremos']\n",
    "    UMBRAL_MINUTOS_EXTREMOS = parametros_features['umbral_minutos_extremos']\n",
    "    PESO_DESTINOS = parametros_features['peso_destinos']\n",
    "    PESO_SPRAY_RATIO = parametros_features['peso_spray_ratio']\n",
    "    \n",
    "    # Obtener contexto del pa√≠s (si existe)\n",
    "    if pais in stats_pais_dict:\n",
    "        pais_stats = stats_pais_dict[pais]\n",
    "        categoria = pais_stats['CATEGORIA']\n",
    "        \n",
    "        # Normalizar por el contexto del pa√≠s\n",
    "        llamadas_norm = min(llamadas / max(pais_stats['LLAMADAS_P95'], 1), 1.5)\n",
    "        destinos_norm = min(destinos / max(pais_stats['DESTINOS_P95'], 1), 1.5)\n",
    "        \n",
    "        # Detecci√≥n inteligente de minutos extremos\n",
    "        minutos_p90 = pais_stats.get('MINUTOS_P90', pais_stats['MINUTOS_P95'] * 0.9)\n",
    "        minutos_p95 = pais_stats['MINUTOS_P95']\n",
    "        \n",
    "        # Transformaci√≥n adaptativa de minutos\n",
    "        if minutos >= UMBRAL_MINUTOS_EXTREMOS:\n",
    "            minutos_norm = min(minutos / max(minutos_p90, 1), 3.0)\n",
    "            peso_minutos = PESO_MINUTOS_EXTREMOS\n",
    "        else:\n",
    "            minutos_norm = min(np.log1p(minutos) / np.log1p(max(minutos_p90, 1)), 1.2)\n",
    "            peso_minutos = PESO_MINUTOS_NORMAL\n",
    "            \n",
    "    else:\n",
    "        # Pa√≠s nuevo - SIEMPRE clasificar como 'Muy_Bajo'\n",
    "        categoria = 'Muy_Bajo'\n",
    "        llamadas_norm = min(llamadas / 10, 2.0)\n",
    "        destinos_norm = min(destinos / 5, 2.0)\n",
    "        \n",
    "        if minutos >= UMBRAL_MINUTOS_EXTREMOS:\n",
    "            minutos_norm = min(minutos / 50, 3.0)\n",
    "            peso_minutos = PESO_MINUTOS_EXTREMOS * 1.2\n",
    "        else:\n",
    "            minutos_norm = min(np.log1p(minutos) / np.log1p(60), 1.2)\n",
    "            peso_minutos = PESO_MINUTOS_NORMAL\n",
    "    \n",
    "    # Features principales (id√©nticas al modelo original)\n",
    "    features = {\n",
    "        'llamadas_norm': llamadas_norm * 0.8,\n",
    "        'destinos_norm': destinos_norm * PESO_DESTINOS,\n",
    "        'minutos_norm': minutos_norm * peso_minutos,\n",
    "        'diversidad_destinos': min(destinos / max(llamadas, 1), 1.0),\n",
    "        'spray_ratio': min(destinos / max(llamadas, 1) * PESO_SPRAY_RATIO, 1.0) if destinos >= 5 else 0,\n",
    "        'minutos_extremos': 1.0 if minutos >= UMBRAL_MINUTOS_EXTREMOS else 0.0,\n",
    "        'minutos_sospechosos': min((minutos - 200) / 300, 1.0) if minutos > 200 else 0.0,\n",
    "        'patron_spray_fuerte': 1.0 if (destinos >= 10 and llamadas >= 20) else 0.0,\n",
    "        'patron_spray_medio': 0.5 if (destinos >= 6 and llamadas >= 12) else 0.0,\n",
    "        'alta_diversidad': min(destinos / 12, 1) if destinos >= 5 else 0,\n",
    "        'volumen_llamadas_alto': min((llamadas - 30) / 50, 1) if llamadas > 30 else 0,\n",
    "        'volumen_destinos_alto': min((destinos - 10) / 20, 1) if destinos > 10 else 0,\n",
    "        'llamadas_por_destino': min(llamadas / max(destinos, 1) / 5, 1),\n",
    "        'eficiencia_destinos': min(destinos / max(llamadas * 0.5, 1), 1),\n",
    "        'factor_pais_bajo': 1.5 if categoria in ['Muy_Bajo', 'Bajo'] else 1.0,\n",
    "        'factor_pais_alto': 0.9 if categoria in ['Alto', 'Medio'] else 1.0\n",
    "    }\n",
    "    \n",
    "    return features\n",
    "\n",
    "print(\"üîß Funci√≥n de features cargada (consistente con modelo original)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a928d0",
   "metadata": {},
   "source": [
    "# # 5. APRENDIZAJE INCREMENTAL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fbbb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nüéì Iniciando aprendizaje incremental...\")\n",
    "\n",
    "# Contadores\n",
    "registros_procesados = 0\n",
    "paises_nuevos = set()\n",
    "registros_por_pais = {}\n",
    "\n",
    "# Procesar cada registro del d√≠a anterior\n",
    "for idx, row in df_ayer.iterrows():\n",
    "    # Crear features\n",
    "    features = crear_features_contextualizadas_mejorada(row, stats_dict)\n",
    "    \n",
    "    # Normalizar con el scaler existente\n",
    "    scaler.learn_one(features)  # Actualizar scaler incrementalmente\n",
    "    features_scaled = scaler.transform_one(features)\n",
    "    \n",
    "    # Obtener score antes del aprendizaje (para logging)\n",
    "    score_antes = modelo.score_one(features_scaled)\n",
    "    \n",
    "    # APRENDER del nuevo registro\n",
    "    modelo.learn_one(features_scaled)\n",
    "    \n",
    "    # Estad√≠sticas\n",
    "    pais = row['CODIGODEPAIS']\n",
    "    registros_procesados += 1\n",
    "    \n",
    "    # Detectar pa√≠ses nuevos\n",
    "    if pais not in contexto_historico and pais not in stats_dict:\n",
    "        paises_nuevos.add(pais)\n",
    "    \n",
    "    # Contar registros por pa√≠s\n",
    "    registros_por_pais[pais] = registros_por_pais.get(pais, 0) + 1\n",
    "    \n",
    "    if registros_procesados % 1000 == 0:\n",
    "        print(f\"   Procesados: {registros_procesados}/{len(df_ayer)} registros\")\n",
    "\n",
    "print(f\"‚úÖ Aprendizaje incremental completado\")\n",
    "print(f\"üìä Registros procesados: {registros_procesados}\")\n",
    "print(f\"üÜï Pa√≠ses nuevos detectados: {len(paises_nuevos)}\")\n",
    "\n",
    "if paises_nuevos:\n",
    "    print(f\"üåç Pa√≠ses nuevos: {list(paises_nuevos)[:10]}...\")  # Mostrar primeros 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef046aa1",
   "metadata": {},
   "source": [
    "# # 6. ACTUALIZAR ESTAD√çSTICAS POR PA√çS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43156ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nüìä Actualizando estad√≠sticas por pa√≠s...\")\n",
    "\n",
    "# Calcular nuevas estad√≠sticas para pa√≠ses con suficientes datos\n",
    "stats_nuevos = {}\n",
    "umbral_minimo = 20  # M√≠nimo de registros para actualizar stats\n",
    "\n",
    "for pais in df_ayer['CODIGODEPAIS'].unique():\n",
    "    datos_pais = df_ayer[df_ayer['CODIGODEPAIS'] == pais]\n",
    "    \n",
    "    if len(datos_pais) >= umbral_minimo:\n",
    "        # Calcular estad√≠sticas nuevas\n",
    "        stats_nuevos[pais] = {\n",
    "            'REGISTROS': len(datos_pais),\n",
    "            'LLAMADAS_MEAN': datos_pais['N_LLAMADAS'].mean(),\n",
    "            'LLAMADAS_STD': datos_pais['N_LLAMADAS'].std(),\n",
    "            'LLAMADAS_P90': datos_pais['N_LLAMADAS'].quantile(0.90),\n",
    "            'LLAMADAS_P95': datos_pais['N_LLAMADAS'].quantile(0.95),\n",
    "            'MINUTOS_MEAN': datos_pais['N_MINUTOS'].mean(),\n",
    "            'MINUTOS_STD': datos_pais['N_MINUTOS'].std(),\n",
    "            'MINUTOS_P90': datos_pais['N_MINUTOS'].quantile(0.90),\n",
    "            'MINUTOS_P95': datos_pais['N_MINUTOS'].quantile(0.95),\n",
    "            'DESTINOS_MEAN': datos_pais['N_DESTINOS'].mean(),\n",
    "            'DESTINOS_STD': datos_pais['N_DESTINOS'].std(),\n",
    "            'DESTINOS_P90': datos_pais['N_DESTINOS'].quantile(0.90),\n",
    "            'DESTINOS_P95': datos_pais['N_DESTINOS'].quantile(0.95),\n",
    "        }\n",
    "        \n",
    "        # Clasificar pa√≠s por volumen\n",
    "        registros_totales = stats_nuevos[pais]['REGISTROS']\n",
    "        if pais in stats_dict:\n",
    "            registros_totales += stats_dict[pais].get('REGISTROS', 0)\n",
    "        \n",
    "        if registros_totales <= 50:\n",
    "            categoria = 'Muy_Bajo'\n",
    "        elif registros_totales <= 200:\n",
    "            categoria = 'Bajo'\n",
    "        elif registros_totales <= 1000:\n",
    "            categoria = 'Medio'\n",
    "        else:\n",
    "            categoria = 'Alto'\n",
    "        \n",
    "        stats_nuevos[pais]['CATEGORIA'] = categoria\n",
    "\n",
    "# Fusionar estad√≠sticas existentes con nuevas\n",
    "stats_actualizados = stats_dict.copy()\n",
    "\n",
    "for pais, stats_nuevas in stats_nuevos.items():\n",
    "    if pais in stats_actualizados:\n",
    "        # Promediar con estad√≠sticas existentes (weighted average)\n",
    "        peso_existente = 0.7  # Dar m√°s peso a datos hist√≥ricos\n",
    "        peso_nuevo = 0.3\n",
    "        \n",
    "        for campo in ['LLAMADAS_MEAN', 'MINUTOS_MEAN', 'DESTINOS_MEAN']:\n",
    "            if campo in stats_actualizados[pais]:\n",
    "                valor_existente = stats_actualizados[pais][campo]\n",
    "                valor_nuevo = stats_nuevas[campo]\n",
    "                stats_actualizados[pais][campo] = (valor_existente * peso_existente + \n",
    "                                                 valor_nuevo * peso_nuevo)\n",
    "        \n",
    "        # Actualizar percentiles con datos m√°s recientes\n",
    "        for campo in ['LLAMADAS_P90', 'LLAMADAS_P95', 'MINUTOS_P90', 'MINUTOS_P95', \n",
    "                     'DESTINOS_P90', 'DESTINOS_P95']:\n",
    "            if campo in stats_nuevas:\n",
    "                stats_actualizados[pais][campo] = stats_nuevas[campo]\n",
    "        \n",
    "        # Actualizar categor√≠a si cambi√≥\n",
    "        stats_actualizados[pais]['CATEGORIA'] = stats_nuevas['CATEGORIA']\n",
    "        \n",
    "    else:\n",
    "        # Pa√≠s nuevo - agregar completamente\n",
    "        stats_actualizados[pais] = stats_nuevas\n",
    "        \n",
    "        # IMPORTANTE: Pa√≠ses nuevos NO se agregan al contexto hist√≥rico\n",
    "        # El contexto hist√≥rico permanece INMUTABLE\n",
    "\n",
    "print(f\"üìä Estad√≠sticas actualizadas para {len(stats_nuevos)} pa√≠ses\")\n",
    "print(f\"üìä Total pa√≠ses en estad√≠sticas: {len(stats_actualizados)}\")\n",
    "print(f\"üîí Contexto hist√≥rico INMUTABLE: {len(contexto_historico)} pa√≠ses\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f1d8ea",
   "metadata": {},
   "source": [
    "# # 7. GUARDAR MODELO ACTUALIZADO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c97eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nüíæ Guardando modelo actualizado...\")\n",
    "\n",
    "# Guardar modelo actualizado\n",
    "modelo_path = os.path.join(MODELS_PATH, \"modelo_general.pkl\")\n",
    "with open(modelo_path, 'wb') as f:\n",
    "    pickle.dump(modelo, f)\n",
    "\n",
    "# Guardar scaler actualizado\n",
    "scaler_path = os.path.join(MODELS_PATH, \"scaler_general.pkl\")\n",
    "with open(scaler_path, 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "# Actualizar configuraci√≥n\n",
    "config['stats_por_pais'] = stats_actualizados\n",
    "config['fecha_ultima_actualizacion'] = datetime.now().isoformat()\n",
    "config['registros_ultima_actualizacion'] = registros_procesados\n",
    "config['paises_nuevos_detectados'] = list(paises_nuevos)\n",
    "\n",
    "# IMPORTANTE: contexto_historico NO se modifica\n",
    "# config['contexto_historico'] permanece igual\n",
    "\n",
    "# Guardar configuraci√≥n actualizada\n",
    "config_path = os.path.join(MODELS_PATH, \"config_modelo_general.pkl\")\n",
    "with open(config_path, 'wb') as f:\n",
    "    pickle.dump(config, f)\n",
    "\n",
    "# Crear respaldo con timestamp\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "backup_path = os.path.join(MODELS_PATH, f\"backup_config_{timestamp}.pkl\")\n",
    "with open(backup_path, 'wb') as f:\n",
    "    pickle.dump(config, f)\n",
    "\n",
    "print(\"‚úÖ Modelo actualizado y guardado exitosamente\")\n",
    "print(f\"üìÅ Backup creado: backup_config_{timestamp}.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
