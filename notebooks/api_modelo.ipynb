{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f66de2f",
   "metadata": {},
   "source": [
    "# API REST - SISTEMA DE DETECCIÓN DE FRAUDE TELEFÓNICO\n",
    "## API completa con FastAPI para scoring, aprendizaje incremental y gestión del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f921a04d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Librerías importadas para API\n"
     ]
    }
   ],
   "source": [
    "from fastapi import FastAPI, File, UploadFile, HTTPException, BackgroundTasks, Query\n",
    "from fastapi.responses import JSONResponse, FileResponse\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional, Dict, Any\n",
    "from datetime import datetime, timedelta\n",
    "from enum import Enum\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "import logging\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configurar logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"✅ Librerías importadas para API\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a007df83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import oracledb\n",
    "from datetime import datetime\n",
    "import os\n",
    "from typing import Optional\n",
    "from contextlib import contextmanager\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f1f217",
   "metadata": {},
   "source": [
    "# ## CONFIGURACIÓN DE ORACLE DATABASE CON ORACLEDB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67f1fb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de Oracle (ajusta según tu ambiente)\n",
    "ORACLE_CONFIG = {\n",
    "    \"user\": \"SYSTEM\",\n",
    "    \"password\": \"erick710132\",\n",
    "    \"dsn\": \"localhost:1521/xe\",  # Formato: host:port/service_name\n",
    "    # Alternativa con TNS:\n",
    "    # \"dsn\": \"XEPDB1\"  # Si usas tnsnames.ora\n",
    "}\n",
    "\n",
    "# Pool de conexiones para mejor rendimiento\n",
    "oracle_pool = None\n",
    "\n",
    "def inicializar_oracle_pool():\n",
    "    \"\"\"Inicializa el pool de conexiones de Oracle\"\"\"\n",
    "    global oracle_pool\n",
    "    try:\n",
    "        # Crear pool de conexiones\n",
    "        oracle_pool = oracledb.create_pool(\n",
    "            user=ORACLE_CONFIG[\"user\"],\n",
    "            password=ORACLE_CONFIG[\"password\"],\n",
    "            dsn=ORACLE_CONFIG[\"dsn\"],\n",
    "            min=2,\n",
    "            max=10,\n",
    "            increment=1\n",
    "        )\n",
    "        logger.info(\"Pool de conexiones Oracle inicializado\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error inicializando Oracle: {e}\")\n",
    "        return False\n",
    "\n",
    "@contextmanager\n",
    "def get_oracle_connection():\n",
    "    \"\"\"Context manager para obtener conexión del pool\"\"\"\n",
    "    connection = oracle_pool.acquire()\n",
    "    connection.encoding = \"UTF-8\"\n",
    "    connection.nencoding = \"UTF-8\"\n",
    "    try:\n",
    "        yield connection\n",
    "    finally:\n",
    "        oracle_pool.release(connection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746dd2b6",
   "metadata": {},
   "source": [
    "# ## FUNCIONES ACTUALIZADAS PARA ORACLEDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03f0015a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crear_tablas_oracle():\n",
    "    \"\"\"Crea las tablas necesarias en Oracle si no existen\"\"\"\n",
    "    \n",
    "    # Script SQL para crear las tablas\n",
    "    sql_scripts = [\n",
    "\n",
    "        \"\"\"\n",
    "        CREATE TABLE ALERTAS_FRAUDE (\n",
    "            ID_ALERTA NUMBER GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,\n",
    "            FECHA_PROCESAMIENTO TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "            FECHA_REGISTRO VARCHAR2(10),\n",
    "            CODIGO_PAIS NUMBER,\n",
    "            LINEA VARCHAR2(20),\n",
    "            N_LLAMADAS NUMBER,\n",
    "            N_MINUTOS NUMBER(10,2),\n",
    "            N_DESTINOS NUMBER,\n",
    "            SCORE_ANOMALIA NUMBER(10,4),\n",
    "            UMBRAL NUMBER(10,4),\n",
    "            TIPO_ANOMALIA VARCHAR2(50),\n",
    "            TIPO_CONTEXTO VARCHAR2(50),\n",
    "            RAZON_DECISION VARCHAR2(200),\n",
    "            ARCHIVO_ORIGEN VARCHAR2(100),\n",
    "            LOTE_PROCESAMIENTO VARCHAR2(50)\n",
    "        )\n",
    "        \"\"\",\n",
    "        \n",
    "        \"\"\"\n",
    "        CREATE TABLE LOTES_PROCESAMIENTO (\n",
    "            ID_LOTE NUMBER GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,\n",
    "            TIMESTAMP_INICIO TIMESTAMP,\n",
    "            TIMESTAMP_FIN TIMESTAMP,\n",
    "            ARCHIVO_ENTRADA VARCHAR2(200),\n",
    "            TOTAL_REGISTROS NUMBER,\n",
    "            TOTAL_ANOMALIAS NUMBER,\n",
    "            TASA_ANOMALIAS NUMBER(5,2),\n",
    "            ARCHIVO_SALIDA VARCHAR2(200),\n",
    "            ESTADO VARCHAR2(20),\n",
    "            MENSAJE_ERROR VARCHAR2(500)\n",
    "        )\n",
    "        \"\"\",\n",
    "        \n",
    "        \"\"\"\n",
    "        CREATE TABLE HISTORICO_UMBRALES (\n",
    "            ID_CAMBIO NUMBER GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,\n",
    "            FECHA_CAMBIO TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "            UMBRAL_ANTERIOR NUMBER(10,4),\n",
    "            UMBRAL_NUEVO NUMBER(10,4),\n",
    "            RAZON_CAMBIO VARCHAR2(200),\n",
    "            USUARIO VARCHAR2(50)\n",
    "        )\n",
    "        \"\"\",\n",
    "        \n",
    "        \"\"\"\n",
    "        CREATE TABLE APRENDIZAJE_INCREMENTAL (\n",
    "            ID_APRENDIZAJE NUMBER GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,\n",
    "            FECHA_EJECUCION TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "            FECHA_DATOS DATE,\n",
    "            ARCHIVO_ENTRADA VARCHAR2(200),\n",
    "            \n",
    "            -- Estadísticas del procesamiento\n",
    "            TOTAL_REGISTROS NUMBER,\n",
    "            REGISTROS_PROCESADOS NUMBER,\n",
    "            PAISES_TOTALES NUMBER,\n",
    "            PAISES_NUEVOS NUMBER,\n",
    "            PAISES_ACTUALIZADOS NUMBER,\n",
    "            \n",
    "            -- Información del modelo antes\n",
    "            UMBRAL_ANTERIOR NUMBER(10,4),\n",
    "            N_TREES_ANTERIOR NUMBER,\n",
    "            PAISES_CONOCIDOS_ANTERIOR NUMBER,\n",
    "            \n",
    "            -- Información del modelo después\n",
    "            UMBRAL_NUEVO NUMBER(10,4),\n",
    "            N_TREES_NUEVO NUMBER,\n",
    "            PAISES_CONOCIDOS_NUEVO NUMBER,\n",
    "            \n",
    "            -- Cambios en estadísticas\n",
    "            PAISES_CON_CAMBIO_CATEGORIA NUMBER,\n",
    "            DETALLE_CAMBIOS_CATEGORIA CLOB,\n",
    "            \n",
    "            -- Métricas de performance\n",
    "            TIEMPO_PROCESAMIENTO_SEG NUMBER(10,2),\n",
    "            MEMORIA_USADA_MB NUMBER(10,2),\n",
    "            \n",
    "            -- Estado y respaldos\n",
    "            ESTADO VARCHAR2(20),\n",
    "            MENSAJE_ERROR VARCHAR2(500),\n",
    "            ARCHIVO_BACKUP_CONFIG VARCHAR2(200),\n",
    "            \n",
    "            -- Estadísticas agregadas\n",
    "            LLAMADAS_PROMEDIO_GLOBAL NUMBER(10,2),\n",
    "            MINUTOS_PROMEDIO_GLOBAL NUMBER(10,2),\n",
    "            DESTINOS_PROMEDIO_GLOBAL NUMBER(10,2),\n",
    "            \n",
    "            -- Umbrales de features\n",
    "            UMBRAL_MINUTOS_EXTREMOS NUMBER(10,2),\n",
    "            PESO_SPRAY_RATIO NUMBER(10,2),\n",
    "            \n",
    "            -- Observaciones\n",
    "            OBSERVACIONES VARCHAR2(1000)\n",
    "        )\n",
    "        \"\"\",\n",
    "        \n",
    "        # Índices para mejorar performance\n",
    "        \"CREATE INDEX IDX_ALERTAS_FECHA ON ALERTAS_FRAUDE(FECHA_PROCESAMIENTO)\",\n",
    "        \"CREATE INDEX IDX_ALERTAS_PAIS ON ALERTAS_FRAUDE(CODIGO_PAIS)\",\n",
    "        \"CREATE INDEX IDX_ALERTAS_LINEA ON ALERTAS_FRAUDE(LINEA)\",\n",
    "        \"CREATE INDEX IDX_ALERTAS_TIPO ON ALERTAS_FRAUDE(TIPO_ANOMALIA)\",\n",
    "        \"CREATE INDEX IDX_APRENDIZAJE_FECHA ON APRENDIZAJE_INCREMENTAL(FECHA_EJECUCION)\",\n",
    "        \"CREATE INDEX IDX_APRENDIZAJE_ESTADO ON APRENDIZAJE_INCREMENTAL(ESTADO)\"\n",
    "    ]\n",
    "    \n",
    "    with get_oracle_connection() as conn:\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        for script in sql_scripts:\n",
    "            try:\n",
    "                cursor.execute(script)\n",
    "                logger.info(f\"Tabla/Índice creado exitosamente\")\n",
    "            except oracledb.DatabaseError as e:\n",
    "                error, = e.args\n",
    "                if error.code == 955:  # Tabla ya existe\n",
    "                    logger.info(\"Tabla ya existe, continuando...\")\n",
    "                else:\n",
    "                    logger.error(f\"Error creando tabla: {e}\")\n",
    "                    raise\n",
    "        \n",
    "        conn.commit()\n",
    "        logger.info(\"Todas las tablas creadas/verificadas exitosamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80918b40",
   "metadata": {},
   "source": [
    "# ## FUNCIONES DE GUARDADO EN ORACLE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64399f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def guardar_anomalias_oracle(anomalias_df, archivo_origen, lote_id):\n",
    "    \"\"\"Guarda las anomalías detectadas en Oracle\"\"\"\n",
    "    \n",
    "    if anomalias_df.empty:\n",
    "        return\n",
    "    \n",
    "    with get_oracle_connection() as conn:\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Preparar datos para inserción\n",
    "        datos_insertar = []\n",
    "        for _, row in anomalias_df.iterrows():\n",
    "            datos_insertar.append((\n",
    "                row['FECHA'],\n",
    "                int(row['CODIGODEPAIS']),\n",
    "                row['LINEA'],\n",
    "                int(row['N_LLAMADAS']),\n",
    "                float(row['N_MINUTOS']),\n",
    "                int(row['N_DESTINOS']),\n",
    "                float(row['score_anomalia']),\n",
    "                float(row['umbral']),\n",
    "                row['tipo_anomalia'],\n",
    "                row['tipo_contexto'],\n",
    "                row['razon_decision'],\n",
    "                archivo_origen,\n",
    "                lote_id\n",
    "            ))\n",
    "        \n",
    "        # Inserción masiva\n",
    "        insert_sql = \"\"\"\n",
    "            INSERT INTO ALERTAS_FRAUDE (\n",
    "                FECHA_REGISTRO, CODIGO_PAIS, LINEA, N_LLAMADAS, N_MINUTOS,\n",
    "                N_DESTINOS, SCORE_ANOMALIA, UMBRAL, TIPO_ANOMALIA,\n",
    "                TIPO_CONTEXTO, RAZON_DECISION, ARCHIVO_ORIGEN, LOTE_PROCESAMIENTO\n",
    "            ) VALUES (:1, :2, :3, :4, :5, :6, :7, :8, :9, :10, :11, :12, :13)\n",
    "        \"\"\"\n",
    "        \n",
    "        cursor.executemany(insert_sql, datos_insertar)\n",
    "        conn.commit()\n",
    "        \n",
    "        logger.info(f\"Se guardaron {len(datos_insertar)} anomalías en Oracle\")\n",
    "\n",
    "def registrar_lote_procesamiento(archivo_entrada, total_registros, total_anomalias, \n",
    "                               tasa_anomalias, archivo_salida, estado=\"COMPLETADO\"):\n",
    "    \"\"\"Registra información del lote procesado\"\"\"\n",
    "    \n",
    "    with get_oracle_connection() as conn:\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        insert_sql = \"\"\"\n",
    "            INSERT INTO LOTES_PROCESAMIENTO (\n",
    "                TIMESTAMP_INICIO, TIMESTAMP_FIN, ARCHIVO_ENTRADA, TOTAL_REGISTROS,\n",
    "                TOTAL_ANOMALIAS, TASA_ANOMALIAS, ARCHIVO_SALIDA, ESTADO\n",
    "            ) VALUES (\n",
    "                :1, :2, :3, :4, :5, :6, :7, :8\n",
    "            ) RETURNING ID_LOTE INTO :id_lote\n",
    "        \"\"\"\n",
    "        \n",
    "        id_lote = cursor.var(oracledb.NUMBER)\n",
    "        cursor.execute(insert_sql, [\n",
    "            datetime.now(),\n",
    "            datetime.now(),\n",
    "            archivo_entrada,\n",
    "            total_registros,\n",
    "            total_anomalias,\n",
    "            tasa_anomalias,\n",
    "            archivo_salida,\n",
    "            estado,\n",
    "            id_lote\n",
    "        ])\n",
    "        \n",
    "        conn.commit()\n",
    "        return id_lote.getvalue()[0]\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57075df",
   "metadata": {},
   "source": [
    "# ## 1. CONFIGURACIÓN Y MODELOS DE DATOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e87df5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de rutas\n",
    "MODELS_PATH = r\"C:\\Users\\User\\Desktop\\TESIS\\CodigoGithub\\MLTallerProyecto1\\Modelos\"\n",
    "OUTPUT_PATH = r\"C:\\Users\\User\\Desktop\\TESIS\\CodigoGithub\\MLTallerProyecto1\\Resultados_API\"\n",
    "TEMP_PATH = r\"C:\\Users\\User\\Desktop\\TESIS\\CodigoGithub\\MLTallerProyecto1\\Temp_APII\"\n",
    "LOG_PATH = r\"C:\\Users\\User\\Desktop\\TESIS\\CodigoGithub\\MLTallerProyecto1\\Logs_API\"\n",
    "\n",
    "# Crear directorios si no existen\n",
    "for path in [OUTPUT_PATH, TEMP_PATH, LOG_PATH]:\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "# Configuración del modelo\n",
    "modelo = None\n",
    "scaler = None\n",
    "config = None\n",
    "stats_dict = None\n",
    "contexto_historico = None\n",
    "umbral_global = None\n",
    "parametros_features = None\n",
    "\n",
    "# Pool de threads para operaciones pesadas\n",
    "executor = ThreadPoolExecutor(max_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4f1eda",
   "metadata": {},
   "source": [
    "# ## 2. MODELOS PYDANTIC PARA VALIDACIÓN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d4e15bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TipoAnomalia(str, Enum):\n",
    "    MINUTOS_EXTREMOS = \"MINUTOS_EXTREMOS\"\n",
    "    SPRAY_CALLING = \"SPRAY_CALLING\"\n",
    "    VOLUMEN_ALTO = \"VOLUMEN_ALTO\"\n",
    "    PAIS_BAJO_TRAFICO = \"PAIS_BAJO_TRAFICO\"\n",
    "    NO_ANOMALIA = \"NO_ANOMALIA\"\n",
    "\n",
    "class RegistroTelefonico(BaseModel):\n",
    "    fecha: str = Field(..., description=\"Fecha en formato DD/MM/YYYY\")\n",
    "    codigodepais: int = Field(..., description=\"Código del país como número entero\")  # Cambiado a int\n",
    "    linea: str = Field(..., description=\"Número de línea telefónica\")\n",
    "    n_llamadas: int = Field(..., ge=0, description=\"Número de llamadas\")\n",
    "    n_minutos: float = Field(..., ge=0, description=\"Minutos totales\")\n",
    "    n_destinos: int = Field(..., ge=0, description=\"Número de destinos únicos\")\n",
    "\n",
    "class ResultadoScoring(BaseModel):\n",
    "    fecha: str\n",
    "    codigodepais: int  # Cambiado a int\n",
    "    linea: str\n",
    "    n_llamadas: int\n",
    "    n_minutos: float\n",
    "    n_destinos: int\n",
    "    score_anomalia: float\n",
    "    umbral: float\n",
    "    es_anomalia: bool\n",
    "    tipo_anomalia: TipoAnomalia\n",
    "    tipo_contexto: str\n",
    "    razon_decision: str\n",
    "    timestamp_procesamiento: datetime\n",
    "\n",
    "class EstadisticasModelo(BaseModel):\n",
    "    fecha_entrenamiento: str\n",
    "    fecha_ultima_actualizacion: Optional[str]\n",
    "    umbral_global: float\n",
    "    n_trees: int\n",
    "    tree_height: int\n",
    "    registros_entrenamiento: int\n",
    "    paises_entrenamiento: int\n",
    "    paises_conocidos: int\n",
    "    version: str = \"1.0\"\n",
    "\n",
    "class ConfiguracionUmbral(BaseModel):\n",
    "    nuevo_umbral: float = Field(..., gt=0, le=1, description=\"Nuevo umbral entre 0 y 1\")\n",
    "    razon: str = Field(..., description=\"Razón del cambio\")\n",
    "\n",
    "class EstadoSistema(BaseModel):\n",
    "    estado: str\n",
    "    modelo_cargado: bool\n",
    "    ultimo_procesamiento: Optional[datetime]\n",
    "    registros_procesados_hoy: int\n",
    "    anomalias_detectadas_hoy: int\n",
    "    espacio_disco_gb: float\n",
    "    version_api: str = \"1.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99db8044",
   "metadata": {},
   "source": [
    "# ## 3. FUNCIONES AUXILIARES DEL MODELO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad7ab240",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargar_modelo():\n",
    "    \"\"\"Carga el modelo y configuración desde disco\"\"\"\n",
    "    global modelo, scaler, config, stats_dict, contexto_historico, umbral_global, parametros_features\n",
    "    \n",
    "    try:\n",
    "        # Cargar modelo\n",
    "        with open(os.path.join(MODELS_PATH, \"modelo_general.pkl\"), 'rb') as f:\n",
    "            modelo = pickle.load(f)\n",
    "        \n",
    "        # Cargar scaler\n",
    "        with open(os.path.join(MODELS_PATH, \"scaler_general.pkl\"), 'rb') as f:\n",
    "            scaler = pickle.load(f)\n",
    "        \n",
    "        # Cargar configuración\n",
    "        with open(os.path.join(MODELS_PATH, \"config_modelo_general.pkl\"), 'rb') as f:\n",
    "            config = pickle.load(f)\n",
    "        \n",
    "        # Extraer componentes\n",
    "        stats_dict = config['stats_por_pais']\n",
    "        contexto_historico = config.get('contexto_historico', {})\n",
    "        umbral_global = config['umbral_global']\n",
    "        parametros_features = config['parametros_features']\n",
    "        \n",
    "        logger.info(\"Modelo cargado exitosamente\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error al cargar modelo: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def crear_features_contextualizadas_mejorada(row, stats_pais_dict):\n",
    "    \"\"\"Crea features para el modelo (idéntica a la original)\"\"\"\n",
    "    pais = row['CODIGODEPAIS']\n",
    "    llamadas = row['N_LLAMADAS']\n",
    "    minutos = row['N_MINUTOS']\n",
    "    destinos = row['N_DESTINOS']\n",
    "    \n",
    "    PESO_MINUTOS_NORMAL = parametros_features['peso_minutos_normal']\n",
    "    PESO_MINUTOS_EXTREMOS = parametros_features['peso_minutos_extremos']\n",
    "    UMBRAL_MINUTOS_EXTREMOS = parametros_features['umbral_minutos_extremos']\n",
    "    PESO_DESTINOS = parametros_features['peso_destinos']\n",
    "    PESO_SPRAY_RATIO = parametros_features['peso_spray_ratio']\n",
    "    \n",
    "    if pais in stats_pais_dict:\n",
    "        pais_stats = stats_pais_dict[pais]\n",
    "        categoria = pais_stats['CATEGORIA']\n",
    "        \n",
    "        llamadas_norm = min(llamadas / max(pais_stats['LLAMADAS_P95'], 1), 1.5)\n",
    "        destinos_norm = min(destinos / max(pais_stats['DESTINOS_P95'], 1), 1.5)\n",
    "        \n",
    "        minutos_p90 = pais_stats.get('MINUTOS_P90', pais_stats['MINUTOS_P95'] * 0.9)\n",
    "        \n",
    "        if minutos >= UMBRAL_MINUTOS_EXTREMOS:\n",
    "            minutos_norm = min(minutos / max(minutos_p90, 1), 3.0)\n",
    "            peso_minutos = PESO_MINUTOS_EXTREMOS\n",
    "        else:\n",
    "            minutos_norm = min(np.log1p(minutos) / np.log1p(max(minutos_p90, 1)), 1.2)\n",
    "            peso_minutos = PESO_MINUTOS_NORMAL\n",
    "    else:\n",
    "        categoria = 'Muy_Bajo'\n",
    "        llamadas_norm = min(llamadas / 10, 2.0)\n",
    "        destinos_norm = min(destinos / 5, 2.0)\n",
    "        \n",
    "        if minutos >= UMBRAL_MINUTOS_EXTREMOS:\n",
    "            minutos_norm = min(minutos / 50, 3.0)\n",
    "            peso_minutos = PESO_MINUTOS_EXTREMOS * 1.2\n",
    "        else:\n",
    "            minutos_norm = min(np.log1p(minutos) / np.log1p(60), 1.2)\n",
    "            peso_minutos = PESO_MINUTOS_NORMAL\n",
    "    \n",
    "    features = {\n",
    "        'llamadas_norm': llamadas_norm * 0.8,\n",
    "        'destinos_norm': destinos_norm * PESO_DESTINOS,\n",
    "        'minutos_norm': minutos_norm * peso_minutos,\n",
    "        'diversidad_destinos': min(destinos / max(llamadas, 1), 1.0),\n",
    "        'spray_ratio': min(destinos / max(llamadas, 1) * PESO_SPRAY_RATIO, 1.0) if destinos >= 5 else 0,\n",
    "        'minutos_extremos': 1.0 if minutos >= UMBRAL_MINUTOS_EXTREMOS else 0.0,\n",
    "        'minutos_sospechosos': min((minutos - 200) / 300, 1.0) if minutos > 200 else 0.0,\n",
    "        'patron_spray_fuerte': 1.0 if (destinos >= 10 and llamadas >= 20) else 0.0,\n",
    "        'patron_spray_medio': 0.5 if (destinos >= 6 and llamadas >= 12) else 0.0,\n",
    "        'alta_diversidad': min(destinos / 12, 1) if destinos >= 5 else 0,\n",
    "        'volumen_llamadas_alto': min((llamadas - 30) / 50, 1) if llamadas > 30 else 0,\n",
    "        'volumen_destinos_alto': min((destinos - 10) / 20, 1) if destinos > 10 else 0,\n",
    "        'llamadas_por_destino': min(llamadas / max(destinos, 1) / 5, 1),\n",
    "        'eficiencia_destinos': min(destinos / max(llamadas * 0.5, 1), 1),\n",
    "        'factor_pais_bajo': 1.5 if categoria in ['Muy_Bajo', 'Bajo'] else 1.0,\n",
    "        'factor_pais_alto': 0.9 if categoria in ['Alto', 'Medio'] else 1.0\n",
    "    }\n",
    "    \n",
    "    return features\n",
    "\n",
    "def predecir_anomalia_individual(row_dict):\n",
    "    \"\"\"Realiza predicción para un registro individual\"\"\"\n",
    "    if not modelo:\n",
    "        raise ValueError(\"Modelo no cargado\")\n",
    "    \n",
    "    # Extraer valores al inicio para usar en todo el ámbito\n",
    "    # Convertir codigodepais a int si viene como string\n",
    "    pais = int(row_dict['CODIGODEPAIS']) if isinstance(row_dict['CODIGODEPAIS'], str) else row_dict['CODIGODEPAIS']\n",
    "    llamadas = row_dict['N_LLAMADAS']\n",
    "    minutos = row_dict['N_MINUTOS']\n",
    "    destinos = row_dict['N_DESTINOS']\n",
    "    \n",
    "    # Actualizar row_dict con el valor convertido\n",
    "    row_dict['CODIGODEPAIS'] = pais\n",
    "    \n",
    "    # Crear features\n",
    "    features = crear_features_contextualizadas_mejorada(row_dict, stats_dict)\n",
    "    \n",
    "    # Normalizar\n",
    "    features_scaled = scaler.transform_one(features)\n",
    "    \n",
    "    # Obtener score\n",
    "    score = modelo.score_one(features_scaled)\n",
    "    \n",
    "    # Lógica de decisión\n",
    "    es_anomalia_base = score > umbral_global\n",
    "    \n",
    "    if es_anomalia_base:\n",
    "        if minutos >= parametros_features['umbral_minutos_extremos']:\n",
    "            es_anomalia_final = True\n",
    "            razon = f\"Minutos extremos ({minutos:.1f} min)\"\n",
    "            tipo_anomalia = TipoAnomalia.MINUTOS_EXTREMOS\n",
    "        elif destinos >= 6 and llamadas >= 12:\n",
    "            es_anomalia_final = True\n",
    "            razon = \"Patrón de spray calling confirmado\"\n",
    "            tipo_anomalia = TipoAnomalia.SPRAY_CALLING\n",
    "        elif llamadas > 50 or destinos > 15:\n",
    "            es_anomalia_final = True\n",
    "            razon = \"Volumen excepcionalmente alto\"\n",
    "            tipo_anomalia = TipoAnomalia.VOLUMEN_ALTO\n",
    "        elif pais not in stats_dict or stats_dict.get(pais, {}).get('CATEGORIA') in ['Muy_Bajo', 'Bajo']:\n",
    "            if destinos >= 4 and llamadas >= 8:\n",
    "                es_anomalia_final = True\n",
    "                razon = \"Actividad sospechosa en país de bajo tráfico\"\n",
    "                tipo_anomalia = TipoAnomalia.PAIS_BAJO_TRAFICO\n",
    "            else:\n",
    "                es_anomalia_final = False\n",
    "                razon = \"Actividad baja en país de bajo tráfico\"\n",
    "                tipo_anomalia = TipoAnomalia.NO_ANOMALIA\n",
    "        else:\n",
    "            es_anomalia_final = False\n",
    "            razon = \"No cumple criterios de confirmación\"\n",
    "            tipo_anomalia = TipoAnomalia.NO_ANOMALIA\n",
    "    else:\n",
    "        es_anomalia_final = False\n",
    "        razon = \"Score bajo umbral\"\n",
    "        tipo_anomalia = TipoAnomalia.NO_ANOMALIA\n",
    "    \n",
    "    # Determinar contexto\n",
    "    if contexto_historico and pais in contexto_historico:\n",
    "        tipo_contexto = contexto_historico[pais]\n",
    "    elif pais in stats_dict:\n",
    "        tipo_contexto = stats_dict[pais]['CATEGORIA']\n",
    "    else:\n",
    "        tipo_contexto = \"Muy_Bajo\"\n",
    "    \n",
    "    return {\n",
    "        'score': score,\n",
    "        'umbral': umbral_global,\n",
    "        'es_anomalia': es_anomalia_final,\n",
    "        'tipo_anomalia': tipo_anomalia,\n",
    "        'tipo_contexto': tipo_contexto,\n",
    "        'razon_decision': razon\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b00e146",
   "metadata": {},
   "source": [
    "# ## 4. INICIALIZACIÓN DE LA API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b17c77e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear aplicación FastAPI\n",
    "app = FastAPI(\n",
    "    title=\"API de Detección de Fraude Telefónico\",\n",
    "    description=\"Sistema de detección de anomalías en llamadas telefónicas usando Machine Learning\",\n",
    "    version=\"1.0\",\n",
    "    docs_url=\"/docs\",\n",
    "    redoc_url=\"/redoc\"\n",
    ")\n",
    "\n",
    "# Configurar CORS\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "@app.on_event(\"startup\")\n",
    "async def startup_event():\n",
    "    \"\"\"Carga el modelo e inicializa Oracle al iniciar la API\"\"\"\n",
    "    logger.info(\"Iniciando API...\")\n",
    "    \n",
    "    # Cargar modelo\n",
    "    modelo_cargado = cargar_modelo()\n",
    "    \n",
    "    # Inicializar Oracle\n",
    "    oracle_iniciado = False\n",
    "    try:\n",
    "        oracle_iniciado = inicializar_oracle_pool()\n",
    "        if oracle_iniciado:\n",
    "            crear_tablas_oracle()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error con Oracle: {e}\")\n",
    "        logger.info(\"La API funcionará sin Oracle\")\n",
    "    \n",
    "    if modelo_cargado:\n",
    "        logger.info(f\"API iniciada - Modelo: OK, Oracle: {'OK' if oracle_iniciado else 'No disponible'}\")\n",
    "    else:\n",
    "        logger.error(\"Error al iniciar API - Modelo no cargado\")\n",
    "\n",
    "@app.on_event(\"shutdown\")\n",
    "async def shutdown_event():\n",
    "    \"\"\"Cierra el pool de Oracle al detener la API\"\"\"\n",
    "    global oracle_pool\n",
    "    if oracle_pool:\n",
    "        oracle_pool.close()\n",
    "        logger.info(\"Pool de Oracle cerrado\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfa018c",
   "metadata": {},
   "source": [
    "# ## 5. ENDPOINTS DE SCORING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ad7b128",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.get(\"/\", tags=[\"General\"])\n",
    "async def root():\n",
    "    \"\"\"Endpoint raíz con información de la API\"\"\"\n",
    "    return {\n",
    "        \"mensaje\": \"API de Detección de Fraude Telefónico\",\n",
    "        \"version\": \"1.0\",\n",
    "        \"documentacion\": \"/docs\",\n",
    "        \"estado\": \"activo\" if modelo else \"modelo no cargado\"\n",
    "    }\n",
    "\n",
    "@app.post(\"/scoring/individual\", response_model=ResultadoScoring, tags=[\"Scoring\"])\n",
    "async def scoring_individual(registro: RegistroTelefonico):\n",
    "    \"\"\"\n",
    "    Realiza scoring de un registro individual\n",
    "    \"\"\"\n",
    "    if not modelo:\n",
    "        raise HTTPException(status_code=503, detail=\"Modelo no disponible\")\n",
    "    \n",
    "    try:\n",
    "        # Preparar datos\n",
    "        row_dict = {\n",
    "            'CODIGODEPAIS': registro.codigodepais,\n",
    "            'N_LLAMADAS': registro.n_llamadas,\n",
    "            'N_MINUTOS': registro.n_minutos,\n",
    "            'N_DESTINOS': registro.n_destinos\n",
    "        }\n",
    "        \n",
    "        # Realizar predicción\n",
    "        resultado = predecir_anomalia_individual(row_dict)\n",
    "        \n",
    "        # Construir respuesta\n",
    "        return ResultadoScoring(\n",
    "            fecha=registro.fecha,\n",
    "            codigodepais=registro.codigodepais,\n",
    "            linea=registro.linea,\n",
    "            n_llamadas=registro.n_llamadas,\n",
    "            n_minutos=registro.n_minutos,\n",
    "            n_destinos=registro.n_destinos,\n",
    "            score_anomalia=round(resultado['score'], 4),\n",
    "            umbral=round(resultado['umbral'], 4),\n",
    "            es_anomalia=resultado['es_anomalia'],\n",
    "            tipo_anomalia=resultado['tipo_anomalia'],\n",
    "            tipo_contexto=resultado['tipo_contexto'],\n",
    "            razon_decision=resultado['razon_decision'],\n",
    "            timestamp_procesamiento=datetime.now()\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error en scoring individual: {str(e)}\")\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.post(\"/scoring/batch\", tags=[\"Scoring\"])\n",
    "async def scoring_batch(file: UploadFile = File(...)):\n",
    "    \"\"\"\n",
    "    Realiza scoring de un archivo CSV con múltiples registros\n",
    "    y guarda anomalías en Oracle\n",
    "    \"\"\"\n",
    "    if not modelo:\n",
    "        raise HTTPException(status_code=503, detail=\"Modelo no disponible\")\n",
    "    \n",
    "    lote_id = None\n",
    "    temp_file = None\n",
    "    \n",
    "    try:\n",
    "        # Guardar archivo temporal\n",
    "        temp_file = os.path.join(TEMP_PATH, f\"temp_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{file.filename}\")\n",
    "        \n",
    "        with open(temp_file, \"wb\") as f:\n",
    "            content = await file.read()\n",
    "            f.write(content)\n",
    "        \n",
    "        # Leer CSV\n",
    "        df = pd.read_csv(temp_file)\n",
    "        \n",
    "        # Convertir CODIGODEPAIS a int si es necesario\n",
    "        if 'CODIGODEPAIS' in df.columns:\n",
    "            df['CODIGODEPAIS'] = pd.to_numeric(df['CODIGODEPAIS'], errors='coerce').astype('Int64')\n",
    "        \n",
    "        # Validar columnas\n",
    "        columnas_requeridas = ['FECHA', 'CODIGODEPAIS', 'LINEA', 'N_LLAMADAS', 'N_MINUTOS', 'N_DESTINOS']\n",
    "        columnas_faltantes = [col for col in columnas_requeridas if col not in df.columns]\n",
    "        \n",
    "        if columnas_faltantes:\n",
    "            os.remove(temp_file)\n",
    "            raise HTTPException(status_code=400, detail=f\"Columnas faltantes: {columnas_faltantes}\")\n",
    "        \n",
    "        # Procesar registros\n",
    "        resultados = []\n",
    "        anomalias_detectadas = 0\n",
    "        \n",
    "        for idx, row in df.iterrows():\n",
    "            # Saltar filas con CODIGODEPAIS nulo\n",
    "            if pd.isna(row['CODIGODEPAIS']):\n",
    "                continue\n",
    "                \n",
    "            row_dict = {\n",
    "                'CODIGODEPAIS': int(row['CODIGODEPAIS']),\n",
    "                'N_LLAMADAS': row['N_LLAMADAS'],\n",
    "                'N_MINUTOS': row['N_MINUTOS'],\n",
    "                'N_DESTINOS': row['N_DESTINOS']\n",
    "            }\n",
    "            \n",
    "            resultado = predecir_anomalia_individual(row_dict)\n",
    "            \n",
    "            resultado_completo = {\n",
    "                'FECHA': row['FECHA'],\n",
    "                'CODIGODEPAIS': int(row['CODIGODEPAIS']),\n",
    "                'LINEA': row['LINEA'],\n",
    "                'N_LLAMADAS': row['N_LLAMADAS'],\n",
    "                'N_MINUTOS': row['N_MINUTOS'],\n",
    "                'N_DESTINOS': row['N_DESTINOS'],\n",
    "                'score_anomalia': round(resultado['score'], 4),\n",
    "                'umbral': round(resultado['umbral'], 4),\n",
    "                'es_anomalia': resultado['es_anomalia'],\n",
    "                'tipo_anomalia': resultado['tipo_anomalia'].value,\n",
    "                'tipo_contexto': resultado['tipo_contexto'],\n",
    "                'razon_decision': resultado['razon_decision'],\n",
    "                'timestamp_procesamiento': datetime.now()\n",
    "            }\n",
    "            \n",
    "            resultados.append(resultado_completo)\n",
    "            if resultado['es_anomalia']:\n",
    "                anomalias_detectadas += 1\n",
    "        \n",
    "        # Guardar resultados en CSV\n",
    "        df_resultados = pd.DataFrame(resultados)\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        archivo_salida = os.path.join(OUTPUT_PATH, f\"resultados_batch_{timestamp}.csv\")\n",
    "        df_resultados.to_csv(archivo_salida, index=False)\n",
    "        \n",
    "        # NUEVO: Guardar en Oracle si está disponible\n",
    "        if oracle_pool:\n",
    "            try:\n",
    "                # Registrar lote\n",
    "                lote_id = registrar_lote_procesamiento(\n",
    "                    archivo_entrada=file.filename,\n",
    "                    total_registros=len(resultados),\n",
    "                    total_anomalias=anomalias_detectadas,\n",
    "                    tasa_anomalias=round(anomalias_detectadas/len(resultados)*100, 2) if len(resultados) > 0 else 0,\n",
    "                    archivo_salida=os.path.basename(archivo_salida)\n",
    "                )\n",
    "                \n",
    "                # Guardar solo las anomalías\n",
    "                df_anomalias = df_resultados[df_resultados['es_anomalia'] == True]\n",
    "                if not df_anomalias.empty:\n",
    "                    guardar_anomalias_oracle(df_anomalias, file.filename, str(lote_id))\n",
    "                    \n",
    "                logger.info(f\"Datos guardados en Oracle - Lote ID: {lote_id}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error guardando en Oracle: {e}\")\n",
    "                # No fallar el proceso si Oracle falla\n",
    "        \n",
    "        # Limpiar archivo temporal\n",
    "        os.remove(temp_file)\n",
    "        \n",
    "        # Respuesta\n",
    "        return {\n",
    "            \"mensaje\": \"Procesamiento completado\",\n",
    "            \"registros_procesados\": len(resultados),\n",
    "            \"anomalias_detectadas\": anomalias_detectadas,\n",
    "            \"tasa_anomalias\": round(anomalias_detectadas/len(resultados)*100, 2) if len(resultados) > 0 else 0,\n",
    "            \"archivo_resultados\": os.path.basename(archivo_salida),\n",
    "            \"lote_id\": lote_id,\n",
    "            \"timestamp\": datetime.now(),\n",
    "            \"guardado_oracle\": oracle_pool is not None\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error en scoring batch: {str(e)}\")\n",
    "        if temp_file and os.path.exists(temp_file):\n",
    "            os.remove(temp_file)\n",
    "        \n",
    "        # Registrar error en Oracle si es posible\n",
    "        if oracle_pool and lote_id is None:\n",
    "            try:\n",
    "                with get_oracle_connection() as conn:\n",
    "                    cursor = conn.cursor()\n",
    "                    cursor.execute(\"\"\"\n",
    "                        INSERT INTO LOTES_PROCESAMIENTO \n",
    "                        (TIMESTAMP_INICIO, ARCHIVO_ENTRADA, ESTADO, MENSAJE_ERROR)\n",
    "                        VALUES (:1, :2, :3, :4)\n",
    "                    \"\"\", [datetime.now(), file.filename, \"ERROR\", str(e)])\n",
    "                    conn.commit()\n",
    "            except:\n",
    "                pass\n",
    "                \n",
    "        raise HTTPException(status_code=500, detail=str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6ec712",
   "metadata": {},
   "source": [
    "# ## NUEVO ENDPOINT CONSULTAS ORACLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28f694c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.get(\"/alertas/consultar\", tags=[\"Alertas Oracle\"])\n",
    "async def consultar_alertas(\n",
    "    fecha_inicio: Optional[str] = Query(None, description=\"Fecha inicio YYYY-MM-DD\"),\n",
    "    fecha_fin: Optional[str] = Query(None, description=\"Fecha fin YYYY-MM-DD\"),\n",
    "    codigo_pais: Optional[int] = Query(None, description=\"Código de país\"),\n",
    "    tipo_anomalia: Optional[str] = Query(None, description=\"Tipo de anomalía\"),\n",
    "    limite: int = Query(100, description=\"Número máximo de registros\")\n",
    "):\n",
    "    \"\"\"\n",
    "    Consulta las alertas guardadas en Oracle con filtros opcionales\n",
    "    \"\"\"\n",
    "    if not oracle_pool:\n",
    "        raise HTTPException(status_code=503, detail=\"Oracle no disponible\")\n",
    "    \n",
    "    try:\n",
    "        with get_oracle_connection() as conn:\n",
    "            cursor = conn.cursor()\n",
    "            \n",
    "            # Construir query dinámicamente\n",
    "            query = \"\"\"\n",
    "                SELECT ID_ALERTA, FECHA_PROCESAMIENTO, FECHA_REGISTRO, CODIGO_PAIS, \n",
    "                       LINEA, N_LLAMADAS, N_MINUTOS, N_DESTINOS, SCORE_ANOMALIA,\n",
    "                       UMBRAL, TIPO_ANOMALIA, TIPO_CONTEXTO, RAZON_DECISION\n",
    "                FROM ALERTAS_FRAUDE\n",
    "                WHERE 1=1\n",
    "            \"\"\"\n",
    "            params = []\n",
    "            \n",
    "            if fecha_inicio:\n",
    "                query += \" AND FECHA_PROCESAMIENTO >= TO_DATE(:fecha_inicio, 'YYYY-MM-DD')\"\n",
    "                params.append(fecha_inicio)\n",
    "            \n",
    "            if fecha_fin:\n",
    "                query += \" AND FECHA_PROCESAMIENTO <= TO_DATE(:fecha_fin, 'YYYY-MM-DD') + 1\"\n",
    "                params.append(fecha_fin)\n",
    "            \n",
    "            if codigo_pais:\n",
    "                query += \" AND CODIGO_PAIS = :codigo_pais\"\n",
    "                params.append(codigo_pais)\n",
    "            \n",
    "            if tipo_anomalia:\n",
    "                query += \" AND TIPO_ANOMALIA = :tipo_anomalia\"\n",
    "                params.append(tipo_anomalia)\n",
    "            \n",
    "            query += \" ORDER BY FECHA_PROCESAMIENTO DESC\"\n",
    "            query += f\" FETCH FIRST {limite} ROWS ONLY\"\n",
    "            \n",
    "            cursor.execute(query, params)\n",
    "            \n",
    "            # Obtener nombres de columnas\n",
    "            columnas = [col[0] for col in cursor.description]\n",
    "            \n",
    "            # Convertir a lista de diccionarios\n",
    "            alertas = []\n",
    "            for row in cursor:\n",
    "                alerta = dict(zip(columnas, row))\n",
    "                # Convertir datetime a string\n",
    "                if alerta['FECHA_PROCESAMIENTO']:\n",
    "                    alerta['FECHA_PROCESAMIENTO'] = alerta['FECHA_PROCESAMIENTO'].isoformat()\n",
    "                alertas.append(alerta)\n",
    "            \n",
    "            return {\n",
    "                \"total\": len(alertas),\n",
    "                \"alertas\": alertas\n",
    "            }\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error consultando alertas: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.get(\"/alertas/estadisticas\", tags=[\"Alertas Oracle\"])\n",
    "async def estadisticas_alertas(\n",
    "    fecha_inicio: str = Query(..., description=\"Fecha inicio YYYY-MM-DD\"),\n",
    "    fecha_fin: str = Query(..., description=\"Fecha fin YYYY-MM-DD\")\n",
    "):\n",
    "    \"\"\"\n",
    "    Obtiene estadísticas de las alertas en un rango de fechas\n",
    "    \"\"\"\n",
    "    if not oracle_pool:\n",
    "        raise HTTPException(status_code=503, detail=\"Oracle no disponible\")\n",
    "    \n",
    "    try:\n",
    "        with get_oracle_connection() as conn:\n",
    "            cursor = conn.cursor()\n",
    "            \n",
    "            # Estadísticas generales\n",
    "            cursor.execute(\"\"\"\n",
    "                SELECT \n",
    "                    COUNT(*) as total_alertas,\n",
    "                    COUNT(DISTINCT CODIGO_PAIS) as paises_afectados,\n",
    "                    COUNT(DISTINCT LINEA) as lineas_afectadas,\n",
    "                    AVG(SCORE_ANOMALIA) as score_promedio,\n",
    "                    MAX(SCORE_ANOMALIA) as score_maximo\n",
    "                FROM ALERTAS_FRAUDE\n",
    "                WHERE FECHA_PROCESAMIENTO >= TO_DATE(:1, 'YYYY-MM-DD')\n",
    "                  AND FECHA_PROCESAMIENTO <= TO_DATE(:2, 'YYYY-MM-DD') + 1\n",
    "            \"\"\", [fecha_inicio, fecha_fin])\n",
    "            \n",
    "            stats_generales = dict(zip(\n",
    "                ['total_alertas', 'paises_afectados', 'lineas_afectadas', \n",
    "                 'score_promedio', 'score_maximo'],\n",
    "                cursor.fetchone()\n",
    "            ))\n",
    "            \n",
    "            # Distribución por tipo\n",
    "            cursor.execute(\"\"\"\n",
    "                SELECT TIPO_ANOMALIA, COUNT(*) as cantidad\n",
    "                FROM ALERTAS_FRAUDE\n",
    "                WHERE FECHA_PROCESAMIENTO >= TO_DATE(:1, 'YYYY-MM-DD')\n",
    "                  AND FECHA_PROCESAMIENTO <= TO_DATE(:2, 'YYYY-MM-DD') + 1\n",
    "                GROUP BY TIPO_ANOMALIA\n",
    "                ORDER BY cantidad DESC\n",
    "            \"\"\", [fecha_inicio, fecha_fin])\n",
    "            \n",
    "            distribucion_tipos = [\n",
    "                {\"tipo\": row[0], \"cantidad\": row[1]}\n",
    "                for row in cursor.fetchall()\n",
    "            ]\n",
    "            \n",
    "            # Top países\n",
    "            cursor.execute(\"\"\"\n",
    "                SELECT CODIGO_PAIS, COUNT(*) as cantidad\n",
    "                FROM ALERTAS_FRAUDE\n",
    "                WHERE FECHA_PROCESAMIENTO >= TO_DATE(:1, 'YYYY-MM-DD')\n",
    "                  AND FECHA_PROCESAMIENTO <= TO_DATE(:2, 'YYYY-MM-DD') + 1\n",
    "                GROUP BY CODIGO_PAIS\n",
    "                ORDER BY cantidad DESC\n",
    "                FETCH FIRST 10 ROWS ONLY\n",
    "            \"\"\", [fecha_inicio, fecha_fin])\n",
    "            \n",
    "            top_paises = [\n",
    "                {\"codigo_pais\": row[0], \"cantidad\": row[1]}\n",
    "                for row in cursor.fetchall()\n",
    "            ]\n",
    "            \n",
    "            # Tendencia diaria\n",
    "            cursor.execute(\"\"\"\n",
    "                SELECT \n",
    "                    TO_CHAR(FECHA_PROCESAMIENTO, 'YYYY-MM-DD') as fecha,\n",
    "                    COUNT(*) as cantidad\n",
    "                FROM ALERTAS_FRAUDE\n",
    "                WHERE FECHA_PROCESAMIENTO >= TO_DATE(:1, 'YYYY-MM-DD')\n",
    "                  AND FECHA_PROCESAMIENTO <= TO_DATE(:2, 'YYYY-MM-DD') + 1\n",
    "                GROUP BY TO_CHAR(FECHA_PROCESAMIENTO, 'YYYY-MM-DD')\n",
    "                ORDER BY fecha\n",
    "            \"\"\", [fecha_inicio, fecha_fin])\n",
    "            \n",
    "            tendencia_diaria = [\n",
    "                {\"fecha\": row[0], \"cantidad\": row[1]}\n",
    "                for row in cursor.fetchall()\n",
    "            ]\n",
    "            \n",
    "            return {\n",
    "                \"periodo\": {\n",
    "                    \"fecha_inicio\": fecha_inicio,\n",
    "                    \"fecha_fin\": fecha_fin\n",
    "                },\n",
    "                \"estadisticas_generales\": stats_generales,\n",
    "                \"distribucion_tipos\": distribucion_tipos,\n",
    "                \"top_paises\": top_paises,\n",
    "                \"tendencia_diaria\": tendencia_diaria\n",
    "            }\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error obteniendo estadísticas: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.get(\"/lotes/historial\", tags=[\"Alertas Oracle\"])\n",
    "async def historial_lotes(limite: int = Query(50, description=\"Número de lotes a mostrar\")):\n",
    "    \"\"\"\n",
    "    Obtiene el historial de lotes procesados\n",
    "    \"\"\"\n",
    "    if not oracle_pool:\n",
    "        raise HTTPException(status_code=503, detail=\"Oracle no disponible\")\n",
    "    \n",
    "    try:\n",
    "        with get_oracle_connection() as conn:\n",
    "            cursor = conn.cursor()\n",
    "            \n",
    "            cursor.execute(\"\"\"\n",
    "                SELECT ID_LOTE, TIMESTAMP_INICIO, TIMESTAMP_FIN, ARCHIVO_ENTRADA,\n",
    "                       TOTAL_REGISTROS, TOTAL_ANOMALIAS, TASA_ANOMALIAS,\n",
    "                       ARCHIVO_SALIDA, ESTADO\n",
    "                FROM LOTES_PROCESAMIENTO\n",
    "                ORDER BY TIMESTAMP_INICIO DESC\n",
    "                FETCH FIRST :limite ROWS ONLY\n",
    "            \"\"\", [limite])\n",
    "            \n",
    "            columnas = [col[0] for col in cursor.description]\n",
    "            lotes = []\n",
    "            \n",
    "            for row in cursor:\n",
    "                lote = dict(zip(columnas, row))\n",
    "                # Convertir timestamps\n",
    "                if lote['TIMESTAMP_INICIO']:\n",
    "                    lote['TIMESTAMP_INICIO'] = lote['TIMESTAMP_INICIO'].isoformat()\n",
    "                if lote['TIMESTAMP_FIN']:\n",
    "                    lote['TIMESTAMP_FIN'] = lote['TIMESTAMP_FIN'].isoformat()\n",
    "                lotes.append(lote)\n",
    "            \n",
    "            return {\n",
    "                \"total\": len(lotes),\n",
    "                \"lotes\": lotes\n",
    "            }\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error obteniendo historial de lotes: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c0761c",
   "metadata": {},
   "source": [
    "# ##FUNCIONES PARA APRENDISAJE INCREMENTAL / ORACLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f53d0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determinar_categoria_pais(stats):\n",
    "    \"\"\"Determina la categoría de un país basado en sus estadísticas\"\"\"\n",
    "    # Basado en el promedio de llamadas\n",
    "    llamadas_mean = stats.get('LLAMADAS_MEAN', 0)\n",
    "    \n",
    "    if llamadas_mean >= 50:\n",
    "        return 'Alto'\n",
    "    elif llamadas_mean >= 20:\n",
    "        return 'Medio'\n",
    "    elif llamadas_mean >= 10:\n",
    "        return 'Bajo'\n",
    "    else:\n",
    "        return 'Muy_Bajo'\n",
    "\n",
    "def calcular_umbral_adaptativo(modelo, scaler, df_muestra, stats_dict):\n",
    "    \"\"\"Calcula un umbral adaptativo basado en los scores actuales\"\"\"\n",
    "    scores = []\n",
    "    \n",
    "    # Tomar una muestra aleatoria\n",
    "    muestra = df_muestra.sample(min(1000, len(df_muestra)))\n",
    "    \n",
    "    for _, row in muestra.iterrows():\n",
    "        row_dict = {\n",
    "            'CODIGODEPAIS': int(row['CODIGODEPAIS']) if not pd.isna(row['CODIGODEPAIS']) else 0,\n",
    "            'N_LLAMADAS': row['N_LLAMADAS'],\n",
    "            'N_MINUTOS': row['N_MINUTOS'],\n",
    "            'N_DESTINOS': row['N_DESTINOS']\n",
    "        }\n",
    "        \n",
    "        features = crear_features_contextualizadas_mejorada(row_dict, stats_dict)\n",
    "        features_scaled = scaler.transform_one(features)\n",
    "        score = modelo.score_one(features_scaled)\n",
    "        scores.append(score)\n",
    "    \n",
    "    # Nuevo umbral = percentil 95 de los scores\n",
    "    return np.percentile(scores, 95)\n",
    "\n",
    "def registrar_inicio_aprendizaje(archivo_path, total_registros, estado_inicial):\n",
    "    \"\"\"Registra el inicio del aprendizaje incremental en Oracle\"\"\"\n",
    "    with get_oracle_connection() as conn:\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Contar países únicos en el archivo\n",
    "        try:\n",
    "            df_temp = pd.read_csv(archivo_path)\n",
    "            paises_en_archivo = df_temp['CODIGODEPAIS'].nunique()\n",
    "        except:\n",
    "            paises_en_archivo = 0\n",
    "        \n",
    "        id_var = cursor.var(oracledb.NUMBER)\n",
    "        \n",
    "        cursor.execute(\"\"\"\n",
    "            INSERT INTO APRENDIZAJE_INCREMENTAL (\n",
    "                FECHA_DATOS, ARCHIVO_ENTRADA, TOTAL_REGISTROS,\n",
    "                PAISES_TOTALES, UMBRAL_ANTERIOR, N_TREES_ANTERIOR,\n",
    "                PAISES_CONOCIDOS_ANTERIOR, ESTADO\n",
    "            ) VALUES (\n",
    "                TRUNC(SYSDATE), :1, :2, :3, :4, :5, :6, 'EN_PROCESO'\n",
    "            ) RETURNING ID_APRENDIZAJE INTO :id\n",
    "        \"\"\", [\n",
    "            os.path.basename(archivo_path),\n",
    "            total_registros,\n",
    "            paises_en_archivo,\n",
    "            estado_inicial['umbral'],\n",
    "            estado_inicial['n_trees'],\n",
    "            estado_inicial['paises_conocidos'],\n",
    "            id_var\n",
    "        ])\n",
    "        \n",
    "        conn.commit()\n",
    "        return id_var.getvalue()[0]\n",
    "\n",
    "def actualizar_fin_aprendizaje(id_aprendizaje, **kwargs):\n",
    "    \"\"\"Actualiza el registro al finalizar el aprendizaje\"\"\"\n",
    "    with get_oracle_connection() as conn:\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Preparar JSON de cambios de categoría\n",
    "        cambios_json = json.dumps(kwargs.get('cambios_categoria', {}))\n",
    "        \n",
    "        cursor.execute(\"\"\"\n",
    "            UPDATE APRENDIZAJE_INCREMENTAL SET\n",
    "                REGISTROS_PROCESADOS = :1,\n",
    "                PAISES_NUEVOS = :2,\n",
    "                PAISES_ACTUALIZADOS = :3,\n",
    "                UMBRAL_NUEVO = :4,\n",
    "                N_TREES_NUEVO = :5,\n",
    "                PAISES_CONOCIDOS_NUEVO = :6,\n",
    "                PAISES_CON_CAMBIO_CATEGORIA = :7,\n",
    "                DETALLE_CAMBIOS_CATEGORIA = :8,\n",
    "                TIEMPO_PROCESAMIENTO_SEG = :9,\n",
    "                ESTADO = :10,\n",
    "                ARCHIVO_BACKUP_CONFIG = :11,\n",
    "                LLAMADAS_PROMEDIO_GLOBAL = :12,\n",
    "                MINUTOS_PROMEDIO_GLOBAL = :13,\n",
    "                DESTINOS_PROMEDIO_GLOBAL = :14,\n",
    "                UMBRAL_MINUTOS_EXTREMOS = :15,\n",
    "                PESO_SPRAY_RATIO = :16\n",
    "            WHERE ID_APRENDIZAJE = :17\n",
    "        \"\"\", [\n",
    "            kwargs['registros_procesados'],\n",
    "            kwargs['paises_nuevos'],\n",
    "            kwargs['paises_actualizados'],\n",
    "            kwargs['umbral_nuevo'],\n",
    "            config.get('n_trees', 0),\n",
    "            len(stats_dict),\n",
    "            len(kwargs.get('cambios_categoria', {})),\n",
    "            cambios_json,\n",
    "            kwargs['tiempo_procesamiento'],\n",
    "            kwargs['estado'],\n",
    "            os.path.basename(kwargs['backup_path']),\n",
    "            kwargs['promedios_globales']['llamadas'],\n",
    "            kwargs['promedios_globales']['minutos'],\n",
    "            kwargs['promedios_globales']['destinos'],\n",
    "            parametros_features.get('umbral_minutos_extremos', 0),\n",
    "            parametros_features.get('peso_spray_ratio', 0),\n",
    "            id_aprendizaje\n",
    "        ])\n",
    "        \n",
    "        conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde76305",
   "metadata": {},
   "source": [
    "# ## 6. ENDPOINTS DE APRENDIZAJE INCREMENTAL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2352c12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.post(\"/modelo/aprendizaje-incremental\", tags=[\"Modelo\"])\n",
    "async def aprendizaje_incremental(\n",
    "    file: UploadFile = File(...),\n",
    "    background_tasks: BackgroundTasks = BackgroundTasks()\n",
    "):\n",
    "    \"\"\"\n",
    "    Realiza aprendizaje incremental con nuevos datos\n",
    "    \"\"\"\n",
    "    if not modelo:\n",
    "        raise HTTPException(status_code=503, detail=\"Modelo no disponible\")\n",
    "    \n",
    "    try:\n",
    "        # Guardar archivo temporal\n",
    "        temp_file = os.path.join(TEMP_PATH, f\"incremental_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{file.filename}\")\n",
    "        \n",
    "        with open(temp_file, \"wb\") as f:\n",
    "            content = await file.read()\n",
    "            f.write(content)\n",
    "        \n",
    "        # Validar archivo\n",
    "        df = pd.read_csv(temp_file)\n",
    "        \n",
    "        # Agregar tarea en background\n",
    "        background_tasks.add_task(procesar_aprendizaje_incremental, temp_file, df)\n",
    "        \n",
    "        return {\n",
    "            \"mensaje\": \"Aprendizaje incremental iniciado en background\",\n",
    "            \"registros_a_procesar\": len(df),\n",
    "            \"timestamp\": datetime.now()\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error iniciando aprendizaje incremental: {str(e)}\")\n",
    "        if os.path.exists(temp_file):\n",
    "            os.remove(temp_file)\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "def procesar_aprendizaje_incremental(archivo_path: str, df: pd.DataFrame):\n",
    "    \"\"\"Procesa el aprendizaje incremental en background y registra en Oracle\"\"\"\n",
    "    global stats_dict, umbral_global\n",
    "    \n",
    "    # Variables para tracking\n",
    "    tiempo_inicio = datetime.now()\n",
    "    id_aprendizaje = None\n",
    "    estado_inicial = {\n",
    "        'umbral': umbral_global,\n",
    "        'n_trees': config.get('n_trees', 0),\n",
    "        'paises_conocidos': len(stats_dict),\n",
    "        'categorias': {pais: stats.get('CATEGORIA', 'Desconocido') \n",
    "                      for pais, stats in stats_dict.items()}\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        logger.info(f\"Iniciando aprendizaje incremental con {len(df)} registros\")\n",
    "        \n",
    "        # Registrar inicio en Oracle\n",
    "        if oracle_pool:\n",
    "            id_aprendizaje = registrar_inicio_aprendizaje(\n",
    "                archivo_path, len(df), estado_inicial\n",
    "            )\n",
    "        \n",
    "        registros_procesados = 0\n",
    "        paises_nuevos = set()\n",
    "        paises_actualizados = set()\n",
    "        cambios_categoria = {}\n",
    "        \n",
    "        # Procesar cada registro\n",
    "        for idx, row in df.iterrows():\n",
    "            row_dict = {\n",
    "                'CODIGODEPAIS': int(row['CODIGODEPAIS']) if not pd.isna(row['CODIGODEPAIS']) else 0,\n",
    "                'N_LLAMADAS': row['N_LLAMADAS'],\n",
    "                'N_MINUTOS': row['N_MINUTOS'],\n",
    "                'N_DESTINOS': row['N_DESTINOS']\n",
    "            }\n",
    "            \n",
    "            # Crear features\n",
    "            features = crear_features_contextualizadas_mejorada(row_dict, stats_dict)\n",
    "            \n",
    "            # Actualizar scaler\n",
    "            scaler.learn_one(features)\n",
    "            features_scaled = scaler.transform_one(features)\n",
    "            \n",
    "            # Aprender\n",
    "            modelo.learn_one(features_scaled)\n",
    "            \n",
    "            # Detectar países nuevos\n",
    "            pais = row_dict['CODIGODEPAIS']\n",
    "            if pais not in stats_dict:\n",
    "                paises_nuevos.add(pais)\n",
    "            \n",
    "            registros_procesados += 1\n",
    "        \n",
    "        # Actualizar estadísticas por país\n",
    "        estadisticas_globales = {\n",
    "            'llamadas_total': 0,\n",
    "            'minutos_total': 0,\n",
    "            'destinos_total': 0,\n",
    "            'registros_total': 0\n",
    "        }\n",
    "        \n",
    "        for pais in df['CODIGODEPAIS'].unique():\n",
    "            if pd.isna(pais):\n",
    "                continue\n",
    "                \n",
    "            pais = int(pais)\n",
    "            datos_pais = df[df['CODIGODEPAIS'] == pais]\n",
    "            \n",
    "            if len(datos_pais) >= 20:  # Mínimo 20 registros\n",
    "                categoria_anterior = stats_dict.get(pais, {}).get('CATEGORIA', None)\n",
    "                \n",
    "                stats_nuevas = {\n",
    "                    'REGISTROS': len(datos_pais),\n",
    "                    'LLAMADAS_MEAN': datos_pais['N_LLAMADAS'].mean(),\n",
    "                    'LLAMADAS_STD': datos_pais['N_LLAMADAS'].std(),\n",
    "                    'LLAMADAS_P90': datos_pais['N_LLAMADAS'].quantile(0.90),\n",
    "                    'LLAMADAS_P95': datos_pais['N_LLAMADAS'].quantile(0.95),\n",
    "                    'MINUTOS_MEAN': datos_pais['N_MINUTOS'].mean(),\n",
    "                    'MINUTOS_STD': datos_pais['N_MINUTOS'].std(),\n",
    "                    'MINUTOS_P90': datos_pais['N_MINUTOS'].quantile(0.90),\n",
    "                    'MINUTOS_P95': datos_pais['N_MINUTOS'].quantile(0.95),\n",
    "                    'DESTINOS_MEAN': datos_pais['N_DESTINOS'].mean(),\n",
    "                    'DESTINOS_STD': datos_pais['N_DESTINOS'].std(),\n",
    "                    'DESTINOS_P90': datos_pais['N_DESTINOS'].quantile(0.90),\n",
    "                    'DESTINOS_P95': datos_pais['N_DESTINOS'].quantile(0.95),\n",
    "                }\n",
    "                \n",
    "                # Actualizar estadísticas globales\n",
    "                estadisticas_globales['llamadas_total'] += datos_pais['N_LLAMADAS'].sum()\n",
    "                estadisticas_globales['minutos_total'] += datos_pais['N_MINUTOS'].sum()\n",
    "                estadisticas_globales['destinos_total'] += datos_pais['N_DESTINOS'].sum()\n",
    "                estadisticas_globales['registros_total'] += len(datos_pais)\n",
    "                \n",
    "                # Determinar nueva categoría basada en volumen\n",
    "                nueva_categoria = determinar_categoria_pais(stats_nuevas)\n",
    "                stats_nuevas['CATEGORIA'] = nueva_categoria\n",
    "                \n",
    "                # Actualizar o agregar país\n",
    "                if pais in stats_dict:\n",
    "                    # Promediar con existente\n",
    "                    for campo in ['LLAMADAS_MEAN', 'MINUTOS_MEAN', 'DESTINOS_MEAN']:\n",
    "                        if campo in stats_dict[pais]:\n",
    "                            stats_dict[pais][campo] = (stats_dict[pais][campo] * 0.7 + \n",
    "                                                      stats_nuevas[campo] * 0.3)\n",
    "                    \n",
    "                    # Actualizar percentiles y categoría\n",
    "                    for campo in ['LLAMADAS_P90', 'LLAMADAS_P95', 'MINUTOS_P90', 'MINUTOS_P95', \n",
    "                                 'DESTINOS_P90', 'DESTINOS_P95', 'CATEGORIA']:\n",
    "                        stats_dict[pais][campo] = stats_nuevas[campo]\n",
    "                    \n",
    "                    paises_actualizados.add(pais)\n",
    "                    \n",
    "                    # Detectar cambio de categoría\n",
    "                    if categoria_anterior and categoria_anterior != nueva_categoria:\n",
    "                        cambios_categoria[pais] = {\n",
    "                            'anterior': categoria_anterior,\n",
    "                            'nueva': nueva_categoria\n",
    "                        }\n",
    "                else:\n",
    "                    # País nuevo\n",
    "                    stats_dict[pais] = stats_nuevas\n",
    "        \n",
    "        # Calcular promedios globales\n",
    "        if estadisticas_globales['registros_total'] > 0:\n",
    "            promedios_globales = {\n",
    "                'llamadas': estadisticas_globales['llamadas_total'] / estadisticas_globales['registros_total'],\n",
    "                'minutos': estadisticas_globales['minutos_total'] / estadisticas_globales['registros_total'],\n",
    "                'destinos': estadisticas_globales['destinos_total'] / estadisticas_globales['registros_total']\n",
    "            }\n",
    "        else:\n",
    "            promedios_globales = {'llamadas': 0, 'minutos': 0, 'destinos': 0}\n",
    "        \n",
    "        # Recalcular umbral adaptativo si hay suficientes datos\n",
    "        if registros_procesados > 1000:\n",
    "            umbral_global = calcular_umbral_adaptativo(modelo, scaler, df, stats_dict)\n",
    "            config['umbral_global'] = umbral_global\n",
    "        \n",
    "        # Actualizar configuración\n",
    "        config['stats_por_pais'] = stats_dict\n",
    "        config['fecha_ultima_actualizacion'] = datetime.now().isoformat()\n",
    "        config['ultimo_aprendizaje_incremental'] = {\n",
    "            'fecha': datetime.now().isoformat(),\n",
    "            'registros_procesados': registros_procesados,\n",
    "            'paises_nuevos': len(paises_nuevos),\n",
    "            'paises_actualizados': len(paises_actualizados)\n",
    "        }\n",
    "        \n",
    "        # Guardar modelo actualizado\n",
    "        with open(os.path.join(MODELS_PATH, \"modelo_general.pkl\"), 'wb') as f:\n",
    "            pickle.dump(modelo, f)\n",
    "        \n",
    "        with open(os.path.join(MODELS_PATH, \"scaler_general.pkl\"), 'wb') as f:\n",
    "            pickle.dump(scaler, f)\n",
    "        \n",
    "        with open(os.path.join(MODELS_PATH, \"config_modelo_general.pkl\"), 'wb') as f:\n",
    "            pickle.dump(config, f)\n",
    "        \n",
    "        # Crear backup con timestamp\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        backup_path = os.path.join(MODELS_PATH, f\"backup_config_{timestamp}.pkl\")\n",
    "        with open(backup_path, 'wb') as f:\n",
    "            pickle.dump(config, f)\n",
    "        \n",
    "        # Calcular tiempo de procesamiento\n",
    "        tiempo_fin = datetime.now()\n",
    "        tiempo_procesamiento = (tiempo_fin - tiempo_inicio).total_seconds()\n",
    "        \n",
    "        # Actualizar registro en Oracle\n",
    "        if oracle_pool and id_aprendizaje:\n",
    "            actualizar_fin_aprendizaje(\n",
    "                id_aprendizaje=id_aprendizaje,\n",
    "                registros_procesados=registros_procesados,\n",
    "                paises_nuevos=len(paises_nuevos),\n",
    "                paises_actualizados=len(paises_actualizados),\n",
    "                umbral_nuevo=umbral_global,\n",
    "                cambios_categoria=cambios_categoria,\n",
    "                tiempo_procesamiento=tiempo_procesamiento,\n",
    "                backup_path=backup_path,\n",
    "                promedios_globales=promedios_globales,\n",
    "                estado=\"COMPLETADO\"\n",
    "            )\n",
    "        \n",
    "        # Limpiar archivo temporal\n",
    "        os.remove(archivo_path)\n",
    "        \n",
    "        logger.info(f\"\"\"\n",
    "        Aprendizaje incremental completado:\n",
    "        - Registros procesados: {registros_procesados}\n",
    "        - Países nuevos: {len(paises_nuevos)}\n",
    "        - Países actualizados: {len(paises_actualizados)}\n",
    "        - Cambios de categoría: {len(cambios_categoria)}\n",
    "        - Nuevo umbral: {umbral_global:.4f}\n",
    "        - Tiempo: {tiempo_procesamiento:.2f} segundos\n",
    "        \"\"\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error en aprendizaje incremental: {str(e)}\")\n",
    "        \n",
    "        # Actualizar registro de error en Oracle\n",
    "        if oracle_pool and id_aprendizaje:\n",
    "            try:\n",
    "                with get_oracle_connection() as conn:\n",
    "                    cursor = conn.cursor()\n",
    "                    cursor.execute(\"\"\"\n",
    "                        UPDATE APRENDIZAJE_INCREMENTAL\n",
    "                        SET ESTADO = 'ERROR',\n",
    "                            MENSAJE_ERROR = :1,\n",
    "                            TIEMPO_PROCESAMIENTO_SEG = :2\n",
    "                        WHERE ID_APRENDIZAJE = :3\n",
    "                    \"\"\", [str(e), (datetime.now() - tiempo_inicio).total_seconds(), id_aprendizaje])\n",
    "                    conn.commit()\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        if os.path.exists(archivo_path):\n",
    "            os.remove(archivo_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c3b57a",
   "metadata": {},
   "source": [
    "# ## 7. ENDPOINTS DE GESTIÓN DEL MODELO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64ca656e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.get(\"/modelo/estadisticas\", response_model=EstadisticasModelo, tags=[\"Modelo\"])\n",
    "async def obtener_estadisticas_modelo():\n",
    "    \"\"\"\n",
    "    Obtiene las estadísticas actuales del modelo\n",
    "    \"\"\"\n",
    "    if not modelo or not config:\n",
    "        raise HTTPException(status_code=503, detail=\"Modelo no disponible\")\n",
    "\n",
    "    return EstadisticasModelo(\n",
    "        fecha_entrenamiento=config.get('fecha_entrenamiento', 'No disponible'),\n",
    "        fecha_ultima_actualizacion=config.get('fecha_ultima_actualizacion'),\n",
    "        umbral_global=umbral_global,\n",
    "        n_trees=config.get('n_trees', 0),\n",
    "        tree_height=config.get('tree_height', 0),\n",
    "        registros_entrenamiento=config.get('registros_entrenamiento', 0),\n",
    "        paises_entrenamiento=config.get('paises_entrenamiento', 0),\n",
    "        paises_conocidos=len(stats_dict)\n",
    "    )\n",
    "\n",
    "@app.put(\"/modelo/actualizar-umbral\", tags=[\"Modelo\"])\n",
    "async def actualizar_umbral(configuracion: ConfiguracionUmbral):\n",
    "    \"\"\"\n",
    "    Actualiza el umbral de detección de anomalías\n",
    "    \"\"\"\n",
    "    global umbral_global\n",
    "\n",
    "    if not modelo:\n",
    "        raise HTTPException(status_code=503, detail=\"Modelo no disponible\")\n",
    "\n",
    "    try:\n",
    "        # Guardar umbral anterior\n",
    "        umbral_anterior = umbral_global\n",
    "\n",
    "        # Actualizar umbral\n",
    "        umbral_global = configuracion.nuevo_umbral\n",
    "        config['umbral_global'] = umbral_global\n",
    "\n",
    "        # Guardar configuración actualizada\n",
    "        with open(os.path.join(MODELS_PATH, \"config_modelo_general.pkl\"), 'wb') as f:\n",
    "            pickle.dump(config, f)\n",
    "\n",
    "        # NUEVO: Guardar en Oracle si está disponible\n",
    "        if oracle_pool:\n",
    "            try:\n",
    "                with get_oracle_connection() as conn:\n",
    "                    cursor = conn.cursor()\n",
    "                    cursor.execute(\"\"\"\n",
    "                        INSERT INTO HISTORICO_UMBRALES \n",
    "                        (UMBRAL_ANTERIOR, UMBRAL_NUEVO, RAZON_CAMBIO, USUARIO)\n",
    "                        VALUES (:1, :2, :3, :4)\n",
    "                    \"\"\", [umbral_anterior, umbral_global, configuracion.razon, \"API\"])\n",
    "                    conn.commit()\n",
    "                    logger.info(\"Cambio de umbral registrado en Oracle\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error guardando cambio de umbral en Oracle: {e}\")\n",
    "\n",
    "        # Guardar también en archivo JSON (como respaldo)\n",
    "        log_cambio = {\n",
    "            'timestamp': datetime.now(),\n",
    "            'umbral_anterior': umbral_anterior,\n",
    "            'umbral_nuevo': umbral_global,\n",
    "            'razon': configuracion.razon\n",
    "        }\n",
    "        log_file = os.path.join(LOG_PATH, f\"cambios_umbral_{datetime.now():%Y%m}.json\")\n",
    "\n",
    "        if os.path.exists(log_file):\n",
    "            with open(log_file, 'r') as f:\n",
    "                logs = json.load(f)\n",
    "        else:\n",
    "            logs = []\n",
    "\n",
    "        logs.append(log_cambio)\n",
    "        with open(log_file, 'w') as f:\n",
    "            json.dump(logs, f, indent=2, default=str)\n",
    "\n",
    "        return {\n",
    "            \"mensaje\": \"Umbral actualizado exitosamente\",\n",
    "            \"umbral_anterior\": umbral_anterior,\n",
    "            \"umbral_nuevo\": umbral_global,\n",
    "            \"timestamp\": datetime.now(),\n",
    "            \"guardado_oracle\": oracle_pool is not None\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error actualizando umbral: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "\n",
    "@app.post(\"/modelo/recargar\", tags=[\"Modelo\"])\n",
    "async def recargar_modelo():\n",
    "    \"\"\"\n",
    "    Recarga el modelo desde disco (útil si se actualizó externamente)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if cargar_modelo():\n",
    "            return {\n",
    "                \"mensaje\": \"Modelo recargado exitosamente\",\n",
    "                \"timestamp\": datetime.now()\n",
    "            }\n",
    "        else:\n",
    "            raise HTTPException(status_code=500, detail=\"Error al recargar modelo\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error recargando modelo: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d778ace6",
   "metadata": {},
   "source": [
    "## 8. ENDPOINTS DE ANÁLISIS Y REPORTES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f715a082",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "@app.get(\"/analisis/resumen-diario\", tags=[\"Análisis\"])\n",
    "async def resumen_diario(fecha: str = Query(..., description=\"Fecha en formato YYYY-MM-DD\")):\n",
    "    \"\"\"\n",
    "    Obtiene un resumen de las anomalías detectadas en un día específico\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Convertir fecha\n",
    "        fecha_obj = datetime.strptime(fecha, \"%Y-%m-%d\")\n",
    "\n",
    "        # Buscar archivos de resultados del día\n",
    "        archivos_dia = [\n",
    "            f for f in os.listdir(OUTPUT_PATH)\n",
    "            if f.startswith(\"resultados_\") and fecha.replace(\"-\", \"\") in f\n",
    "        ]\n",
    "\n",
    "        if not archivos_dia:\n",
    "            return {\n",
    "                \"fecha\": fecha,\n",
    "                \"registros_procesados\": 0,\n",
    "                \"anomalias_detectadas\": 0,\n",
    "                \"mensaje\": \"No hay datos para esta fecha\"\n",
    "            }\n",
    "\n",
    "        # Cargar y combinar datos\n",
    "        dfs = [pd.read_csv(os.path.join(OUTPUT_PATH, f)) for f in archivos_dia]\n",
    "        df_total = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "        # Calcular estadísticas\n",
    "        total_registros = len(df_total)\n",
    "        total_anomalias = int(df_total['es_anomalia'].sum())\n",
    "\n",
    "        # Distribución por tipo\n",
    "        distribucion_tipos = {}\n",
    "        anomalias = df_total[df_total['es_anomalia'] == True]\n",
    "        for tipo, cnt in anomalias['tipo_anomalia'].value_counts().items():\n",
    "            distribucion_tipos[tipo] = {\n",
    "                \"cantidad\": int(cnt),\n",
    "                \"porcentaje\": round(cnt / len(anomalias) * 100, 2)\n",
    "            }\n",
    "\n",
    "        # Top países\n",
    "        top_paises = [\n",
    "            {\"pais\": pais, \"anomalias\": int(cnt)}\n",
    "            for pais, cnt in anomalias['CODIGODEPAIS'].value_counts().head(10).items()\n",
    "        ]\n",
    "\n",
    "        return {\n",
    "            \"fecha\": fecha,\n",
    "            \"registros_procesados\": total_registros,\n",
    "            \"anomalias_detectadas\": total_anomalias,\n",
    "            \"tasa_anomalias\": round(total_anomalias / total_registros * 100, 2) if total_registros > 0 else 0,\n",
    "            \"distribucion_tipos\": distribucion_tipos,\n",
    "            \"top_paises_anomalias\": top_paises\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error generando resumen diario: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "\n",
    "@app.get(\"/analisis/tendencias\", tags=[\"Análisis\"])\n",
    "async def obtener_tendencias(dias: int = Query(7, description=\"Número de días a analizar\")):\n",
    "    \"\"\"\n",
    "    Obtiene las tendencias de anomalías de los últimos N días\n",
    "    \"\"\"\n",
    "    try:\n",
    "        fecha_fin = datetime.now()\n",
    "        fecha_inicio = fecha_fin - timedelta(days=dias)\n",
    "        tendencias = []\n",
    "\n",
    "        for i in range(dias):\n",
    "            fecha_i = fecha_inicio + timedelta(days=i)\n",
    "            archivos = [\n",
    "                f for f in os.listdir(OUTPUT_PATH)\n",
    "                if f.startswith(\"resultados_\") and fecha_i.strftime(\"%Y%m%d\") in f\n",
    "            ]\n",
    "\n",
    "            if archivos:\n",
    "                dfs = [pd.read_csv(os.path.join(OUTPUT_PATH, f)) for f in archivos]\n",
    "                df_dia = pd.concat(dfs, ignore_index=True)\n",
    "                anom_count = int(df_dia['es_anomalia'].sum())\n",
    "                tasas = round(df_dia['es_anomalia'].mean() * 100, 2) if len(df_dia) > 0 else 0\n",
    "                registros = len(df_dia)\n",
    "            else:\n",
    "                anom_count = 0\n",
    "                tasas = 0\n",
    "                registros = 0\n",
    "\n",
    "            tendencias.append({\n",
    "                \"fecha\": fecha_i.strftime(\"%Y-%m-%d\"),\n",
    "                \"registros\": registros,\n",
    "                \"anomalias\": anom_count,\n",
    "                \"tasa\": tasas\n",
    "            })\n",
    "\n",
    "        registros_totales = sum(t['registros'] for t in tendencias)\n",
    "        anom_totales = sum(t['anomalias'] for t in tendencias)\n",
    "\n",
    "        return {\n",
    "            \"periodo\": f\"Últimos {dias} días\",\n",
    "            \"fecha_inicio\": fecha_inicio.strftime(\"%Y-%m-%d\"),\n",
    "            \"fecha_fin\": fecha_fin.strftime(\"%Y-%m-%d\"),\n",
    "            \"tendencias\": tendencias,\n",
    "            \"resumen\": {\n",
    "                \"registros_totales\": registros_totales,\n",
    "                \"anomalias_totales\": anom_totales,\n",
    "                \"tasa_promedio\": round(anom_totales / registros_totales * 100, 2) if registros_totales > 0 else 0\n",
    "            }\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error obteniendo tendencias: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "\n",
    "@app.get(\"/analisis/exportar-anomalias\", tags=[\"Análisis\"])\n",
    "async def exportar_anomalias(\n",
    "    fecha_inicio: str = Query(..., description=\"Fecha inicio YYYY-MM-DD\"),\n",
    "    fecha_fin: str    = Query(..., description=\"Fecha fin YYYY-MM-DD\")\n",
    "):\n",
    "    \"\"\"\n",
    "    Exporta todas las anomalías detectadas en un rango de fechas\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Convertir fechas\n",
    "        inicio = datetime.strptime(fecha_inicio, \"%Y-%m-%d\")\n",
    "        fin    = datetime.strptime(fecha_fin,    \"%Y-%m-%d\")\n",
    "\n",
    "        if inicio > fin:\n",
    "            raise HTTPException(status_code=400, detail=\"Fecha inicio debe ser anterior a fecha fin\")\n",
    "\n",
    "        anomalias_totales = []\n",
    "        for archivo in os.listdir(OUTPUT_PATH):\n",
    "            if archivo.startswith(\"resultados_\"):\n",
    "                fecha_arch = archivo.split(\"_\")[2][:8]\n",
    "                try:\n",
    "                    fecha_obj = datetime.strptime(fecha_arch, \"%Y%m%d\")\n",
    "                except ValueError:\n",
    "                    continue\n",
    "\n",
    "                if inicio <= fecha_obj <= fin:\n",
    "                    df = pd.read_csv(os.path.join(OUTPUT_PATH, archivo))\n",
    "                    anomalias_totales.append(df[df['es_anomalia'] == True])\n",
    "\n",
    "        if not anomalias_totales:\n",
    "            return {\n",
    "                \"mensaje\": \"No se encontraron anomalías en el rango especificado\",\n",
    "                \"fecha_inicio\": fecha_inicio,\n",
    "                \"fecha_fin\": fecha_fin\n",
    "            }\n",
    "\n",
    "        # Combinar y exportar\n",
    "        df_out = pd.concat(anomalias_totales, ignore_index=True)\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        out_path = os.path.join(OUTPUT_PATH, f\"exportacion_anomalias_{timestamp}.csv\")\n",
    "        df_out.to_csv(out_path, index=False)\n",
    "\n",
    "        return FileResponse(out_path, media_type=\"text/csv\",\n",
    "                            filename=f\"anomalias_{fecha_inicio}_{fecha_fin}.csv\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error exportando anomalías: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b18032",
   "metadata": {},
   "source": [
    "## 9. ENDPOINTS DE MONITOREO Y SALUD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f644a89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.get(\"/salud\", response_model=EstadoSistema, tags=[\"Monitoreo\"])\n",
    "async def verificar_salud():\n",
    "    \"\"\"\n",
    "    Verifica el estado de salud del sistema\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Calcular estadísticas del día\n",
    "        hoy = datetime.now().strftime(\"%Y%m%d\")\n",
    "        registros_hoy = 0\n",
    "        anomalias_hoy = 0\n",
    "        for archivo in os.listdir(OUTPUT_PATH):\n",
    "            if archivo.startswith(\"resultados_\") and hoy in archivo:\n",
    "                df = pd.read_csv(os.path.join(OUTPUT_PATH, archivo))\n",
    "                registros_hoy += len(df)\n",
    "                anomalias_hoy += int(df['es_anomalia'].sum())\n",
    "\n",
    "        # Espacio en disco\n",
    "        stat = shutil.disk_usage(OUTPUT_PATH)\n",
    "        espacio_gb = stat.free / (1024**3)\n",
    "\n",
    "        # Último procesamiento\n",
    "        archivos = [f for f in os.listdir(OUTPUT_PATH) if f.startswith(\"resultados_\")]\n",
    "        ultimo_procesamiento = None\n",
    "        if archivos:\n",
    "            ultimo_archivo = max(archivos)\n",
    "            # Extraer timestamp del nombre del archivo\n",
    "            try:\n",
    "                parts = ultimo_archivo.split(\"_\")\n",
    "                timestamp_str = parts[2] + parts[3].replace(\".csv\", \"\")\n",
    "                ultimo_procesamiento = datetime.strptime(timestamp_str, \"%Y%m%d%H%M%S\")\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        return EstadoSistema(\n",
    "            estado=\"saludable\" if modelo else \"modelo no cargado\",\n",
    "            modelo_cargado=bool(modelo),\n",
    "            ultimo_procesamiento=ultimo_procesamiento,\n",
    "            registros_procesados_hoy=registros_hoy,\n",
    "            anomalias_detectadas_hoy=anomalias_hoy,\n",
    "            espacio_disco_gb=round(espacio_gb, 2)\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error verificando salud: {e}\")\n",
    "        return EstadoSistema(\n",
    "            estado=\"error\",\n",
    "            modelo_cargado=False,\n",
    "            ultimo_procesamiento=None,\n",
    "            registros_procesados_hoy=0,\n",
    "            anomalias_detectadas_hoy=0,\n",
    "            espacio_disco_gb=0\n",
    "        )\n",
    "\n",
    "\n",
    "@app.get(\"/logs/recientes\", tags=[\"Monitoreo\"])\n",
    "async def obtener_logs_recientes(limite: int = Query(100, description=\"Número de logs a retornar\")):\n",
    "    \"\"\"\n",
    "    Obtiene los logs más recientes del sistema\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logs = []\n",
    "        # Leer todos los JSON de logs\n",
    "        for archivo in os.listdir(LOG_PATH):\n",
    "            if archivo.endswith(\".json\"):\n",
    "                with open(os.path.join(LOG_PATH, archivo), \"r\") as f:\n",
    "                    contenido = json.load(f)\n",
    "                if isinstance(contenido, list):\n",
    "                    logs.extend(contenido)\n",
    "                else:\n",
    "                    logs.append(contenido)\n",
    "\n",
    "        # Ordenar y recortar\n",
    "        logs_ordenados = sorted(\n",
    "            logs,\n",
    "            key=lambda x: x.get(\"timestamp\", \"\"),\n",
    "            reverse=True\n",
    "        )[:limite]\n",
    "\n",
    "        return {\n",
    "            \"total_logs\": len(logs_ordenados),\n",
    "            \"logs\": logs_ordenados\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error obteniendo logs: {e}\")\n",
    "        return {\"total_logs\": 0, \"logs\": []}\n",
    "\n",
    "\n",
    "@app.delete(\"/limpiar/archivos-antiguos\", tags=[\"Monitoreo\"])\n",
    "async def limpiar_archivos_antiguos(dias_retener: int = Query(7, description=\"Días a retener\")):\n",
    "    \"\"\"\n",
    "    Limpia archivos más antiguos que los días especificados\n",
    "    \"\"\"\n",
    "    try:\n",
    "        fecha_limite = datetime.now() - timedelta(days=dias_retener)\n",
    "        archivos_eliminados = 0\n",
    "        espacio_liberado = 0\n",
    "\n",
    "        # OUTPUT_PATH\n",
    "        for archivo in os.listdir(OUTPUT_PATH):\n",
    "            ruta = os.path.join(OUTPUT_PATH, archivo)\n",
    "            if os.path.isfile(ruta):\n",
    "                mod_time = datetime.fromtimestamp(os.path.getmtime(ruta))\n",
    "                if mod_time < fecha_limite:\n",
    "                    tamaño = os.path.getsize(ruta)\n",
    "                    os.remove(ruta)\n",
    "                    archivos_eliminados += 1\n",
    "                    espacio_liberado += tamaño\n",
    "\n",
    "        # TEMP_PATH\n",
    "        for archivo in os.listdir(TEMP_PATH):\n",
    "            ruta = os.path.join(TEMP_PATH, archivo)\n",
    "            if os.path.isfile(ruta):\n",
    "                mod_time = datetime.fromtimestamp(os.path.getmtime(ruta))\n",
    "                if mod_time < fecha_limite:\n",
    "                    tamaño = os.path.getsize(ruta)\n",
    "                    os.remove(ruta)\n",
    "                    archivos_eliminados += 1\n",
    "                    espacio_liberado += tamaño\n",
    "\n",
    "        espacio_mb = espacio_liberado / (1024**2)\n",
    "        return {\n",
    "            \"mensaje\": \"Limpieza completada\",\n",
    "            \"archivos_eliminados\": archivos_eliminados,\n",
    "            \"espacio_liberado_mb\": round(espacio_mb, 2),\n",
    "            \"fecha_limite\": fecha_limite.strftime(\"%Y-%m-%d\")\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error limpiando archivos: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a9df4c",
   "metadata": {},
   "source": [
    "## 10. ENDPOINTS DE INFORMACIÓN DE PAÍSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8bcbd792",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.get(\"/paises/lista\", tags=[\"Información\"])\n",
    "async def listar_paises():\n",
    "    \"\"\"\n",
    "    Lista todos los países conocidos por el modelo\n",
    "    \"\"\"\n",
    "    if not stats_dict:\n",
    "        raise HTTPException(status_code=503, detail=\"Modelo no disponible\")\n",
    "\n",
    "    paises = []\n",
    "    for pais, stats in stats_dict.items():\n",
    "        paises.append({\n",
    "            \"codigo\": pais,\n",
    "            \"categoria\": stats.get(\"CATEGORIA\", \"Desconocido\"),\n",
    "            \"registros_historicos\": stats.get(\"REGISTROS\", 0),\n",
    "            \"promedio_llamadas\": round(stats.get(\"LLAMADAS_MEAN\", 0), 2),\n",
    "            \"promedio_minutos\": round(stats.get(\"MINUTOS_MEAN\", 0), 2),\n",
    "            \"promedio_destinos\": round(stats.get(\"DESTINOS_MEAN\", 0), 2),\n",
    "        })\n",
    "\n",
    "    return {\n",
    "        \"total_paises\": len(paises),\n",
    "        \"paises\": sorted(\n",
    "            paises,\n",
    "            key=lambda x: x[\"registros_historicos\"],\n",
    "            reverse=True\n",
    "        )\n",
    "    }\n",
    "\n",
    "\n",
    "@app.get(\"/paises/{codigo_pais}/estadisticas\", tags=[\"Información\"])\n",
    "async def estadisticas_pais(codigo_pais: str):\n",
    "    \"\"\"\n",
    "    Obtiene estadísticas detalladas de un país específico\n",
    "    \"\"\"\n",
    "    if not stats_dict:\n",
    "        raise HTTPException(status_code=503, detail=\"Modelo no disponible\")\n",
    "\n",
    "    if codigo_pais not in stats_dict:\n",
    "        raise HTTPException(\n",
    "            status_code=404,\n",
    "            detail=f\"País {codigo_pais} no encontrado\"\n",
    "        )\n",
    "\n",
    "    stats = stats_dict[codigo_pais]\n",
    "\n",
    "    return {\n",
    "        \"codigo_pais\": codigo_pais,\n",
    "        \"categoria\": stats.get(\"CATEGORIA\"),\n",
    "        \"contexto_historico\": contexto_historico.get(codigo_pais, \"No disponible\"),\n",
    "        \"estadisticas\": {\n",
    "            \"registros\": stats.get(\"REGISTROS\", 0),\n",
    "            \"llamadas\": {\n",
    "                \"promedio\": round(stats.get(\"LLAMADAS_MEAN\", 0), 2),\n",
    "                \"desviacion\": round(stats.get(\"LLAMADAS_STD\", 0), 2),\n",
    "                \"percentil_90\": round(stats.get(\"LLAMADAS_P90\", 0), 2),\n",
    "                \"percentil_95\": round(stats.get(\"LLAMADAS_P95\", 0), 2),\n",
    "            },\n",
    "            \"minutos\": {\n",
    "                \"promedio\": round(stats.get(\"MINUTOS_MEAN\", 0), 2),\n",
    "                \"desviacion\": round(stats.get(\"MINUTOS_STD\", 0), 2),\n",
    "                \"percentil_90\": round(stats.get(\"MINUTOS_P90\", 0), 2),\n",
    "                \"percentil_95\": round(stats.get(\"MINUTOS_P95\", 0), 2),\n",
    "            },\n",
    "            \"destinos\": {\n",
    "                \"promedio\": round(stats.get(\"DESTINOS_MEAN\", 0), 2),\n",
    "                \"desviacion\": round(stats.get(\"DESTINOS_STD\", 0), 2),\n",
    "                \"percentil_90\": round(stats.get(\"DESTINOS_P90\", 0), 2),\n",
    "                \"percentil_95\": round(stats.get(\"DESTINOS_P95\", 0), 2),\n",
    "            },\n",
    "        }\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53d05df",
   "metadata": {},
   "source": [
    "# ##NUEVO ENDPOINT PARA CONSULTAR APRENDISAJE INCREMENTAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "995897e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.get(\"/aprendizaje/historial\", tags=[\"Aprendizaje Incremental\"])\n",
    "async def historial_aprendizaje(\n",
    "    limite: int = Query(30, description=\"Número de registros a mostrar\")\n",
    "):\n",
    "    \"\"\"\n",
    "    Obtiene el historial de aprendizajes incrementales realizados\n",
    "    \"\"\"\n",
    "    if not oracle_pool:\n",
    "        raise HTTPException(status_code=503, detail=\"Oracle no disponible\")\n",
    "    \n",
    "    try:\n",
    "        with get_oracle_connection() as conn:\n",
    "            cursor = conn.cursor()\n",
    "            \n",
    "            cursor.execute(\"\"\"\n",
    "                SELECT \n",
    "                    ID_APRENDIZAJE,\n",
    "                    FECHA_EJECUCION,\n",
    "                    FECHA_DATOS,\n",
    "                    TOTAL_REGISTROS,\n",
    "                    REGISTROS_PROCESADOS,\n",
    "                    PAISES_NUEVOS,\n",
    "                    PAISES_ACTUALIZADOS,\n",
    "                    UMBRAL_ANTERIOR,\n",
    "                    UMBRAL_NUEVO,\n",
    "                    PAISES_CON_CAMBIO_CATEGORIA,\n",
    "                    TIEMPO_PROCESAMIENTO_SEG,\n",
    "                    ESTADO\n",
    "                FROM APRENDIZAJE_INCREMENTAL\n",
    "                ORDER BY FECHA_EJECUCION DESC\n",
    "                FETCH FIRST :limite ROWS ONLY\n",
    "            \"\"\", [limite])\n",
    "            \n",
    "            columnas = [col[0] for col in cursor.description]\n",
    "            historiales = []\n",
    "            \n",
    "            for row in cursor:\n",
    "                hist = dict(zip(columnas, row))\n",
    "                # Convertir timestamps\n",
    "                if hist['FECHA_EJECUCION']:\n",
    "                    hist['FECHA_EJECUCION'] = hist['FECHA_EJECUCION'].isoformat()\n",
    "                if hist['FECHA_DATOS']:\n",
    "                    hist['FECHA_DATOS'] = hist['FECHA_DATOS'].isoformat()\n",
    "                historiales.append(hist)\n",
    "            \n",
    "            return {\n",
    "                \"total\": len(historiales),\n",
    "                \"historiales\": historiales\n",
    "            }\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error obteniendo historial de aprendizaje: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.get(\"/aprendizaje/{id_aprendizaje}/detalle\", tags=[\"Aprendizaje Incremental\"])\n",
    "async def detalle_aprendizaje(id_aprendizaje: int):\n",
    "    \"\"\"\n",
    "    Obtiene el detalle completo de un aprendizaje incremental específico\n",
    "    \"\"\"\n",
    "    if not oracle_pool:\n",
    "        raise HTTPException(status_code=503, detail=\"Oracle no disponible\")\n",
    "    \n",
    "    try:\n",
    "        with get_oracle_connection() as conn:\n",
    "            cursor = conn.cursor()\n",
    "            \n",
    "            cursor.execute(\"\"\"\n",
    "                SELECT * FROM APRENDIZAJE_INCREMENTAL\n",
    "                WHERE ID_APRENDIZAJE = :id\n",
    "            \"\"\", [id_aprendizaje])\n",
    "            \n",
    "            row = cursor.fetchone()\n",
    "            if not row:\n",
    "                raise HTTPException(status_code=404, detail=\"Aprendizaje no encontrado\")\n",
    "            \n",
    "            columnas = [col[0] for col in cursor.description]\n",
    "            detalle = dict(zip(columnas, row))\n",
    "            \n",
    "            # Convertir timestamps y CLOB\n",
    "            if detalle['FECHA_EJECUCION']:\n",
    "                detalle['FECHA_EJECUCION'] = detalle['FECHA_EJECUCION'].isoformat()\n",
    "            if detalle['FECHA_DATOS']:\n",
    "                detalle['FECHA_DATOS'] = detalle['FECHA_DATOS'].isoformat()\n",
    "            \n",
    "            # Parsear JSON de cambios de categoría si existe\n",
    "            if detalle['DETALLE_CAMBIOS_CATEGORIA']:\n",
    "                try:\n",
    "                    detalle['DETALLE_CAMBIOS_CATEGORIA'] = json.loads(\n",
    "                        detalle['DETALLE_CAMBIOS_CATEGORIA'].read()\n",
    "                    )\n",
    "                except:\n",
    "                    detalle['DETALLE_CAMBIOS_CATEGORIA'] = {}\n",
    "            \n",
    "            return detalle\n",
    "            \n",
    "    except HTTPException:\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error obteniendo detalle de aprendizaje: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.get(\"/aprendizaje/estadisticas-evolucion\", tags=[\"Aprendizaje Incremental\"])\n",
    "async def estadisticas_evolucion():\n",
    "    \"\"\"\n",
    "    Muestra la evolución del modelo a través de los aprendizajes incrementales\n",
    "    \"\"\"\n",
    "    if not oracle_pool:\n",
    "        raise HTTPException(status_code=503, detail=\"Oracle no disponible\")\n",
    "    \n",
    "    try:\n",
    "        with get_oracle_connection() as conn:\n",
    "            cursor = conn.cursor()\n",
    "            \n",
    "            # Evolución del umbral\n",
    "            cursor.execute(\"\"\"\n",
    "                SELECT \n",
    "                    TO_CHAR(FECHA_EJECUCION, 'YYYY-MM-DD') as fecha,\n",
    "                    UMBRAL_ANTERIOR,\n",
    "                    UMBRAL_NUEVO\n",
    "                FROM APRENDIZAJE_INCREMENTAL\n",
    "                WHERE ESTADO = 'COMPLETADO'\n",
    "                ORDER BY FECHA_EJECUCION\n",
    "            \"\"\")\n",
    "            \n",
    "            evolucion_umbral = [\n",
    "                {\n",
    "                    \"fecha\": row[0],\n",
    "                    \"umbral_anterior\": float(row[1]) if row[1] else None,\n",
    "                    \"umbral_nuevo\": float(row[2]) if row[2] else None\n",
    "                }\n",
    "                for row in cursor.fetchall()\n",
    "            ]\n",
    "            \n",
    "            # Evolución de países conocidos\n",
    "            cursor.execute(\"\"\"\n",
    "                SELECT \n",
    "                    TO_CHAR(FECHA_EJECUCION, 'YYYY-MM-DD') as fecha,\n",
    "                    PAISES_CONOCIDOS_ANTERIOR,\n",
    "                    PAISES_CONOCIDOS_NUEVO,\n",
    "                    PAISES_NUEVOS\n",
    "                FROM APRENDIZAJE_INCREMENTAL\n",
    "                WHERE ESTADO = 'COMPLETADO'\n",
    "                ORDER BY FECHA_EJECUCION\n",
    "            \"\"\")\n",
    "            \n",
    "            evolucion_paises = [\n",
    "                {\n",
    "                    \"fecha\": row[0],\n",
    "                    \"paises_antes\": row[1],\n",
    "                    \"paises_despues\": row[2],\n",
    "                    \"paises_nuevos\": row[3]\n",
    "                }\n",
    "                for row in cursor.fetchall()\n",
    "            ]\n",
    "            \n",
    "            # Estadísticas agregadas\n",
    "            cursor.execute(\"\"\"\n",
    "                SELECT \n",
    "                    COUNT(*) as total_ejecuciones,\n",
    "                    SUM(REGISTROS_PROCESADOS) as total_registros,\n",
    "                    SUM(PAISES_NUEVOS) as total_paises_nuevos,\n",
    "                    AVG(TIEMPO_PROCESAMIENTO_SEG) as tiempo_promedio,\n",
    "                    MAX(FECHA_EJECUCION) as ultima_ejecucion\n",
    "                FROM APRENDIZAJE_INCREMENTAL\n",
    "                WHERE ESTADO = 'COMPLETADO'\n",
    "            \"\"\")\n",
    "            \n",
    "            stats = cursor.fetchone()\n",
    "            estadisticas_generales = {\n",
    "                \"total_ejecuciones\": stats[0] or 0,\n",
    "                \"total_registros_procesados\": stats[1] or 0,\n",
    "                \"total_paises_descubiertos\": stats[2] or 0,\n",
    "                \"tiempo_promedio_segundos\": float(stats[3]) if stats[3] else 0,\n",
    "                \"ultima_actualizacion\": stats[4].isoformat() if stats[4] else None\n",
    "            }\n",
    "            \n",
    "            return {\n",
    "                \"estadisticas_generales\": estadisticas_generales,\n",
    "                \"evolucion_umbral\": evolucion_umbral,\n",
    "                \"evolucion_paises\": evolucion_paises\n",
    "            }\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error obteniendo estadísticas de evolución: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.post(\"/aprendizaje/programar-diario\", tags=[\"Aprendizaje Incremental\"])\n",
    "async def programar_aprendizaje_diario(\n",
    "    hora_ejecucion: str = Query(\"02:00\", description=\"Hora de ejecución diaria (HH:MM)\"),\n",
    "    directorio_datos: str = Query(..., description=\"Directorio donde se encuentran los datos diarios\")\n",
    "):\n",
    "    \"\"\"\n",
    "    Programa el aprendizaje incremental diario (requiere implementación de scheduler)\n",
    "    \"\"\"\n",
    "    # Esta es una función placeholder que muestra cómo se podría implementar\n",
    "    return {\n",
    "        \"mensaje\": \"Funcionalidad de programación diaria\",\n",
    "        \"nota\": \"Requiere implementar un scheduler (como APScheduler) para ejecución automática\",\n",
    "        \"configuracion_sugerida\": {\n",
    "            \"hora_ejecucion\": hora_ejecucion,\n",
    "            \"directorio_datos\": directorio_datos,\n",
    "            \"comando_cron\": f\"0 {hora_ejecucion.split(':')[1]} {hora_ejecucion.split(':')[0]} * * python ejecutar_aprendizaje_diario.py\"\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23a3682",
   "metadata": {},
   "source": [
    "## 11. CONFIGURACIÓN PARA EJECUTAR LA API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5320478a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iniciar_api_notebook():\n",
    "    \"\"\"Inicia la API en modo notebook\"\"\"\n",
    "    import nest_asyncio\n",
    "    nest_asyncio.apply()\n",
    "    import uvicorn\n",
    "\n",
    "    print(\"\\n🚀 INICIANDO API DE DETECCIÓN DE FRAUDE\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"📍 URL Base: http://localhost:8000\")\n",
    "    print(\"📚 Documentación: http://localhost:8000/docs\")\n",
    "    print(\"📊 Documentación alternativa: http://localhost:8000/redoc\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nEndpoints principales:\")\n",
    "    print(\"  POST   /scoring/individual        - Scoring de un registro\")\n",
    "    print(\"  POST   /scoring/batch            - Scoring de archivo CSV\")\n",
    "    print(\"  POST   /modelo/aprendizaje-incremental - Actualizar modelo\")\n",
    "    print(\"  GET    /modelo/estadisticas      - Ver estadísticas del modelo\")\n",
    "    print(\"  GET    /analisis/resumen-diario  - Resumen por día\")\n",
    "    print(\"  GET    /salud                    - Estado del sistema\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\n⏹️  Para detener: Interrumpir el kernel del notebook\\n\")\n",
    "\n",
    "    # Ejecutar servidor\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000, reload=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1119813d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [24908]\n",
      "INFO:     Waiting for application startup.\n",
      "2025-06-18 20:33:25,369 - __main__ - INFO - Iniciando API...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 INICIANDO API DE DETECCIÓN DE FRAUDE\n",
      "============================================================\n",
      "📍 URL Base: http://localhost:8000\n",
      "📚 Documentación: http://localhost:8000/docs\n",
      "📊 Documentación alternativa: http://localhost:8000/redoc\n",
      "============================================================\n",
      "\n",
      "Endpoints principales:\n",
      "  POST   /scoring/individual        - Scoring de un registro\n",
      "  POST   /scoring/batch            - Scoring de archivo CSV\n",
      "  POST   /modelo/aprendizaje-incremental - Actualizar modelo\n",
      "  GET    /modelo/estadisticas      - Ver estadísticas del modelo\n",
      "  GET    /analisis/resumen-diario  - Resumen por día\n",
      "  GET    /salud                    - Estado del sistema\n",
      "============================================================\n",
      "\n",
      "⏹️  Para detener: Interrumpir el kernel del notebook\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 20:33:26,576 - __main__ - INFO - Modelo cargado exitosamente\n",
      "2025-06-18 20:33:26,578 - __main__ - INFO - Pool de conexiones Oracle inicializado\n",
      "2025-06-18 20:33:26,654 - __main__ - INFO - Tabla ya existe, continuando...\n",
      "2025-06-18 20:33:26,658 - __main__ - INFO - Tabla ya existe, continuando...\n",
      "2025-06-18 20:33:26,662 - __main__ - INFO - Tabla ya existe, continuando...\n",
      "2025-06-18 20:33:26,667 - __main__ - INFO - Tabla ya existe, continuando...\n",
      "2025-06-18 20:33:26,671 - __main__ - INFO - Tabla ya existe, continuando...\n",
      "2025-06-18 20:33:26,673 - __main__ - INFO - Tabla ya existe, continuando...\n",
      "2025-06-18 20:33:26,676 - __main__ - INFO - Tabla ya existe, continuando...\n",
      "2025-06-18 20:33:26,679 - __main__ - INFO - Tabla ya existe, continuando...\n",
      "2025-06-18 20:33:26,683 - __main__ - INFO - Tabla ya existe, continuando...\n",
      "2025-06-18 20:33:26,686 - __main__ - INFO - Tabla ya existe, continuando...\n",
      "2025-06-18 20:33:26,687 - __main__ - INFO - Todas las tablas creadas/verificadas exitosamente\n",
      "2025-06-18 20:33:26,688 - __main__ - INFO - API iniciada - Modelo: OK, Oracle: OK\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58801 - \"GET /docs HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:58801 - \"GET /openapi.json HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:58802 - \"POST /modelo/aprendizaje-incremental HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 20:33:51,730 - __main__ - INFO - Iniciando aprendizaje incremental con 102684 registros\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58818 - \"GET /aprendizaje/historial?limite=30 HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:58851 - \"GET /aprendizaje/historial?limite=30 HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 20:35:10,469 - __main__ - INFO - \n",
      "        Aprendizaje incremental completado:\n",
      "        - Registros procesados: 102684\n",
      "        - Países nuevos: 0\n",
      "        - Países actualizados: 85\n",
      "        - Cambios de categoría: 56\n",
      "        - Nuevo umbral: 0.9742\n",
      "        - Tiempo: 78.73 segundos\n",
      "        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:59042 - \"GET /aprendizaje/historial?limite=30 HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:59043 - \"GET /aprendizaje/estadisticas-evolucion HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:59361 - \"GET /modelo/estadisticas HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:59362 - \"GET /aprendizaje/historial?limite=30 HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 20:40:45,151 - __main__ - INFO - Cambio de umbral registrado en Oracle\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:59488 - \"PUT /modelo/actualizar-umbral HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:59489 - \"GET /modelo/estadisticas HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 20:41:13,348 - __main__ - INFO - Modelo cargado exitosamente\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:59564 - \"POST /modelo/recargar HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:59564 - \"GET /modelo/estadisticas HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:59565 - \"GET /aprendizaje/historial?limite=30 HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 20:42:17,761 - __main__ - INFO - Modelo cargado exitosamente\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:59669 - \"POST /modelo/recargar HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:59669 - \"GET /modelo/estadisticas HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 20:42:44,161 - __main__ - INFO - Modelo cargado exitosamente\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:59670 - \"POST /modelo/recargar HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:59670 - \"GET /modelo/estadisticas HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:59716 - \"GET /lotes/historial?limite=50 HTTP/1.1\" 200 OK\n"
     ]
    }
   ],
   "source": [
    "iniciar_api_notebook()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
