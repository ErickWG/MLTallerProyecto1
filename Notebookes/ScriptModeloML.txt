# %% [markdown]
# # 1. IMPORTAR LIBRER√çAS NECESARIAS
# 

# %%
import pandas as pd
import numpy as np
from river import anomaly
from river import preprocessing
import pickle
import os
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

print("‚úÖ Librer√≠as importadas correctamente")

# %% [markdown]
# # 2. CONFIGURACI√ìN INICIAL Y PAR√ÅMETROS
# 

# %%
# Rutas de archivos
DATASET_PATH = r"C:\Users\User\Desktop\TESIS\Datasets\DataSetAgrupadoNoSuper3.csv"
MODELS_PATH = r"C:\Users\User\Desktop\TESIS\Modelos"

# Crear directorio de modelos si no existe
os.makedirs(MODELS_PATH, exist_ok=True)

# PAR√ÅMETROS AJUSTABLES DEL MODELO ‚öôÔ∏è
N_TREES = 80  # üîß AJUSTABLE: 60-120 (m√°s √°rboles = m√°s precisi√≥n pero m√°s lento)
TREE_HEIGHT = 10  # üîß AJUSTABLE: 8-15 (mayor altura = patrones m√°s complejos)
MIN_RECORDS_WARMUP = 50  # M√≠nimo de registros para warm-up por pa√≠s

# PAR√ÅMETROS DE FEATURES ‚öôÔ∏è
PESO_MINUTOS_NORMAL = 0.4  # üîß AJUSTABLE: 0.2-0.6 (peso normal de minutos)
PESO_MINUTOS_EXTREMOS = 1.5  # üîß AJUSTABLE: 1.2-2.0 (peso cuando minutos son extremos)
UMBRAL_MINUTOS_EXTREMOS = 300  # üîß AJUSTABLE: 200-500 (minutos para considerar extremo)
PESO_DESTINOS = 1.2  # üîß AJUSTABLE: 1.0-1.5 (importancia de destinos)
PESO_SPRAY_RATIO = 1.5  # üîß AJUSTABLE: 1.2-2.0 (importancia del ratio spray)

# PAR√ÅMETROS DE UMBRAL ‚öôÔ∏è
PERCENTIL_BASE = 99  # üîß AJUSTABLE: 95-99.5 (percentil para umbral)
AJUSTE_UMBRAL = 1.0  # üîß AJUSTABLE: 0.8-1.2 (multiplicador del umbral)

print(f"üìÅ Directorio de modelos: {MODELS_PATH}")
print(f"ü§ñ Modelo general con {N_TREES} √°rboles, altura {TREE_HEIGHT}")
print(f"‚öôÔ∏è Par√°metros configurables definidos")

# %% [markdown]
# # 3. CARGAR Y EXPLORAR EL DATASET

# %%
print("\nüîÑ Cargando dataset...")
df = pd.read_csv(DATASET_PATH)

# Convertir fecha a datetime
df['FECHA'] = pd.to_datetime(df['FECHA'], format='%d/%m/%Y', errors='coerce')

print(f"üìã Dataset cargado - Shape: {df.shape}")
print(f"üìÖ Rango de fechas: {df['FECHA'].min()} a {df['FECHA'].max()}")
print(f"üåç Pa√≠ses √∫nicos: {df['CODIGODEPAIS'].nunique()}")
print(f"üìû L√≠neas √∫nicas: {df['LINEA'].nunique()}")

# Mostrar estad√≠sticas generales
print(f"\nüìä Estad√≠sticas generales:")
print(f"üìû Llamadas - Min: {df['N_LLAMADAS'].min()}, Max: {df['N_LLAMADAS'].max()}, Media: {df['N_LLAMADAS'].mean():.1f}")
print(f"‚è±Ô∏è Minutos - Min: {df['N_MINUTOS'].min()}, Max: {df['N_MINUTOS'].max()}, Media: {df['N_MINUTOS'].mean():.1f}")
print(f"üéØ Destinos - Min: {df['N_DESTINOS'].min()}, Max: {df['N_DESTINOS'].max()}, Media: {df['N_DESTINOS'].mean():.1f}")


# %% [markdown]
# # 4. DIVISI√ìN TEMPORAL PARA WARM-UP Y SCORING

# %%
df_sorted = df.sort_values('FECHA')
fechas_unicas = sorted(df['FECHA'].dt.to_period('M').unique())

if len(fechas_unicas) >= 2:
    primer_mes = fechas_unicas[0]
    segundo_mes = fechas_unicas[1]
    
    df_warmup = df[df['FECHA'].dt.to_period('M') == primer_mes].copy()
    df_scoring = df[df['FECHA'].dt.to_period('M') == segundo_mes].copy()
    
    print(f"\nüìÜ Per√≠odo de warm-up: {primer_mes}")
    print(f"üìÜ Per√≠odo de scoring: {segundo_mes}")
else:
    # Divisi√≥n por mediana de fechas
    fecha_corte = df_sorted['FECHA'].quantile(0.5)
    df_warmup = df[df['FECHA'] <= fecha_corte].copy()
    df_scoring = df[df['FECHA'] > fecha_corte].copy()
    
    print(f"\nüìÜ Divisi√≥n por fecha de corte: {fecha_corte}")

print(f"üî¢ Registros warm-up: {len(df_warmup)}")
print(f"üî¢ Registros scoring: {len(df_scoring)}")

# %% [markdown]
# # 5. AN√ÅLISIS DE CONTEXTO POR PA√çS (PARA NORMALIZACI√ìN)
# 

# %%
print(f"\nüåç Analizando contexto por pa√≠s...")

# Calcular estad√≠sticas por pa√≠s en per√≠odo de warm-up
stats_por_pais = df_warmup.groupby('CODIGODEPAIS').agg({
    'N_LLAMADAS': ['count', 'mean', 'std', lambda x: x.quantile(0.90), lambda x: x.quantile(0.95)],
    'N_MINUTOS': ['mean', 'std', lambda x: x.quantile(0.90), lambda x: x.quantile(0.95)],
    'N_DESTINOS': ['mean', 'std', lambda x: x.quantile(0.90), lambda x: x.quantile(0.95)]
}).round(2)

stats_por_pais.columns = ['REGISTROS', 'LLAMADAS_MEAN', 'LLAMADAS_STD', 'LLAMADAS_P90', 'LLAMADAS_P95',
                         'MINUTOS_MEAN', 'MINUTOS_STD', 'MINUTOS_P90', 'MINUTOS_P95',
                         'DESTINOS_MEAN', 'DESTINOS_STD', 'DESTINOS_P90', 'DESTINOS_P95']

# Clasificar pa√≠ses por volumen de tr√°fico
stats_por_pais['CATEGORIA'] = pd.cut(stats_por_pais['REGISTROS'], 
                                   bins=[0, 50, 200, 1000, float('inf')],
                                   labels=['Muy_Bajo', 'Bajo', 'Medio', 'Alto'])

print(f"üìä Distribuci√≥n de pa√≠ses por tr√°fico:")
print(stats_por_pais['CATEGORIA'].value_counts())

# Mostrar ejemplos por categor√≠a
print(f"\nüîç Ejemplos por categor√≠a de tr√°fico:")
for categoria in ['Muy_Bajo', 'Bajo', 'Medio', 'Alto']:
    paises_cat = stats_por_pais[stats_por_pais['CATEGORIA'] == categoria]
    if len(paises_cat) > 0:
        print(f"\n{categoria} ({len(paises_cat)} pa√≠ses):")
        print(paises_cat[['REGISTROS', 'LLAMADAS_MEAN', 'DESTINOS_MEAN', 'MINUTOS_MEAN']].head(3))


# %% [markdown]
# # 6. FUNCI√ìN PARA CREAR CARACTER√çSTICAS CONTEXTUALIZADAS

# %%
def crear_features_contextualizadas_mejorada(row, stats_pais_dict):
    """
    Crea caracter√≠sticas balanceadas que detecten minutos extremos y spray calling
    """
    pais = row['CODIGODEPAIS']
    llamadas = row['N_LLAMADAS']
    minutos = row['N_MINUTOS']
    destinos = row['N_DESTINOS']
    
    # Obtener contexto del pa√≠s (si existe)
    if pais in stats_pais_dict:
        pais_stats = stats_pais_dict[pais]
        categoria = pais_stats['CATEGORIA']
        
        # Normalizar por el contexto del pa√≠s
        llamadas_norm = min(llamadas / max(pais_stats['LLAMADAS_P95'], 1), 1.5)
        destinos_norm = min(destinos / max(pais_stats['DESTINOS_P95'], 1), 1.5)
        
        # MEJORA: Detecci√≥n inteligente de minutos extremos
        minutos_p90 = pais_stats.get('MINUTOS_P90', pais_stats['MINUTOS_P95'] * 0.9)
        minutos_p95 = pais_stats['MINUTOS_P95']
        
        # Transformaci√≥n adaptativa de minutos
        if minutos >= UMBRAL_MINUTOS_EXTREMOS:  # üîß Minutos extremos
            minutos_norm = min(minutos / max(minutos_p90, 1), 3.0)  # Mayor rango para extremos
            peso_minutos = PESO_MINUTOS_EXTREMOS  # Peso alto para extremos
        else:
            minutos_norm = min(np.log1p(minutos) / np.log1p(max(minutos_p90, 1)), 1.2)
            peso_minutos = PESO_MINUTOS_NORMAL  # Peso normal
            
    else:
        # Pa√≠s nuevo - SIEMPRE clasificar como 'Muy_Bajo'
        categoria = 'Muy_Bajo'
        llamadas_norm = min(llamadas / 10, 2.0)  # M√°s sensible para pa√≠ses nuevos
        destinos_norm = min(destinos / 5, 2.0)   # M√°s sensible para pa√≠ses nuevos
        
        # Para pa√≠ses nuevos, ser m√°s sensible a minutos altos
        if minutos >= UMBRAL_MINUTOS_EXTREMOS:
            minutos_norm = min(minutos / 50, 3.0)  # Muy sensible a minutos extremos
            peso_minutos = PESO_MINUTOS_EXTREMOS * 1.2  # Peso extra para pa√≠ses nuevos
        else:
            minutos_norm = min(np.log1p(minutos) / np.log1p(60), 1.2)
            peso_minutos = PESO_MINUTOS_NORMAL
    
    # Features principales - REBALANCEADAS con detecci√≥n de extremos
    features = {
        # 1. Valores normalizados con peso adaptativo
        'llamadas_norm': llamadas_norm * 0.8,
        'destinos_norm': destinos_norm * PESO_DESTINOS,  # üîß Ajustable
        'minutos_norm': minutos_norm * peso_minutos,     # üîß Peso adaptativo
        
        # 2. Ratios cr√≠ticos para fraude
        'diversidad_destinos': min(destinos / max(llamadas, 1), 1.0),
        'spray_ratio': min(destinos / max(llamadas, 1) * PESO_SPRAY_RATIO, 1.0) if destinos >= 5 else 0,
        
        # 3. NUEVA: Detecci√≥n espec√≠fica de minutos extremos
        'minutos_extremos': 1.0 if minutos >= UMBRAL_MINUTOS_EXTREMOS else 0.0,
        'minutos_sospechosos': min((minutos - 200) / 300, 1.0) if minutos > 200 else 0.0,
        
        # 4. Patrones de fraude
        'patron_spray_fuerte': 1.0 if (destinos >= 10 and llamadas >= 20) else 0.0,
        'patron_spray_medio': 0.5 if (destinos >= 6 and llamadas >= 12) else 0.0,
        'alta_diversidad': min(destinos / 12, 1) if destinos >= 5 else 0,
        
        # 5. Indicadores de volumen an√≥malo
        'volumen_llamadas_alto': min((llamadas - 30) / 50, 1) if llamadas > 30 else 0,
        'volumen_destinos_alto': min((destinos - 10) / 20, 1) if destinos > 10 else 0,
        
        # 6. Caracter√≠sticas de comportamiento
        'llamadas_por_destino': min(llamadas / max(destinos, 1) / 5, 1),
        'eficiencia_destinos': min(destinos / max(llamadas * 0.5, 1), 1),
        
        # 7. MEJORA: Ajuste por contexto de pa√≠s
        'factor_pais_bajo': 1.5 if categoria in ['Muy_Bajo', 'Bajo'] else 1.0,  # M√°s sensible
        'factor_pais_alto': 0.9 if categoria in ['Alto', 'Medio'] else 1.0      # Menos sensible
    }
    
    return features

# Convertir stats a diccionario para b√∫squeda r√°pida
stats_dict = {}
for pais, row in stats_por_pais.iterrows():
    stats_dict[pais] = row.to_dict()

print("üîß Funci√≥n de features mejorada definida (detecci√≥n de minutos extremos)")


# %% [markdown]
# # 7. ENTRENAMIENTO DEL MODELO GENERAL
# 

# %%
print(f"\nü§ñ Entrenando modelo con detecci√≥n de extremos...")

# Crear modelo con par√°metros configurables
modelo_mejorado = anomaly.HalfSpaceTrees(
    n_trees=N_TREES,
    height=TREE_HEIGHT,
    seed=42
)

# Crear scaler
scaler_mejorado = preprocessing.StandardScaler()

# Procesar datos con features mejoradas
scores_mejorados = []
features_mejoradas = []

print("üîÑ Procesando con features mejoradas...")
for contador, (idx, row) in enumerate(df_warmup.iterrows()):
    if contador % 2000 == 0:
        print(f"   Procesado: {contador}/{len(df_warmup)} registros")
    
    # Usar funci√≥n mejorada
    features = crear_features_contextualizadas_mejorada(row, stats_dict)
    
    # Normalizar
    scaler_mejorado.learn_one(features)
    features_scaled = scaler_mejorado.transform_one(features)
    
    # Score y entrenamiento
    score = modelo_mejorado.score_one(features_scaled)
    scores_mejorados.append(score)
    features_mejoradas.append(features)
    
    modelo_mejorado.learn_one(features_scaled)

print("‚úÖ Entrenamiento completado")

# %% [markdown]
# # 8. CALCULAR UMBRAL GLOBAL

# %%
# An√°lisis detallado de scores para encontrar umbral √≥ptimo
scores_array = np.array(scores_mejorados)

# Calcular m√∫ltiples percentiles
percentiles = [90, 92, 94, 95, 96, 97, 98, 99, 99.5]
umbrales = {}

print(f"\nüìä AN√ÅLISIS DE UMBRALES POSIBLES:")
for p in percentiles:
    umbral = np.percentile(scores_array, p)
    tasa_anomalia = (scores_array > umbral).mean() * 100
    umbrales[p] = {'umbral': umbral, 'tasa': tasa_anomalia}
    print(f"P{p}: Umbral={umbral:.4f}, Tasa={tasa_anomalia:.2f}%")

# Seleccionar umbral basado en PERCENTIL_BASE configurado
umbral_base = umbrales[PERCENTIL_BASE]['umbral']
umbral_objetivo = umbral_base * AJUSTE_UMBRAL  # üîß Ajuste configurable

# Calcular tasa final
tasa_objetivo = (scores_array > umbral_objetivo).mean() * 100

print(f"\nüéØ UMBRAL SELECCIONADO:")
print(f"üìä Percentil base: P{PERCENTIL_BASE}")
print(f"üî¢ Umbral final: {umbral_objetivo:.4f}")
print(f"üìà Tasa estimada: {tasa_objetivo:.2f}%")
print(f"‚öôÔ∏è Ajuste aplicado: {AJUSTE_UMBRAL}")

# %% [markdown]
# # 9. GUARDAR MODELO Y CONFIGURACI√ìN
# 

# %%
print(f"\nüíæ Guardando modelo mejorado...")

# Guardar modelo
modelo_path = os.path.join(MODELS_PATH, "modelo_general.pkl")
with open(modelo_path, 'wb') as f:
    pickle.dump(modelo_mejorado, f)

# Guardar scaler
scaler_path = os.path.join(MODELS_PATH, "scaler_general.pkl")
with open(scaler_path, 'wb') as f:
    pickle.dump(scaler_mejorado, f)

# Guardar configuraci√≥n
config_general = {
    'umbral_global': umbral_objetivo,
    'stats_por_pais': stats_dict,
    'fecha_entrenamiento': datetime.now().isoformat(),
    'n_trees': N_TREES,
    'tree_height': TREE_HEIGHT,
    'registros_entrenamiento': len(df_warmup),
    'paises_entrenamiento': df_warmup['CODIGODEPAIS'].nunique(),
    'parametros_features': {
        'peso_minutos_normal': PESO_MINUTOS_NORMAL,
        'peso_minutos_extremos': PESO_MINUTOS_EXTREMOS,
        'umbral_minutos_extremos': UMBRAL_MINUTOS_EXTREMOS,
        'peso_destinos': PESO_DESTINOS,
        'peso_spray_ratio': PESO_SPRAY_RATIO
    },
    'parametros_umbral': {
        'percentil_base': PERCENTIL_BASE,
        'ajuste_umbral': AJUSTE_UMBRAL
    }
}

config_path = os.path.join(MODELS_PATH, "config_modelo_general.pkl")
with open(config_path, 'wb') as f:
    pickle.dump(config_general, f)

print("‚úÖ Modelo mejorado guardado exitosamente")

# %% [markdown]
# # 10. FUNCI√ìN DE PREDICCI√ìN GENERAL
# 

# %%
def predecir_anomalia_mejorada(pais, linea, llamadas, minutos, destinos, modelo, scaler, umbral, stats_dict):
    """
    Predicci√≥n con detecci√≥n inteligente de minutos extremos y spray calling
    """
    # Crear row simulado
    row_data = {
        'CODIGODEPAIS': pais,
        'N_LLAMADAS': llamadas,
        'N_MINUTOS': minutos,
        'N_DESTINOS': destinos
    }
    
    # Crear features mejoradas
    features = crear_features_contextualizadas_mejorada(row_data, stats_dict)
    
    # Normalizar
    features_scaled = scaler.transform_one(features)
    
    # Obtener score
    score = modelo.score_one(features_scaled)
    
    # L√ìGICA MEJORADA PARA CONFIRMACI√ìN DE ANOMAL√çAS
    es_anomalia_base = score > umbral
    
    if es_anomalia_base:
        # Confirmar diferentes tipos de anomal√≠as
        
        # Tipo 1: Minutos extremos (NUEVA DETECCI√ìN)
        if minutos >= UMBRAL_MINUTOS_EXTREMOS:
            es_anomalia_final = True
            razon = f"Minutos extremos ({minutos:.1f} min)"
        
        # Tipo 2: Spray calling confirmado
        elif destinos >= 6 and llamadas >= 12:
            es_anomalia_final = True
            razon = "Patr√≥n de spray calling confirmado"
        
        # Tipo 3: Volumen excepcionalmente alto
        elif llamadas > 50 or destinos > 15:
            es_anomalia_final = True
            razon = "Volumen excepcionalmente alto"
        
        # Tipo 4: Pa√≠s de bajo tr√°fico con actividad sospechosa
        elif pais not in stats_dict or stats_dict.get(pais, {}).get('CATEGORIA') in ['Muy_Bajo', 'Bajo']:
            if destinos >= 4 and llamadas >= 8:
                es_anomalia_final = True
                razon = "Actividad sospechosa en pa√≠s de bajo tr√°fico"
            else:
                es_anomalia_final = False
                razon = "Actividad baja en pa√≠s de bajo tr√°fico"
        
        # Reglas de exclusi√≥n
        elif destinos < 3:
            es_anomalia_final = False
            razon = "Muy pocos destinos (<3)"
        elif destinos / max(llamadas, 1) < 0.15:
            es_anomalia_final = False
            razon = "Ratio destinos/llamadas muy bajo"
        elif llamadas < 5:
            es_anomalia_final = False
            razon = "Muy pocas llamadas (<5)"
        else:
            es_anomalia_final = False
            razon = "No cumple criterios de confirmaci√≥n"
    else:
        es_anomalia_final = False
        razon = "Score bajo umbral"
    
    # Determinar contexto
    if pais in stats_dict:
        categoria = stats_dict[pais]['CATEGORIA']
        tipo_contexto = categoria
    else:
        tipo_contexto = "Muy_Bajo"  # üîß Pa√≠ses nuevos siempre como Muy_Bajo
    
    return {
        'score': score,
        'umbral': umbral,
        'es_anomalia': es_anomalia_final,
        'tipo_contexto': tipo_contexto,
        'razon_decision': razon,
        'features': features
    }

print("üîÆ Funci√≥n de predicci√≥n mejorada definida")

# %% [markdown]
# # 11. SCORING EN PER√çODO DE PRUEBA
# 

# %%
print(f"\nüéØ Scoring con modelo mejorado...")

resultados_mejorados = []

for contador, (idx, row) in enumerate(df_scoring.iterrows()):
    if contador % 2000 == 0:
        print(f"   Scoring: {contador}/{len(df_scoring)} registros")
    
    resultado = predecir_anomalia_mejorada(
        pais=row['CODIGODEPAIS'],
        linea=row['LINEA'],
        llamadas=row['N_LLAMADAS'],
        minutos=row['N_MINUTOS'],
        destinos=row['N_DESTINOS'],
        modelo=modelo_mejorado,
        scaler=scaler_mejorado,
        umbral=umbral_objetivo,
        stats_dict=stats_dict
    )
    
    resultado_completo = {
        'FECHA': row['FECHA'],
        'CODIGODEPAIS': row['CODIGODEPAIS'],
        'LINEA': row['LINEA'],
        'N_LLAMADAS': row['N_LLAMADAS'],
        'N_MINUTOS': row['N_MINUTOS'],
        'N_DESTINOS': row['N_DESTINOS'],
        'score_anomalia': resultado['score'],
        'umbral': resultado['umbral'],
        'es_anomalia': resultado['es_anomalia'],
        'tipo_contexto': resultado['tipo_contexto'],
        'razon_decision': resultado['razon_decision']
    }
    
    resultados_mejorados.append(resultado_completo)

df_resultados_mejorados = pd.DataFrame(resultados_mejorados)

print("‚úÖ Scoring mejorado completado")

# %% [markdown]
# # 12. AN√ÅLISIS DETALLADO DE RESULTADOS
# 

# %%
print(f"\nüìä RESULTADOS CON MODELO MEJORADO:")
print(f"üìû Total de l√≠neas evaluadas: {len(df_resultados_mejorados)}")
print(f"üö® Anomal√≠as detectadas: {df_resultados_mejorados['es_anomalia'].sum()}")
print(f"üìà Tasa de anomal√≠as: {df_resultados_mejorados['es_anomalia'].mean()*100:.3f}%")

# An√°lisis de razones de decisi√≥n
print(f"\nüîç RAZONES DE DECISIONES:")
razones = df_resultados_mejorados['razon_decision'].value_counts()
print(razones)

# Anomal√≠as confirmadas
anomalias_confirmadas = df_resultados_mejorados[df_resultados_mejorados['es_anomalia'] == True]
if len(anomalias_confirmadas) > 0:
    print(f"\nüéØ ANOMAL√çAS CONFIRMADAS (Top 10):")
    print(anomalias_confirmadas.sort_values('score_anomalia', ascending=False)[
        ['CODIGODEPAIS', 'LINEA', 'N_LLAMADAS', 'N_MINUTOS', 'N_DESTINOS', 
         'score_anomalia', 'razon_decision']
    ].head(10))
    
    # Estad√≠sticas de anomal√≠as confirmadas
    print(f"\nüìä ESTAD√çSTICAS DE ANOMAL√çAS CONFIRMADAS:")
    print(f"üìû Llamadas - Min: {anomalias_confirmadas['N_LLAMADAS'].min()}, Max: {anomalias_confirmadas['N_LLAMADAS'].max()}, Media: {anomalias_confirmadas['N_LLAMADAS'].mean():.1f}")
    print(f"üéØ Destinos - Min: {anomalias_confirmadas['N_DESTINOS'].min()}, Max: {anomalias_confirmadas['N_DESTINOS'].max()}, Media: {anomalias_confirmadas['N_DESTINOS'].mean():.1f}")
    print(f"‚è±Ô∏è Minutos - Min: {anomalias_confirmadas['N_MINUTOS'].min()}, Max: {anomalias_confirmadas['N_MINUTOS'].max()}, Media: {anomalias_confirmadas['N_MINUTOS'].mean():.1f}")
    print(f"üìä Ratio Destinos/Llamadas promedio: {(anomalias_confirmadas['N_DESTINOS']/anomalias_confirmadas['N_LLAMADAS']).mean():.3f}")
    
    # An√°lisis de minutos extremos
    minutos_extremos = anomalias_confirmadas[anomalias_confirmadas['razon_decision'].str.contains('Minutos extremos')]
    if len(minutos_extremos) > 0:
        print(f"\n‚ö° DETECCIONES POR MINUTOS EXTREMOS: {len(minutos_extremos)}")
        print(f"‚è±Ô∏è Minutos promedio en extremos: {minutos_extremos['N_MINUTOS'].mean():.1f}")
else:
    print("‚ÑπÔ∏è No se detectaron anomal√≠as")

print(f"\n‚úÖ MEJORAS IMPLEMENTADAS:")
print(f"üîß Detecci√≥n inteligente de minutos extremos (‚â•{UMBRAL_MINUTOS_EXTREMOS} min)")
print(f"üåç Pa√≠ses nuevos clasificados como 'Muy_Bajo' autom√°ticamente")
print(f"üìä Umbral configurable (P{PERCENTIL_BASE} √ó {AJUSTE_UMBRAL})")
print(f"üõ°Ô∏è Reglas mejoradas anti-falsos positivos")
print(f"üéØ Mayor sensibilidad para pa√≠ses de bajo tr√°fico")

# %% [markdown]
# # 13. GUARDAR RESULTADOS FINALES
# 

# %%
# Guardar todos los resultados
resultados_path = os.path.join(MODELS_PATH, "resultados_modelo_general.csv")
df_resultados_mejorados.to_csv(resultados_path, index=False)

# Guardar solo anomal√≠as si existen
if len(anomalias_confirmadas) > 0:
    anomalias_path = os.path.join(MODELS_PATH, "anomalias_modelo_general.csv")
    anomalias_confirmadas.to_csv(anomalias_path, index=False)
    print(f"üö® Solo anomal√≠as: {anomalias_path}")
else:
    print("‚ÑπÔ∏è No hay anomal√≠as para guardar por separado")

print(f"\nüíæ RESULTADOS GUARDADOS:")
print(f"üìÑ Todos los resultados: {resultados_path}")

# %% [markdown]
# # # 14. FUNCI√ìN PARA PROBAR REGISTROS INDIVIDUALES
# 

# %%
def probar_registro_individual(pais, linea, llamadas, minutos, destinos, mostrar_detalles=True):
    """
    Funci√≥n para probar un registro individual y ver el an√°lisis completo
    
    Par√°metros:
    - pais: C√≥digo del pa√≠s (ej: "1", "34", "591")
    - linea: N√∫mero de l√≠nea telef√≥nica (ej: "70123456")
    - llamadas: N√∫mero de llamadas realizadas
    - minutos: Minutos totales de las llamadas
    - destinos: N√∫mero de destinos √∫nicos
    - mostrar_detalles: Si mostrar an√°lisis detallado (True/False)
    """
    print(f"\nüîç AN√ÅLISIS DE REGISTRO INDIVIDUAL")
    print(f"=" * 50)
    print(f"üåç Pa√≠s: {pais}")
    print(f"üìû L√≠nea: {linea}")
    print(f"üìä Llamadas: {llamadas}")
    print(f"‚è±Ô∏è Minutos: {minutos}")
    print(f"üéØ Destinos: {destinos}")
    print(f"-" * 50)
    
    # Realizar predicci√≥n
    resultado = predecir_anomalia_mejorada(
        pais=pais,
        linea=linea,
        llamadas=llamadas,
        minutos=minutos,
        destinos=destinos,
        modelo=modelo_mejorado,
        scaler=scaler_mejorado,
        umbral=umbral_objetivo,
        stats_dict=stats_dict
    )
    
    # Mostrar resultado principal
    if resultado['es_anomalia']:
        print(f"üö® RESULTADO: ANOMAL√çA DETECTADA")
        print(f"üî¥ Score: {resultado['score']:.4f} (Umbral: {resultado['umbral']:.4f})")
    else:
        print(f"‚úÖ RESULTADO: REGISTRO NORMAL")
        print(f"üü¢ Score: {resultado['score']:.4f} (Umbral: {resultado['umbral']:.4f})")
    
    print(f"üè∑Ô∏è Contexto del pa√≠s: {resultado['tipo_contexto']}")
    print(f"üí≠ Raz√≥n: {resultado['razon_decision']}")
    
    if mostrar_detalles:
        print(f"\nüìã AN√ÅLISIS DETALLADO:")
        print(f"-" * 30)
        
        # Mostrar contexto del pa√≠s
        if pais in stats_dict:
            pais_stats = stats_dict[pais]
            print(f"üìä Estad√≠sticas del pa√≠s {pais}:")
            print(f"   üìû Llamadas P95: {pais_stats['LLAMADAS_P95']:.1f}")
            print(f"   ‚è±Ô∏è Minutos P95: {pais_stats['MINUTOS_P95']:.1f}")
            print(f"   üéØ Destinos P95: {pais_stats['DESTINOS_P95']:.1f}")
            print(f"   üìà Categor√≠a: {pais_stats['CATEGORIA']}")
        else:
            print(f"‚ö†Ô∏è Pa√≠s {pais} no encontrado en estad√≠sticas (pa√≠s nuevo)")
        
        # Mostrar features calculadas
        print(f"\nüîß CARACTER√çSTICAS CALCULADAS:")
        features = resultado['features']
        for key, value in features.items():
            if value > 0:  # Solo mostrar features activas
                print(f"   {key}: {value:.3f}")
        
        # An√°lisis de patrones
        print(f"\nüéØ AN√ÅLISIS DE PATRONES:")
        ratio_destinos = destinos / max(llamadas, 1)
        minutos_por_llamada = minutos / max(llamadas, 1)
        
        print(f"   üìä Ratio destinos/llamadas: {ratio_destinos:.3f}")
        print(f"   ‚è±Ô∏è Minutos por llamada: {minutos_por_llamada:.2f}")
        
        # Indicadores de riesgo
        print(f"\n‚ö†Ô∏è INDICADORES DE RIESGO:")
        
        if destinos >= 10 and llamadas >= 20:
            print(f"   üî¥ Patr√≥n de spray calling fuerte detectado")
        elif destinos >= 6 and llamadas >= 12:
            print(f"   üü° Patr√≥n de spray calling medio detectado")
        
        if minutos >= UMBRAL_MINUTOS_EXTREMOS:
            print(f"   üî¥ Minutos extremos detectados (‚â•{UMBRAL_MINUTOS_EXTREMOS})")
        
        if ratio_destinos >= 0.5:
            print(f"   üü° Alta diversidad de destinos (‚â•50%)")
        
        if minutos_por_llamada < 2:
            print(f"   üü° Llamadas muy cortas (posible exploraci√≥n)")
        
        if llamadas > 50 or destinos > 15:
            print(f"   üî¥ Volumen excepcionalmente alto")
    
    print(f"=" * 50)
    return resultado

# %%
probar_registro_individual(pais=1, linea='70123456', llamadas=12, minutos=20, destinos=1)


# %% [markdown]
# # # 15. REINICIALIZACI√ìN Y CARGA DEL MODELO ENTRENADO

# %%
import pandas as pd
import numpy as np
from river import anomaly
from river import preprocessing
import pickle
import os
from datetime import datetime
import warnings
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
from sklearn.metrics import precision_score, recall_score, f1_score
import seaborn as sns
import matplotlib.pyplot as plt

warnings.filterwarnings('ignore')

print("‚úÖ Librer√≠as importadas correctamente")

# %% [markdown]
# # # 16. CONFIGURACI√ìN DE RUTAS Y CARGA DE MODELO

# %%
# Rutas de archivos
MODELS_PATH = r"C:\Users\User\Desktop\TESIS\Modelos"
EVALUATION_CSV_PATH = r"C:\Users\User\Desktop\TESIS\NuevoDataSet\DataSetFinalProbarMatriz.csv"  # üîß CAMBIAR ESTA RUTA

# Verificar que el directorio de modelos existe
if not os.path.exists(MODELS_PATH):
    print(f"‚ùå Error: Directorio de modelos no encontrado: {MODELS_PATH}")
    exit()

print(f"üìÅ Directorio de modelos: {MODELS_PATH}")

# %% [markdown]
# # # 17. CARGAR MODELO, SCALER Y CONFIGURACI√ìN

# %%
print("\nüîÑ Cargando modelo entrenado...")

# Cargar modelo
modelo_path = os.path.join(MODELS_PATH, "modelo_general.pkl")
with open(modelo_path, 'rb') as f:
    modelo_cargado = pickle.load(f)

# Cargar scaler
scaler_path = os.path.join(MODELS_PATH, "scaler_general.pkl")
with open(scaler_path, 'rb') as f:
    scaler_cargado = pickle.load(f)

# Cargar configuraci√≥n
config_path = os.path.join(MODELS_PATH, "config_modelo_general.pkl")
with open(config_path, 'rb') as f:
    config_cargado = pickle.load(f)

print("‚úÖ Modelo cargado exitosamente")

# Mostrar informaci√≥n del modelo
print(f"\nüìä INFORMACI√ìN DEL MODELO CARGADO:")
print(f"üéØ Umbral global: {config_cargado['umbral_global']:.4f}")
print(f"üåç Pa√≠ses en entrenamiento: {config_cargado['paises_entrenamiento']}")
print(f"üìà Registros de entrenamiento: {config_cargado['registros_entrenamiento']}")
print(f"üìÖ Fecha de entrenamiento: {config_cargado['fecha_entrenamiento']}")
print(f"üå≥ N√∫mero de √°rboles: {config_cargado['n_trees']}")
print(f"üìè Altura de √°rboles: {config_cargado['tree_height']}")

# Extraer configuraciones
umbral_global = config_cargado['umbral_global']
stats_por_pais_dict = config_cargado['stats_por_pais']
parametros_features = config_cargado['parametros_features']

# Configurar par√°metros de features
PESO_MINUTOS_NORMAL = parametros_features['peso_minutos_normal']
PESO_MINUTOS_EXTREMOS = parametros_features['peso_minutos_extremos']
UMBRAL_MINUTOS_EXTREMOS = parametros_features['umbral_minutos_extremos']
PESO_DESTINOS = parametros_features['peso_destinos']
PESO_SPRAY_RATIO = parametros_features['peso_spray_ratio']

print(f"\n‚öôÔ∏è PAR√ÅMETROS DE FEATURES CARGADOS:")
print(f"üîß Peso minutos normal: {PESO_MINUTOS_NORMAL}")
print(f"üîß Peso minutos extremos: {PESO_MINUTOS_EXTREMOS}")
print(f"üîß Umbral minutos extremos: {UMBRAL_MINUTOS_EXTREMOS}")
print(f"üîß Peso destinos: {PESO_DESTINOS}")
print(f"üîß Peso spray ratio: {PESO_SPRAY_RATIO}")

# %% [markdown]
# # # 18. REDEFINIR FUNCIONES NECESARIAS

# %%
def crear_features_contextualizadas_mejorada(row, stats_pais_dict):
    """
    Crea caracter√≠sticas balanceadas que detecten minutos extremos y spray calling
    """
    pais = row['CODIGODEPAIS']
    llamadas = row['N_LLAMADAS']
    minutos = row['N_MINUTOS']
    destinos = row['N_DESTINOS']
    
    # Obtener contexto del pa√≠s (si existe)
    if pais in stats_pais_dict:
        pais_stats = stats_pais_dict[pais]
        categoria = pais_stats['CATEGORIA']
        
        # Normalizar por el contexto del pa√≠s
        llamadas_norm = min(llamadas / max(pais_stats['LLAMADAS_P95'], 1), 1.5)
        destinos_norm = min(destinos / max(pais_stats['DESTINOS_P95'], 1), 1.5)
        
        # MEJORA: Detecci√≥n inteligente de minutos extremos
        minutos_p90 = pais_stats.get('MINUTOS_P90', pais_stats['MINUTOS_P95'] * 0.9)
        minutos_p95 = pais_stats['MINUTOS_P95']
        
        # Transformaci√≥n adaptativa de minutos
        if minutos >= UMBRAL_MINUTOS_EXTREMOS:  # üîß Minutos extremos
            minutos_norm = min(minutos / max(minutos_p90, 1), 3.0)  # Mayor rango para extremos
            peso_minutos = PESO_MINUTOS_EXTREMOS  # Peso alto para extremos
        else:
            minutos_norm = min(np.log1p(minutos) / np.log1p(max(minutos_p90, 1)), 1.2)
            peso_minutos = PESO_MINUTOS_NORMAL  # Peso normal
            
    else:
        # Pa√≠s nuevo - SIEMPRE clasificar como 'Muy_Bajo'
        categoria = 'Muy_Bajo'
        llamadas_norm = min(llamadas / 10, 2.0)  # M√°s sensible para pa√≠ses nuevos
        destinos_norm = min(destinos / 5, 2.0)   # M√°s sensible para pa√≠ses nuevos
        
        # Para pa√≠ses nuevos, ser m√°s sensible a minutos altos
        if minutos >= UMBRAL_MINUTOS_EXTREMOS:
            minutos_norm = min(minutos / 50, 3.0)  # Muy sensible a minutos extremos
            peso_minutos = PESO_MINUTOS_EXTREMOS * 1.2  # Peso extra para pa√≠ses nuevos
        else:
            minutos_norm = min(np.log1p(minutos) / np.log1p(60), 1.2)
            peso_minutos = PESO_MINUTOS_NORMAL
    
    # Features principales - REBALANCEADAS con detecci√≥n de extremos
    features = {
        # 1. Valores normalizados con peso adaptativo
        'llamadas_norm': llamadas_norm * 0.8,
        'destinos_norm': destinos_norm * PESO_DESTINOS,  # üîß Ajustable
        'minutos_norm': minutos_norm * peso_minutos,     # üîß Peso adaptativo
        
        # 2. Ratios cr√≠ticos para fraude
        'diversidad_destinos': min(destinos / max(llamadas, 1), 1.0),
        'spray_ratio': min(destinos / max(llamadas, 1) * PESO_SPRAY_RATIO, 1.0) if destinos >= 5 else 0,
        
        # 3. NUEVA: Detecci√≥n espec√≠fica de minutos extremos
        'minutos_extremos': 1.0 if minutos >= UMBRAL_MINUTOS_EXTREMOS else 0.0,
        'minutos_sospechosos': min((minutos - 200) / 300, 1.0) if minutos > 200 else 0.0,
        
        # 4. Patrones de fraude
        'patron_spray_fuerte': 1.0 if (destinos >= 10 and llamadas >= 20) else 0.0,
        'patron_spray_medio': 0.5 if (destinos >= 6 and llamadas >= 12) else 0.0,
        'alta_diversidad': min(destinos / 12, 1) if destinos >= 5 else 0,
        
        # 5. Indicadores de volumen an√≥malo
        'volumen_llamadas_alto': min((llamadas - 30) / 50, 1) if llamadas > 30 else 0,
        'volumen_destinos_alto': min((destinos - 10) / 20, 1) if destinos > 10 else 0,
        
        # 6. Caracter√≠sticas de comportamiento
        'llamadas_por_destino': min(llamadas / max(destinos, 1) / 5, 1),
        'eficiencia_destinos': min(destinos / max(llamadas * 0.5, 1), 1),
        
        # 7. MEJORA: Ajuste por contexto de pa√≠s
        'factor_pais_bajo': 1.5 if categoria in ['Muy_Bajo', 'Bajo'] else 1.0,  # M√°s sensible
        'factor_pais_alto': 0.9 if categoria in ['Alto', 'Medio'] else 1.0      # Menos sensible
    }
    
    return features

def predecir_anomalia_mejorada(pais, linea, llamadas, minutos, destinos, modelo, scaler, umbral, stats_dict):
    """
    Predicci√≥n con detecci√≥n inteligente de minutos extremos y spray calling
    """
    # Crear row simulado
    row_data = {
        'CODIGODEPAIS': pais,
        'N_LLAMADAS': llamadas,
        'N_MINUTOS': minutos,
        'N_DESTINOS': destinos
    }
    
    # Crear features mejoradas
    features = crear_features_contextualizadas_mejorada(row_data, stats_dict)
    
    # Normalizar
    features_scaled = scaler.transform_one(features)
    
    # Obtener score
    score = modelo.score_one(features_scaled)
    
    # L√ìGICA MEJORADA PARA CONFIRMACI√ìN DE ANOMAL√çAS
    es_anomalia_base = score > umbral
    
    if es_anomalia_base:
        # Confirmar diferentes tipos de anomal√≠as
        
        # Tipo 1: Minutos extremos (NUEVA DETECCI√ìN)
        if minutos >= UMBRAL_MINUTOS_EXTREMOS:
            es_anomalia_final = True
            razon = f"Minutos extremos ({minutos:.1f} min)"
        
        # Tipo 2: Spray calling confirmado
        elif destinos >= 6 and llamadas >= 12:
            es_anomalia_final = True
            razon = "Patr√≥n de spray calling confirmado"
        
        # Tipo 3: Volumen excepcionalmente alto
        elif llamadas > 50 or destinos > 15:
            es_anomalia_final = True
            razon = "Volumen excepcionalmente alto"
        
        # Tipo 4: Pa√≠s de bajo tr√°fico con actividad sospechosa
        elif pais not in stats_dict or stats_dict.get(pais, {}).get('CATEGORIA') in ['Muy_Bajo', 'Bajo']:
            if destinos >= 4 and llamadas >= 8:
                es_anomalia_final = True
                razon = "Actividad sospechosa en pa√≠s de bajo tr√°fico"
            else:
                es_anomalia_final = False
                razon = "Actividad baja en pa√≠s de bajo tr√°fico"
        
        # Reglas de exclusi√≥n
        elif destinos < 3:
            es_anomalia_final = False
            razon = "Muy pocos destinos (<3)"
        elif destinos / max(llamadas, 1) < 0.15:
            es_anomalia_final = False
            razon = "Ratio destinos/llamadas muy bajo"
        elif llamadas < 5:
            es_anomalia_final = False
            razon = "Muy pocas llamadas (<5)"
        else:
            es_anomalia_final = False
            razon = "No cumple criterios de confirmaci√≥n"
    else:
        es_anomalia_final = False
        razon = "Score bajo umbral"
    
    # Determinar contexto
    if pais in stats_dict:
        categoria = stats_dict[pais]['CATEGORIA']
        tipo_contexto = categoria
    else:
        tipo_contexto = "Muy_Bajo"  # üîß Pa√≠ses nuevos siempre como Muy_Bajo
    
    return {
        'score': score,
        'umbral': umbral,
        'es_anomalia': es_anomalia_final,
        'tipo_contexto': tipo_contexto,
        'razon_decision': razon,
        'features': features
    }

print("üîß Funciones redefinidas correctamente")

# %% [markdown]
# # # 19. CARGAR DATASET DE EVALUACI√ìN CON ETIQUETAS DE FRAUDE
# 

# %%
print(f"\nüìÇ Cargando dataset de evaluaci√≥n...")

# Verificar que el archivo existe
if not os.path.exists(EVALUATION_CSV_PATH):
    print(f"‚ùå Error: Archivo de evaluaci√≥n no encontrado: {EVALUATION_CSV_PATH}")
    print(f"üìù Por favor, aseg√∫rate de que el archivo exista y contenga las columnas:")
    print(f"   - FECHA, CODIGODEPAIS, LINEA, N_LLAMADAS, N_MINUTOS, N_DESTINOS, FRAUDE")
    print(f"   - FRAUDE debe ser 1 (fraudulento) o 0 (normal)")
    exit()

# Cargar dataset
df_evaluacion = pd.read_csv(EVALUATION_CSV_PATH)

# Convertir fecha a datetime si existe
if 'FECHA' in df_evaluacion.columns:
    df_evaluacion['FECHA'] = pd.to_datetime(df_evaluacion['FECHA'], format='%d/%m/%Y', errors='coerce')

print(f"‚úÖ Dataset cargado - Shape: {df_evaluacion.shape}")

# Verificar columnas requeridas
columnas_requeridas = ['CODIGODEPAIS', 'LINEA', 'N_LLAMADAS', 'N_MINUTOS', 'N_DESTINOS', 'FRAUDE']
columnas_faltantes = [col for col in columnas_requeridas if col not in df_evaluacion.columns]

if columnas_faltantes:
    print(f"‚ùå Error: Columnas faltantes: {columnas_faltantes}")
    print(f"üìã Columnas disponibles: {list(df_evaluacion.columns)}")
    exit()

# Verificar valores de FRAUDE
valores_fraude = df_evaluacion['FRAUDE'].unique()
if not all(v in [0, 1] for v in valores_fraude):
    print(f"‚ùå Error: FRAUDE debe contener solo valores 0 o 1. Valores encontrados: {valores_fraude}")
    exit()

print(f"üîç AN√ÅLISIS DEL DATASET DE EVALUACI√ìN:")
print(f"üìä Total de registros: {len(df_evaluacion)}")
print(f"üö® Casos de fraude: {df_evaluacion['FRAUDE'].sum()} ({df_evaluacion['FRAUDE'].mean()*100:.2f}%)")
print(f"‚úÖ Casos normales: {(df_evaluacion['FRAUDE'] == 0).sum()} ({(df_evaluacion['FRAUDE'] == 0).mean()*100:.2f}%)")
print(f"üåç Pa√≠ses √∫nicos: {df_evaluacion['CODIGODEPAIS'].nunique()}")
print(f"üìû L√≠neas √∫nicas: {df_evaluacion['LINEA'].nunique()}")

# Mostrar estad√≠sticas por clase
print(f"\nüìä ESTAD√çSTICAS POR CLASE:")
print(f"CASOS NORMALES (FRAUDE = 0):")
normales = df_evaluacion[df_evaluacion['FRAUDE'] == 0]
print(f"  üìû Llamadas - Min: {normales['N_LLAMADAS'].min()}, Max: {normales['N_LLAMADAS'].max()}, Media: {normales['N_LLAMADAS'].mean():.1f}")
print(f"  ‚è±Ô∏è Minutos - Min: {normales['N_MINUTOS'].min()}, Max: {normales['N_MINUTOS'].max()}, Media: {normales['N_MINUTOS'].mean():.1f}")
print(f"  üéØ Destinos - Min: {normales['N_DESTINOS'].min()}, Max: {normales['N_DESTINOS'].max()}, Media: {normales['N_DESTINOS'].mean():.1f}")

if df_evaluacion['FRAUDE'].sum() > 0:
    print(f"\nCASOS DE FRAUDE (FRAUDE = 1):")
    fraudes = df_evaluacion[df_evaluacion['FRAUDE'] == 1]
    print(f"  üìû Llamadas - Min: {fraudes['N_LLAMADAS'].min()}, Max: {fraudes['N_LLAMADAS'].max()}, Media: {fraudes['N_LLAMADAS'].mean():.1f}")
    print(f"  ‚è±Ô∏è Minutos - Min: {fraudes['N_MINUTOS'].min()}, Max: {fraudes['N_MINUTOS'].max()}, Media: {fraudes['N_MINUTOS'].mean():.1f}")
    print(f"  üéØ Destinos - Min: {fraudes['N_DESTINOS'].min()}, Max: {fraudes['N_DESTINOS'].max()}, Media: {fraudes['N_DESTINOS'].mean():.1f}")


# %% [markdown]
# # # 20. REALIZAR PREDICCIONES EN DATASET DE EVALUACI√ìN
# 

# %%
print(f"\nüîÆ Realizando predicciones en dataset de evaluaci√≥n...")

predicciones = []
scores = []

for contador, (idx, row) in enumerate(df_evaluacion.iterrows()):
    if contador % 1000 == 0:
        print(f"   Procesando: {contador}/{len(df_evaluacion)} registros")
    
    # Realizar predicci√≥n
    resultado = predecir_anomalia_mejorada(
        pais=row['CODIGODEPAIS'],
        linea=row['LINEA'],
        llamadas=row['N_LLAMADAS'],
        minutos=row['N_MINUTOS'],
        destinos=row['N_DESTINOS'],
        modelo=modelo_cargado,
        scaler=scaler_cargado,
        umbral=umbral_global,
        stats_dict=stats_por_pais_dict
    )
    
    # Guardar predicci√≥n (1 si es anomal√≠a, 0 si es normal)
    predicciones.append(1 if resultado['es_anomalia'] else 0)
    scores.append(resultado['score'])

# Agregar predicciones al dataframe
df_evaluacion['PREDICCION'] = predicciones
df_evaluacion['SCORE_ANOMALIA'] = scores

print(f"‚úÖ Predicciones completadas")

# %% [markdown]
# # # 21. CALCULAR M√âTRICAS Y MATRIZ DE CONFUSI√ìN
# 

# %%
print(f"\nüìä CALCULANDO M√âTRICAS DE EVALUACI√ìN...")

# Extraer etiquetas reales y predicciones
y_true = df_evaluacion['FRAUDE'].values
y_pred = df_evaluacion['PREDICCION'].values

# Calcular m√©tricas principales
accuracy = accuracy_score(y_true, y_pred)
precision = precision_score(y_true, y_pred, zero_division=0)
recall = recall_score(y_true, y_pred, zero_division=0)
f1 = f1_score(y_true, y_pred, zero_division=0)

print(f"üéØ M√âTRICAS DE EVALUACI√ìN:")
print(f"üìà Accuracy (Exactitud): {accuracy:.4f} ({accuracy*100:.2f}%)")
print(f"üéØ Precision (Precisi√≥n): {precision:.4f} ({precision*100:.2f}%)")
print(f"üîç Recall (Sensibilidad): {recall:.4f} ({recall*100:.2f}%)")
print(f"‚öñÔ∏è F1-Score: {f1:.4f} ({f1*100:.2f}%)")

# Calcular matriz de confusi√≥n
cm = confusion_matrix(y_true, y_pred)
tn, fp, fn, tp = cm.ravel()

print(f"\nüìã MATRIZ DE CONFUSI√ìN:")
print(f"‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê")
print(f"‚îÇ     REAL \\ PRED ‚îÇ   Normal   ‚îÇ Anomal√≠a ‚îÇ")
print(f"‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§")
print(f"‚îÇ      Normal     ‚îÇ   {tn:6d}   ‚îÇ  {fp:6d}  ‚îÇ")
print(f"‚îÇ     Fraude      ‚îÇ   {fn:6d}   ‚îÇ  {tp:6d}  ‚îÇ")
print(f"‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò")

print(f"\nüî¢ INTERPRETACI√ìN:")
print(f"‚úÖ Verdaderos Negativos (TN): {tn} - Casos normales correctamente identificados")
print(f"‚ùå Falsos Positivos (FP): {fp} - Casos normales incorrectamente marcados como fraude")
print(f"‚ùå Falsos Negativos (FN): {fn} - Casos de fraude no detectados")
print(f"‚úÖ Verdaderos Positivos (TP): {tp} - Casos de fraude correctamente detectados")

# Calcular tasas adicionales
if (tn + fp) > 0:
    especificidad = tn / (tn + fp)
    print(f"üõ°Ô∏è Especificidad (Tasa de Verdaderos Negativos): {especificidad:.4f} ({especificidad*100:.2f}%)")

if (fp + tn) > 0:
    tasa_fp = fp / (fp + tn)
    print(f"‚ö†Ô∏è Tasa de Falsos Positivos: {tasa_fp:.4f} ({tasa_fp*100:.2f}%)")

if (fn + tp) > 0:
    tasa_fn = fn / (fn + tp)
    print(f"‚ö†Ô∏è Tasa de Falsos Negativos: {tasa_fn:.4f} ({tasa_fn*100:.2f}%)")

# %% [markdown]
# # # 22. Guardar Resultados

# %%
resultados_path = os.path.join(MODELS_PATH, "resultados_evaluacion_completa.csv")
df_evaluacion.to_csv(resultados_path, index=False)
print(f"üìÑ Resultados completos: {resultados_path}")




