{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "244f6170",
   "metadata": {},
   "source": [
    "# SCORING EN TIEMPO REAL - DETECCI√ìN DE FRAUDE TELEF√ìNICO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18373c2f",
   "metadata": {},
   "source": [
    "### Este notebook procesa CSVs en tiempo real y genera resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34c77962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Sistema de Scoring en Tiempo Real iniciado\n",
      "üïê Hora de inicio: 2025-06-17 21:25:35\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import time\n",
    "import shutil\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Sistema de Scoring en Tiempo Real iniciado\")\n",
    "print(f\"üïê Hora de inicio: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cea7fb8",
   "metadata": {},
   "source": [
    "# # 1. CONFIGURACI√ìN Y RUTAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "220a3b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Carpeta de entrada: C:\\Users\\User\\Desktop\\TESIS\\CodigoGithub\\MLTallerProyecto1\\Datos_Tiempo_Real\n",
      "üìÅ Carpeta de salida: C:\\Users\\User\\Desktop\\TESIS\\CodigoGithub\\MLTallerProyecto1\\Resultados_Tiempo_Real\n",
      "‚è±Ô∏è Intervalo de procesamiento: 180 segundos\n",
      "üìÑ Archivo a monitorear: datos_dia_actual.csv\n"
     ]
    }
   ],
   "source": [
    "# Rutas principales\n",
    "MODELS_PATH = r\"C:\\Users\\User\\Desktop\\TESIS\\CodigoGithub\\MLTallerProyecto1\\Modelos\"\n",
    "INPUT_PATH = r\"C:\\Users\\User\\Desktop\\TESIS\\CodigoGithub\\MLTallerProyecto1\\Datos_Tiempo_Real\"  # Carpeta donde llegan los CSVs\n",
    "OUTPUT_PATH = r\"C:\\Users\\User\\Desktop\\TESIS\\CodigoGithub\\MLTallerProyecto1\\Resultados_Tiempo_Real\"  # Carpeta para resultados\n",
    "PROCESSED_PATH = r\"C:\\Users\\User\\Desktop\\TESIS\\CodigoGithub\\MLTallerProyecto1\\Datos_Procesados\"  # Carpeta para CSVs ya procesados\n",
    "LOG_PATH = r\"C:\\Users\\User\\Desktop\\TESIS\\CodigoGithub\\MLTallerProyecto1\\Logs\"  # Carpeta para logs\n",
    "\n",
    "# Crear directorios si no existen\n",
    "for path in [INPUT_PATH, OUTPUT_PATH, PROCESSED_PATH, LOG_PATH]:\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "# Configuraci√≥n de procesamiento\n",
    "INTERVALO_SEGUNDOS = 180  # 3 minutos = 180 segundos\n",
    "ARCHIVO_ENTRADA = \"datos_dia_actual.csv\"  # Nombre del archivo que se actualiza\n",
    "PREFIJO_SALIDA = \"resultados\"  # Prefijo para archivos de salida\n",
    "\n",
    "print(f\"üìÅ Carpeta de entrada: {INPUT_PATH}\")\n",
    "print(f\"üìÅ Carpeta de salida: {OUTPUT_PATH}\")\n",
    "print(f\"‚è±Ô∏è Intervalo de procesamiento: {INTERVALO_SEGUNDOS} segundos\")\n",
    "print(f\"üìÑ Archivo a monitorear: {ARCHIVO_ENTRADA}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55489184",
   "metadata": {},
   "source": [
    "# # 2. CARGAR MODELO Y CONFIGURACI√ìN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8aa5a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Cargando modelo y configuraci√≥n...\n",
      "‚úÖ Modelo cargado exitosamente\n",
      "üìä Configuraci√≥n:\n",
      "   - Umbral de anomal√≠a: 0.9724\n",
      "   - Pa√≠ses en contexto: 188\n",
      "   - Fecha de entrenamiento: 2025-06-16T21:08:01.518982\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüîÑ Cargando modelo y configuraci√≥n...\")\n",
    "\n",
    "try:\n",
    "    # Cargar modelo\n",
    "    modelo_path = os.path.join(MODELS_PATH, \"modelo_general.pkl\")\n",
    "    with open(modelo_path, 'rb') as f:\n",
    "        modelo = pickle.load(f)\n",
    "    \n",
    "    # Cargar scaler\n",
    "    scaler_path = os.path.join(MODELS_PATH, \"scaler_general.pkl\")\n",
    "    with open(scaler_path, 'rb') as f:\n",
    "        scaler = pickle.load(f)\n",
    "    \n",
    "    # Cargar configuraci√≥n\n",
    "    config_path = os.path.join(MODELS_PATH, \"config_modelo_general.pkl\")\n",
    "    with open(config_path, 'rb') as f:\n",
    "        config = pickle.load(f)\n",
    "    \n",
    "    # Extraer componentes importantes\n",
    "    stats_dict = config['stats_por_pais']\n",
    "    contexto_historico = config.get('contexto_historico', {})\n",
    "    umbral_global = config['umbral_global']\n",
    "    parametros_features = config['parametros_features']\n",
    "    \n",
    "    print(\"‚úÖ Modelo cargado exitosamente\")\n",
    "    print(f\"üìä Configuraci√≥n:\")\n",
    "    print(f\"   - Umbral de anomal√≠a: {umbral_global:.4f}\")\n",
    "    print(f\"   - Pa√≠ses en contexto: {len(contexto_historico)}\")\n",
    "    print(f\"   - Fecha de entrenamiento: {config.get('fecha_entrenamiento', 'No disponible')}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error al cargar modelo: {str(e)}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c792a43",
   "metadata": {},
   "source": [
    "# # 3. FUNCIONES DE PROCESAMIENTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49fbb6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Funciones de procesamiento cargadas\n"
     ]
    }
   ],
   "source": [
    "def crear_features_contextualizadas_mejorada(row, stats_pais_dict):\n",
    "    \"\"\"\n",
    "    Funci√≥n id√©ntica a la del modelo original para mantener consistencia\n",
    "    \"\"\"\n",
    "    pais = row['CODIGODEPAIS']\n",
    "    llamadas = row['N_LLAMADAS']\n",
    "    minutos = row['N_MINUTOS']\n",
    "    destinos = row['N_DESTINOS']\n",
    "    \n",
    "    # Par√°metros de configuraci√≥n\n",
    "    PESO_MINUTOS_NORMAL = parametros_features['peso_minutos_normal']\n",
    "    PESO_MINUTOS_EXTREMOS = parametros_features['peso_minutos_extremos']\n",
    "    UMBRAL_MINUTOS_EXTREMOS = parametros_features['umbral_minutos_extremos']\n",
    "    PESO_DESTINOS = parametros_features['peso_destinos']\n",
    "    PESO_SPRAY_RATIO = parametros_features['peso_spray_ratio']\n",
    "    \n",
    "    if pais in stats_pais_dict:\n",
    "        pais_stats = stats_pais_dict[pais]\n",
    "        categoria = pais_stats['CATEGORIA']\n",
    "        \n",
    "        llamadas_norm = min(llamadas / max(pais_stats['LLAMADAS_P95'], 1), 1.5)\n",
    "        destinos_norm = min(destinos / max(pais_stats['DESTINOS_P95'], 1), 1.5)\n",
    "        \n",
    "        minutos_p90 = pais_stats.get('MINUTOS_P90', pais_stats['MINUTOS_P95'] * 0.9)\n",
    "        \n",
    "        if minutos >= UMBRAL_MINUTOS_EXTREMOS:\n",
    "            minutos_norm = min(minutos / max(minutos_p90, 1), 3.0)\n",
    "            peso_minutos = PESO_MINUTOS_EXTREMOS\n",
    "        else:\n",
    "            minutos_norm = min(np.log1p(minutos) / np.log1p(max(minutos_p90, 1)), 1.2)\n",
    "            peso_minutos = PESO_MINUTOS_NORMAL\n",
    "    else:\n",
    "        categoria = 'Muy_Bajo'\n",
    "        llamadas_norm = min(llamadas / 10, 2.0)\n",
    "        destinos_norm = min(destinos / 5, 2.0)\n",
    "        \n",
    "        if minutos >= UMBRAL_MINUTOS_EXTREMOS:\n",
    "            minutos_norm = min(minutos / 50, 3.0)\n",
    "            peso_minutos = PESO_MINUTOS_EXTREMOS * 1.2\n",
    "        else:\n",
    "            minutos_norm = min(np.log1p(minutos) / np.log1p(60), 1.2)\n",
    "            peso_minutos = PESO_MINUTOS_NORMAL\n",
    "    \n",
    "    features = {\n",
    "        'llamadas_norm': llamadas_norm * 0.8,\n",
    "        'destinos_norm': destinos_norm * PESO_DESTINOS,\n",
    "        'minutos_norm': minutos_norm * peso_minutos,\n",
    "        'diversidad_destinos': min(destinos / max(llamadas, 1), 1.0),\n",
    "        'spray_ratio': min(destinos / max(llamadas, 1) * PESO_SPRAY_RATIO, 1.0) if destinos >= 5 else 0,\n",
    "        'minutos_extremos': 1.0 if minutos >= UMBRAL_MINUTOS_EXTREMOS else 0.0,\n",
    "        'minutos_sospechosos': min((minutos - 200) / 300, 1.0) if minutos > 200 else 0.0,\n",
    "        'patron_spray_fuerte': 1.0 if (destinos >= 10 and llamadas >= 20) else 0.0,\n",
    "        'patron_spray_medio': 0.5 if (destinos >= 6 and llamadas >= 12) else 0.0,\n",
    "        'alta_diversidad': min(destinos / 12, 1) if destinos >= 5 else 0,\n",
    "        'volumen_llamadas_alto': min((llamadas - 30) / 50, 1) if llamadas > 30 else 0,\n",
    "        'volumen_destinos_alto': min((destinos - 10) / 20, 1) if destinos > 10 else 0,\n",
    "        'llamadas_por_destino': min(llamadas / max(destinos, 1) / 5, 1),\n",
    "        'eficiencia_destinos': min(destinos / max(llamadas * 0.5, 1), 1),\n",
    "        'factor_pais_bajo': 1.5 if categoria in ['Muy_Bajo', 'Bajo'] else 1.0,\n",
    "        'factor_pais_alto': 0.9 if categoria in ['Alto', 'Medio'] else 1.0\n",
    "    }\n",
    "    \n",
    "    return features\n",
    "\n",
    "def predecir_anomalia(row, modelo, scaler, umbral, stats_dict, contexto_historico):\n",
    "    \"\"\"\n",
    "    Realiza la predicci√≥n de anomal√≠a para un registro\n",
    "    \"\"\"\n",
    "    # Crear features\n",
    "    features = crear_features_contextualizadas_mejorada(row, stats_dict)\n",
    "    \n",
    "    # Normalizar\n",
    "    features_scaled = scaler.transform_one(features)\n",
    "    \n",
    "    # Obtener score\n",
    "    score = modelo.score_one(features_scaled)\n",
    "    \n",
    "    # L√≥gica de decisi√≥n\n",
    "    es_anomalia_base = score > umbral\n",
    "    \n",
    "    if es_anomalia_base:\n",
    "        pais = row['CODIGODEPAIS']\n",
    "        llamadas = row['N_LLAMADAS']\n",
    "        minutos = row['N_MINUTOS']\n",
    "        destinos = row['N_DESTINOS']\n",
    "        \n",
    "        # Verificar tipo de anomal√≠a\n",
    "        if minutos >= parametros_features['umbral_minutos_extremos']:\n",
    "            es_anomalia_final = True\n",
    "            razon = f\"Minutos extremos ({minutos:.1f} min)\"\n",
    "            tipo_anomalia = \"MINUTOS_EXTREMOS\"\n",
    "        elif destinos >= 6 and llamadas >= 12:\n",
    "            es_anomalia_final = True\n",
    "            razon = \"Patr√≥n de spray calling confirmado\"\n",
    "            tipo_anomalia = \"SPRAY_CALLING\"\n",
    "        elif llamadas > 50 or destinos > 15:\n",
    "            es_anomalia_final = True\n",
    "            razon = \"Volumen excepcionalmente alto\"\n",
    "            tipo_anomalia = \"VOLUMEN_ALTO\"\n",
    "        elif pais not in stats_dict or stats_dict.get(pais, {}).get('CATEGORIA') in ['Muy_Bajo', 'Bajo']:\n",
    "            if destinos >= 4 and llamadas >= 8:\n",
    "                es_anomalia_final = True\n",
    "                razon = \"Actividad sospechosa en pa√≠s de bajo tr√°fico\"\n",
    "                tipo_anomalia = \"PAIS_BAJO_TRAFICO\"\n",
    "            else:\n",
    "                es_anomalia_final = False\n",
    "                razon = \"Actividad baja en pa√≠s de bajo tr√°fico\"\n",
    "                tipo_anomalia = \"NO_ANOMALIA\"\n",
    "        else:\n",
    "            es_anomalia_final = False\n",
    "            razon = \"No cumple criterios de confirmaci√≥n\"\n",
    "            tipo_anomalia = \"NO_ANOMALIA\"\n",
    "    else:\n",
    "        es_anomalia_final = False\n",
    "        razon = \"Score bajo umbral\"\n",
    "        tipo_anomalia = \"NO_ANOMALIA\"\n",
    "    \n",
    "    # Determinar contexto del pa√≠s\n",
    "    if contexto_historico and pais in contexto_historico:\n",
    "        tipo_contexto = contexto_historico[pais]\n",
    "    elif pais in stats_dict:\n",
    "        tipo_contexto = stats_dict[pais]['CATEGORIA']\n",
    "    else:\n",
    "        tipo_contexto = \"Muy_Bajo\"\n",
    "    \n",
    "    return {\n",
    "        'score': score,\n",
    "        'umbral': umbral,\n",
    "        'es_anomalia': es_anomalia_final,\n",
    "        'tipo_anomalia': tipo_anomalia,\n",
    "        'tipo_contexto': tipo_contexto,\n",
    "        'razon_decision': razon\n",
    "    }\n",
    "\n",
    "print(\"üîß Funciones de procesamiento cargadas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dd8201",
   "metadata": {},
   "source": [
    "# # 4. FUNCI√ìN PRINCIPAL DE PROCESAMIENTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a277f37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def procesar_archivo_csv(archivo_entrada):\n",
    "    \"\"\"\n",
    "    Procesa un archivo CSV y genera resultados\n",
    "    \"\"\"\n",
    "    timestamp = datetime.now()\n",
    "    timestamp_str = timestamp.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üîÑ INICIANDO PROCESAMIENTO - {timestamp.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    try:\n",
    "        # Verificar si existe el archivo\n",
    "        if not os.path.exists(archivo_entrada):\n",
    "            print(f\"‚ö†Ô∏è Archivo no encontrado: {archivo_entrada}\")\n",
    "            return None\n",
    "        \n",
    "        # Cargar datos\n",
    "        print(f\"üìÑ Cargando archivo: {archivo_entrada}\")\n",
    "        df = pd.read_csv(archivo_entrada)\n",
    "        \n",
    "        # Validar columnas requeridas\n",
    "        columnas_requeridas = ['FECHA', 'CODIGODEPAIS', 'LINEA', 'N_LLAMADAS', 'N_MINUTOS', 'N_DESTINOS']\n",
    "        columnas_faltantes = [col for col in columnas_requeridas if col not in df.columns]\n",
    "        \n",
    "        if columnas_faltantes:\n",
    "            print(f\"‚ùå Error: Faltan columnas requeridas: {columnas_faltantes}\")\n",
    "            return None\n",
    "        \n",
    "        # Convertir fecha\n",
    "        df['FECHA'] = pd.to_datetime(df['FECHA'], format='%d/%m/%Y', errors='coerce')\n",
    "        \n",
    "        print(f\"üìä Datos cargados:\")\n",
    "        print(f\"   - Registros: {len(df)}\")\n",
    "        print(f\"   - Pa√≠ses √∫nicos: {df['CODIGODEPAIS'].nunique()}\")\n",
    "        print(f\"   - L√≠neas √∫nicas: {df['LINEA'].nunique()}\")\n",
    "        print(f\"   - Rango de fechas: {df['FECHA'].min()} a {df['FECHA'].max()}\")\n",
    "        \n",
    "        # Procesar cada registro\n",
    "        print(f\"\\nüéØ Aplicando modelo de detecci√≥n...\")\n",
    "        resultados = []\n",
    "        anomalias_detectadas = 0\n",
    "        \n",
    "        for idx, row in df.iterrows():\n",
    "            if idx % 1000 == 0 and idx > 0:\n",
    "                print(f\"   Procesados: {idx}/{len(df)} registros...\")\n",
    "            \n",
    "            # Realizar predicci√≥n\n",
    "            resultado = predecir_anomalia(row, modelo, scaler, umbral_global, stats_dict, contexto_historico)\n",
    "            \n",
    "            # Agregar informaci√≥n completa\n",
    "            resultado_completo = {\n",
    "                'FECHA': row['FECHA'],\n",
    "                'CODIGODEPAIS': row['CODIGODEPAIS'],\n",
    "                'LINEA': row['LINEA'],\n",
    "                'N_LLAMADAS': row['N_LLAMADAS'],\n",
    "                'N_MINUTOS': row['N_MINUTOS'],\n",
    "                'N_DESTINOS': row['N_DESTINOS'],\n",
    "                'score_anomalia': round(resultado['score'], 4),\n",
    "                'umbral': round(resultado['umbral'], 4),\n",
    "                'es_anomalia': resultado['es_anomalia'],\n",
    "                'tipo_anomalia': resultado['tipo_anomalia'],\n",
    "                'tipo_contexto': resultado['tipo_contexto'],\n",
    "                'razon_decision': resultado['razon_decision'],\n",
    "                'timestamp_procesamiento': timestamp\n",
    "            }\n",
    "            \n",
    "            resultados.append(resultado_completo)\n",
    "            \n",
    "            if resultado['es_anomalia']:\n",
    "                anomalias_detectadas += 1\n",
    "        \n",
    "        # Crear DataFrame de resultados\n",
    "        df_resultados = pd.DataFrame(resultados)\n",
    "        \n",
    "        # Generar nombre de archivo de salida\n",
    "        archivo_salida = os.path.join(OUTPUT_PATH, f\"{PREFIJO_SALIDA}_{timestamp_str}.csv\")\n",
    "        \n",
    "        # Guardar resultados completos\n",
    "        df_resultados.to_csv(archivo_salida, index=False)\n",
    "        print(f\"\\nüíæ Resultados guardados: {archivo_salida}\")\n",
    "        \n",
    "        # Guardar solo anomal√≠as si existen\n",
    "        if anomalias_detectadas > 0:\n",
    "            df_anomalias = df_resultados[df_resultados['es_anomalia'] == True]\n",
    "            archivo_anomalias = os.path.join(OUTPUT_PATH, f\"anomalias_{timestamp_str}.csv\")\n",
    "            df_anomalias.to_csv(archivo_anomalias, index=False)\n",
    "            print(f\"üö® Anomal√≠as guardadas: {archivo_anomalias}\")\n",
    "        \n",
    "        # Resumen de resultados\n",
    "        print(f\"\\nüìä RESUMEN DE PROCESAMIENTO:\")\n",
    "        print(f\"   - Total registros procesados: {len(df_resultados)}\")\n",
    "        print(f\"   - Anomal√≠as detectadas: {anomalias_detectadas}\")\n",
    "        print(f\"   - Tasa de anomal√≠as: {(anomalias_detectadas/len(df_resultados)*100):.2f}%\")\n",
    "        \n",
    "        if anomalias_detectadas > 0:\n",
    "            print(f\"\\nüéØ DISTRIBUCI√ìN DE ANOMAL√çAS:\")\n",
    "            distribucion = df_anomalias['tipo_anomalia'].value_counts()\n",
    "            for tipo, cantidad in distribucion.items():\n",
    "                print(f\"   - {tipo}: {cantidad} ({cantidad/anomalias_detectadas*100:.1f}%)\")\n",
    "        \n",
    "        # Crear registro de log\n",
    "        log_entry = {\n",
    "            'timestamp': timestamp,\n",
    "            'archivo_entrada': os.path.basename(archivo_entrada),\n",
    "            'registros_procesados': len(df_resultados),\n",
    "            'anomalias_detectadas': anomalias_detectadas,\n",
    "            'tasa_anomalias': anomalias_detectadas/len(df_resultados)*100,\n",
    "            'archivo_salida': os.path.basename(archivo_salida)\n",
    "        }\n",
    "        \n",
    "        # Guardar log\n",
    "        log_file = os.path.join(LOG_PATH, f\"log_procesamiento_{timestamp.strftime('%Y%m%d')}.csv\")\n",
    "        if os.path.exists(log_file):\n",
    "            df_log = pd.read_csv(log_file)\n",
    "            df_log = pd.concat([df_log, pd.DataFrame([log_entry])], ignore_index=True)\n",
    "        else:\n",
    "            df_log = pd.DataFrame([log_entry])\n",
    "        df_log.to_csv(log_file, index=False)\n",
    "        \n",
    "        print(f\"üìù Log actualizado: {log_file}\")\n",
    "        \n",
    "        return df_resultados\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error durante el procesamiento: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6485b10a",
   "metadata": {},
   "source": [
    "# # 5. MODO DE PROCESAMIENTO √öNICO (MANUAL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26eac73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para procesamiento manual de un archivo espec√≠fico\n",
    "print(\"\\nüîß MODO DE PROCESAMIENTO √öNICO\")\n",
    "\n",
    "# Archivo de entrada\n",
    "archivo_manual = os.path.join(INPUT_PATH, ARCHIVO_ENTRADA)\n",
    "\n",
    "# Verificar si existe\n",
    "if os.path.exists(archivo_manual):\n",
    "    print(f\"‚úÖ Archivo encontrado: {archivo_manual}\")\n",
    "    \n",
    "    # Procesar\n",
    "    resultados = procesar_archivo_csv(archivo_manual)\n",
    "    \n",
    "    if resultados is not None:\n",
    "        print(f\"\\n‚úÖ Procesamiento completado exitosamente\")\n",
    "        \n",
    "        # Mover archivo a procesados (opcional)\n",
    "        archivo_procesado = os.path.join(PROCESSED_PATH, f\"procesado_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{ARCHIVO_ENTRADA}\")\n",
    "        shutil.copy2(archivo_manual, archivo_procesado)\n",
    "        print(f\"üìÅ Archivo copiado a procesados: {archivo_procesado}\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è No se encontr√≥ el archivo: {archivo_manual}\")\n",
    "    print(f\"üìÅ Verificar en: {INPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cce139",
   "metadata": {},
   "source": [
    "# # 6. MODO DE MONITOREO CONTINUO (AUTOM√ÅTICO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84894be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def monitoreo_continuo(max_iteraciones=None):\n",
    "    \"\"\"\n",
    "    Monitorea continuamente la carpeta de entrada y procesa archivos\n",
    "    \n",
    "    Args:\n",
    "        max_iteraciones: N√∫mero m√°ximo de iteraciones (None = infinito)\n",
    "    \"\"\"\n",
    "    print(f\"\\nüîÑ INICIANDO MONITOREO CONTINUO\")\n",
    "    print(f\"‚è±Ô∏è Intervalo: {INTERVALO_SEGUNDOS} segundos\")\n",
    "    print(f\"üìÅ Monitoreando: {INPUT_PATH}\")\n",
    "    print(f\"üìÑ Archivo objetivo: {ARCHIVO_ENTRADA}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(\"Presiona Ctrl+C para detener el monitoreo\\n\")\n",
    "    \n",
    "    iteracion = 0\n",
    "    ultimo_modificado = None\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            iteracion += 1\n",
    "            \n",
    "            if max_iteraciones and iteracion > max_iteraciones:\n",
    "                print(f\"\\n‚úÖ Alcanzado el l√≠mite de {max_iteraciones} iteraciones\")\n",
    "                break\n",
    "            \n",
    "            archivo_entrada = os.path.join(INPUT_PATH, ARCHIVO_ENTRADA)\n",
    "            \n",
    "            # Verificar si el archivo existe\n",
    "            if os.path.exists(archivo_entrada):\n",
    "                # Obtener tiempo de modificaci√≥n\n",
    "                tiempo_modificacion = os.path.getmtime(archivo_entrada)\n",
    "                \n",
    "                # Procesar solo si el archivo fue modificado\n",
    "                if ultimo_modificado is None or tiempo_modificacion > ultimo_modificado:\n",
    "                    print(f\"\\nüîî Iteraci√≥n {iteracion} - Archivo detectado/modificado\")\n",
    "                    \n",
    "                    # Procesar archivo\n",
    "                    resultados = procesar_archivo_csv(archivo_entrada)\n",
    "                    \n",
    "                    if resultados is not None:\n",
    "                        ultimo_modificado = tiempo_modificacion\n",
    "                        \n",
    "                        # Crear copia con timestamp (opcional)\n",
    "                        archivo_backup = os.path.join(\n",
    "                            PROCESSED_PATH, \n",
    "                            f\"backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{ARCHIVO_ENTRADA}\"\n",
    "                        )\n",
    "                        shutil.copy2(archivo_entrada, archivo_backup)\n",
    "                else:\n",
    "                    print(f\"\\r‚è≥ Iteraci√≥n {iteracion} - Sin cambios en el archivo... (esperando {INTERVALO_SEGUNDOS}s)\", end='')\n",
    "            else:\n",
    "                print(f\"\\r‚è≥ Iteraci√≥n {iteracion} - Archivo no encontrado... (esperando {INTERVALO_SEGUNDOS}s)\", end='')\n",
    "            \n",
    "            # Esperar antes de la siguiente verificaci√≥n\n",
    "            time.sleep(INTERVALO_SEGUNDOS)\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        print(f\"\\n\\n‚èπÔ∏è Monitoreo detenido por el usuario\")\n",
    "        print(f\"üìä Total de iteraciones: {iteracion}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fca981a",
   "metadata": {},
   "source": [
    "# # 7. AN√ÅLISIS DE RESULTADOS HIST√ìRICOS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3a0c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analizar_resultados_historicos():\n",
    "    \"\"\"\n",
    "    Analiza todos los resultados generados hasta el momento\n",
    "    \"\"\"\n",
    "    print(f\"\\nüìä AN√ÅLISIS DE RESULTADOS HIST√ìRICOS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Buscar todos los archivos de resultados\n",
    "    archivos_resultados = [f for f in os.listdir(OUTPUT_PATH) if f.startswith(PREFIJO_SALIDA) and f.endswith('.csv')]\n",
    "    \n",
    "    if not archivos_resultados:\n",
    "        print(\"‚ö†Ô∏è No se encontraron archivos de resultados\")\n",
    "        return\n",
    "    \n",
    "    print(f\"üìÑ Archivos encontrados: {len(archivos_resultados)}\")\n",
    "    \n",
    "    # Cargar y combinar todos los resultados\n",
    "    dfs = []\n",
    "    for archivo in archivos_resultados:\n",
    "        try:\n",
    "            df = pd.read_csv(os.path.join(OUTPUT_PATH, archivo))\n",
    "            dfs.append(df)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    if not dfs:\n",
    "        print(\"‚ö†Ô∏è No se pudieron cargar los archivos\")\n",
    "        return\n",
    "    \n",
    "    df_total = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    # An√°lisis general\n",
    "    print(f\"\\nüìä ESTAD√çSTICAS GENERALES:\")\n",
    "    print(f\"   - Total registros analizados: {len(df_total):,}\")\n",
    "    print(f\"   - Total anomal√≠as detectadas: {df_total['es_anomalia'].sum():,}\")\n",
    "    print(f\"   - Tasa promedio de anomal√≠as: {df_total['es_anomalia'].mean()*100:.2f}%\")\n",
    "    print(f\"   - Pa√≠ses √∫nicos: {df_total['CODIGODEPAIS'].nunique()}\")\n",
    "    print(f\"   - L√≠neas √∫nicas: {df_total['LINEA'].nunique()}\")\n",
    "    \n",
    "    # Distribuci√≥n de tipos de anomal√≠as\n",
    "    anomalias = df_total[df_total['es_anomalia'] == True]\n",
    "    if len(anomalias) > 0:\n",
    "        print(f\"\\nüéØ DISTRIBUCI√ìN DE ANOMAL√çAS:\")\n",
    "        dist_tipos = anomalias['tipo_anomalia'].value_counts()\n",
    "        for tipo, cantidad in dist_tipos.items():\n",
    "            print(f\"   - {tipo}: {cantidad:,} ({cantidad/len(anomalias)*100:.1f}%)\")\n",
    "        \n",
    "        # Top pa√≠ses con m√°s anomal√≠as\n",
    "        print(f\"\\nüåç TOP 10 PA√çSES CON M√ÅS ANOMAL√çAS:\")\n",
    "        top_paises = anomalias['CODIGODEPAIS'].value_counts().head(10)\n",
    "        for pais, cantidad in top_paises.items():\n",
    "            print(f\"   - {pais}: {cantidad:,} anomal√≠as\")\n",
    "        \n",
    "        # Evoluci√≥n temporal\n",
    "        if 'timestamp_procesamiento' in df_total.columns:\n",
    "            df_total['timestamp_procesamiento'] = pd.to_datetime(df_total['timestamp_procesamiento'])\n",
    "            df_total['hora'] = df_total['timestamp_procesamiento'].dt.hour\n",
    "            \n",
    "            print(f\"\\n‚è∞ DISTRIBUCI√ìN POR HORA DEL D√çA:\")\n",
    "            anomalias_por_hora = df_total.groupby('hora')['es_anomalia'].agg(['sum', 'count', 'mean'])\n",
    "            anomalias_por_hora.columns = ['anomalias', 'total', 'tasa']\n",
    "            anomalias_por_hora['tasa'] = anomalias_por_hora['tasa'] * 100\n",
    "            \n",
    "            for hora, row in anomalias_por_hora.iterrows():\n",
    "                if row['total'] > 0:\n",
    "                    print(f\"   - {hora:02d}:00: {int(row['anomalias']):,} anomal√≠as de {int(row['total']):,} ({row['tasa']:.1f}%)\")\n",
    "\n",
    "# Ejecutar an√°lisis\n",
    "analizar_resultados_historicos()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893a461f",
   "metadata": {},
   "source": [
    "# # 8. UTILIDADES Y FUNCIONES AUXILIARES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc17c324",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiar_archivos_antiguos(dias_retener=7):\n",
    "    \"\"\"\n",
    "    Limpia archivos de resultados m√°s antiguos que los d√≠as especificados\n",
    "    \"\"\"\n",
    "    from datetime import timedelta\n",
    "    \n",
    "    fecha_limite = datetime.now() - timedelta(days=dias_retener)\n",
    "    archivos_eliminados = 0\n",
    "    \n",
    "    print(f\"\\nüßπ Limpiando archivos anteriores a {fecha_limite.strftime('%Y-%m-%d')}\")\n",
    "    \n",
    "    # Limpiar resultados\n",
    "    for archivo in os.listdir(OUTPUT_PATH):\n",
    "        ruta_completa = os.path.join(OUTPUT_PATH, archivo)\n",
    "        if os.path.isfile(ruta_completa):\n",
    "            fecha_modificacion = datetime.fromtimestamp(os.path.getmtime(ruta_completa))\n",
    "            if fecha_modificacion < fecha_limite:\n",
    "                os.remove(ruta_completa)\n",
    "                archivos_eliminados += 1\n",
    "    \n",
    "    # Limpiar procesados\n",
    "    for archivo in os.listdir(PROCESSED_PATH):\n",
    "        ruta_completa = os.path.join(PROCESSED_PATH, archivo)\n",
    "        if os.path.isfile(ruta_completa):\n",
    "            fecha_modificacion = datetime.fromtimestamp(os.path.getmtime(ruta_completa))\n",
    "            if fecha_modificacion < fecha_limite:\n",
    "                os.remove(ruta_completa)\n",
    "                archivos_eliminados += 1\n",
    "    \n",
    "    print(f\"‚úÖ Archivos eliminados: {archivos_eliminados}\")\n",
    "\n",
    "def verificar_salud_sistema():\n",
    "    \"\"\"\n",
    "    Verifica el estado del sistema de procesamiento\n",
    "    \"\"\"\n",
    "    print(f\"\\nüè• VERIFICACI√ìN DE SALUD DEL SISTEMA\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Verificar modelo\n",
    "    print(f\"ü§ñ Modelo:\")\n",
    "    print(f\"   - Cargado: {'‚úÖ' if 'modelo' in globals() else '‚ùå'}\")\n",
    "    print(f\"   - Umbral: {umbral_global:.4f}\")\n",
    "    \n",
    "    # Verificar directorios\n",
    "    print(f\"\\nüìÅ Directorios:\")\n",
    "    for nombre, ruta in [\n",
    "        (\"Entrada\", INPUT_PATH),\n",
    "        (\"Salida\", OUTPUT_PATH),\n",
    "        (\"Procesados\", PROCESSED_PATH),\n",
    "        (\"Logs\", LOG_PATH)\n",
    "    ]:\n",
    "        existe = os.path.exists(ruta)\n",
    "        print(f\"   - {nombre}: {'‚úÖ' if existe else '‚ùå'} {ruta}\")\n",
    "    \n",
    "    # Verificar espacio en disco\n",
    "    import shutil\n",
    "    stat = shutil.disk_usage(OUTPUT_PATH)\n",
    "    espacio_gb = stat.free / (1024**3)\n",
    "    print(f\"\\nüíæ Espacio en disco:\")\n",
    "    print(f\"   - Libre: {espacio_gb:.1f} GB\")\n",
    "    print(f\"   - Estado: {'‚úÖ' if espacio_gb > 1 else '‚ö†Ô∏è Poco espacio'}\")\n",
    "    \n",
    "    # Verificar archivos recientes\n",
    "    print(f\"\\nüìÑ Archivos recientes:\")\n",
    "    archivos_entrada = len([f for f in os.listdir(INPUT_PATH) if f.endswith('.csv')])\n",
    "    archivos_salida = len([f for f in os.listdir(OUTPUT_PATH) if f.endswith('.csv')])\n",
    "    print(f\"   - Archivos en entrada: {archivos_entrada}\")\n",
    "    print(f\"   - Archivos en salida: {archivos_salida}\")\n",
    "    \n",
    "    # Verificar √∫ltimo procesamiento\n",
    "    archivos_log = [f for f in os.listdir(LOG_PATH) if f.startswith('log_procesamiento')]\n",
    "    if archivos_log:\n",
    "        ultimo_log = max(archivos_log)\n",
    "        print(f\"\\nüìù √öltimo log: {ultimo_log}\")\n",
    "\n",
    "# Ejecutar verificaci√≥n\n",
    "verificar_salud_sistema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bd409b",
   "metadata": {},
   "source": [
    "# # 9. INSTRUCCIONES DE USO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e46be9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìö INSTRUCCIONES DE USO\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n1Ô∏è‚É£ PROCESAMIENTO √öNICO (Manual):\")\n",
    "print(\"   - Coloca tu archivo CSV en:\", INPUT_PATH)\n",
    "print(\"   - El archivo debe llamarse:\", ARCHIVO_ENTRADA)\n",
    "print(\"   - Ejecuta la celda de 'MODO DE PROCESAMIENTO √öNICO'\")\n",
    "print(\"   - Los resultados se guardar√°n en:\", OUTPUT_PATH)\n",
    "\n",
    "print(\"\\n2Ô∏è‚É£ MONITOREO CONTINUO (Autom√°tico):\")\n",
    "print(\"   - Ejecuta: monitoreo_continuo()\")\n",
    "print(\"   - El sistema verificar√° cada\", INTERVALO_SEGUNDOS, \"segundos\")\n",
    "print(\"   - Para limitar iteraciones: monitoreo_continuo(max_iteraciones=10)\")\n",
    "print(\"   - Para detener: presiona Ctrl+C\")\n",
    "\n",
    "print(\"\\n3Ô∏è‚É£ AN√ÅLISIS DE HIST√ìRICOS:\")\n",
    "print(\"   - Ejecuta: analizar_resultados_historicos()\")\n",
    "print(\"   - Ver√°s estad√≠sticas de todos los procesamientos\")\n",
    "\n",
    "print(\"\\n4Ô∏è‚É£ MANTENIMIENTO:\")\n",
    "print(\"   - Para limpiar archivos antiguos: limpiar_archivos_antiguos(dias_retener=7)\")\n",
    "print(\"   - Para verificar el sistema: verificar_salud_sistema()\")\n",
    "\n",
    "print(\"\\nüìÑ FORMATO DEL CSV DE ENTRADA:\")\n",
    "print(\"   - FECHA (formato: DD/MM/YYYY)\")\n",
    "print(\"   - CODIGODEPAIS\")\n",
    "print(\"   - LINEA\")\n",
    "print(\"   - N_LLAMADAS\")\n",
    "print(\"   - N_MINUTOS\")\n",
    "print(\"   - N_DESTINOS\")\n",
    "\n",
    "print(\"\\nüìä ARCHIVOS DE SALIDA GENERADOS:\")\n",
    "print(\"   - resultados_YYYYMMDD_HHMMSS.csv (todos los registros)\")\n",
    "print(\"   - anomalias_YYYYMMDD_HHMMSS.csv (solo anomal√≠as)\")\n",
    "print(\"   - log_procesamiento_YYYYMMDD.csv (registro de procesamientos)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67461430",
   "metadata": {},
   "source": [
    "# # 10. EJEMPLO DE EJECUCI√ìN R√ÅPIDA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b42ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPCI√ìN 1: Procesamiento √∫nico\n",
    "# Descomenta la siguiente l√≠nea para procesar una vez\n",
    "# resultados = procesar_archivo_csv(os.path.join(INPUT_PATH, ARCHIVO_ENTRADA))\n",
    "\n",
    "# OPCI√ìN 2: Monitoreo continuo por 5 iteraciones\n",
    "# Descomenta la siguiente l√≠nea para monitoreo autom√°tico\n",
    "# monitoreo_continuo(max_iteraciones=5)\n",
    "\n",
    "# OPCI√ìN 3: Monitoreo continuo indefinido\n",
    "# Descomenta la siguiente l√≠nea para monitoreo continuo\n",
    "# monitoreo_continuo()\n",
    "\n",
    "print(\"‚úÖ Sistema listo para usar\")\n",
    "print(\"üîß Descomenta una de las opciones anteriores para comenzar\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
